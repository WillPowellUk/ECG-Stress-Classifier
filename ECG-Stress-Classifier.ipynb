{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stress Classifier\n",
    "## Overview\n",
    "* Data Extraction: Downloads and sorts through database\n",
    "* Signal processing: \n",
    "    + Pre-processing - filtering and signal cleaning\n",
    "    + Feature Extraction - PQRST peak extraction\n",
    "    + Feature Addition - Adding new features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and Globals\n",
    "Modify settings to select database, model etc. and tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SETTINGS\n",
    "\n",
    "# Select Database:\n",
    "database = \"Spider\" # database = \"BrainPatch\"\n",
    "\n",
    "if database == 'Spider':\n",
    "    sampling_rate = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import subprocess\n",
    "import Utilities\n",
    "import concurrent.futures\n",
    "\n",
    "# # Repository's modules\n",
    "# import importlib\n",
    "# importlib.reload(Utilities) # checks for updates to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataExtraction class\n",
    "class DataExtraction():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    # Extract data and store to file named Data\n",
    "    def download_data(self):\n",
    "        if database == 'Spider':\n",
    "            directory = 'Data/Spider'\n",
    "            url = 'https://physionet.org/files/ecg-spider-clip/1.0.0/'\n",
    "            if not os.path.isdir(directory):\n",
    "                print(\"Downloading database...this may take a while\")\n",
    "                os.makedirs(directory)\n",
    "                cmd = f\"wget -r -N -c -np -P {directory} {url}\"\n",
    "                print(cmd)\n",
    "                try:\n",
    "                    subprocess.run(cmd)\n",
    "                except:\n",
    "                    print(\"Error: Unable to download database\")\n",
    "                    os.rmdir(directory)\n",
    "            else:\n",
    "                print(\"Using pre-downloaded database\")\n",
    "    \n",
    "    # sorts data into a single dataframe for each participant into a collective dataframe list\n",
    "    def sort_data(self):\n",
    "        print(\"Sorting data...\")\n",
    "\n",
    "        # try loading existing df if available\n",
    "        file_path = f'Data/{database}/StoredDataFrames/Sorted'\n",
    "        ECG_df = Utilities.load_list_of_dataframes(file_path)\n",
    "        if ECG_df:\n",
    "            return ECG_df\n",
    "        \n",
    "        # otherwise create dataframe from scratch\n",
    "        ECG_df = []\n",
    "\n",
    "        if database == 'Spider':\n",
    "            database_directory = 'Data/Spider/physionet.org/files/ecg-spider-clip/1.0.0/'\n",
    "            # Exclude VP70 because of noise\n",
    "            sub_directories = ['VP02', 'VP03','VP05','VP06','VP08','VP09','VP11','VP12','VP14','VP15','VP17','VP18','VP20','VP23','VP24','VP26','VP27',\n",
    "                    'VP29','VP30','VP32','VP33','VP35','VP36','VP38','VP39','VP41','VP42','VP44','VP45','VP47','VP48','VP50','VP51','VP53',\n",
    "                    'VP54','VP56','VP57','VP59','VP61','VP62','VP63','VP64','VP65','VP66','VP68','VP69','VP71','VP72','VP73','VP74',\n",
    "                    'VP75','VP76','VP77','VP78','VP79','VP80']\n",
    "            for index, sub in enumerate(sub_directories):\n",
    "                # set path\n",
    "                ECG_file = f'{database_directory}{sub}/BitalinoECG.txt'\n",
    "                triggers_file = f'{database_directory}{sub}/Triggers.txt'\n",
    "\n",
    "                # append data to dataframe\n",
    "                ECG_participant_df = pd.read_csv(ECG_file, sep='\\t', names = ['ECG','time','NA'], engine='python')\n",
    "                ECG_participant_df = ECG_participant_df.drop(columns=['NA'])\n",
    "                \n",
    "                # set the start time to use to normalize the other times\n",
    "                normalized_time = ECG_participant_df.iloc[0,1]\n",
    "                ECG_participant_df.time = ECG_participant_df.time-normalized_time\n",
    "\n",
    "                # read in trigger file\n",
    "                triggers_df_temp = pd.read_csv(triggers_file, sep='\\t', names = ['clip','on','off'], engine='python')\n",
    "                triggers_df_temp.on = triggers_df_temp.on-normalized_time\n",
    "                triggers_df_temp.off = triggers_df_temp.off-normalized_time\n",
    "\n",
    "                # Create the 'Stressed' (label) column with all zeros\n",
    "                ECG_participant_df[\"Stressed\"] = np.zeros(len(ECG_participant_df))\n",
    "                # This checks which time stamps fall into the time ranges when the clips are delivered, results in a column of \"true\" and \"false\"\n",
    "                conditions = pd.concat([(ECG_participant_df['time'] >= triggers_df_temp.on[i]) & (ECG_participant_df['time'] <= triggers_df_temp.off[i]) for i in range(0,17)],axis=1).any(axis=1)\n",
    "                ECG_participant_df[\"Stressed\"] = conditions\n",
    "\n",
    "                # append data to complete df dictionary\n",
    "                ECG_df.append(ECG_participant_df)\n",
    "\n",
    "                Utilities.progress_bar(index, len(sub_directories)-1)\n",
    "\n",
    "        # save dataframe for nextime and return\n",
    "        Utilities.save_list_of_dataframes(ECG_df, file_path)\n",
    "        return ECG_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Utilities\n",
    "class Utilities():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def progress_bar(current, total, bar_length=20):\n",
    "        fraction = current / total\n",
    "\n",
    "        arrow = int(fraction * bar_length - 1) * '-' + '>'\n",
    "        padding = int(bar_length - len(arrow)) * ' '\n",
    "\n",
    "        ending = '\\n' if current == total else '\\r'\n",
    "\n",
    "        print(f'Progress: [{arrow}{padding}] {int(fraction*100)}%', end=ending)\n",
    "\n",
    "    def save_list_of_dataframes(df_list, folder_path):\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        # save each dataframe as a CSV file in the folder\n",
    "        for i, df in enumerate(df_list):\n",
    "            filename = os.path.join(folder_path, f'df_{i}.csv')\n",
    "            df.to_csv(filename, index=False)\n",
    "\n",
    "        print(\"Dataframes saved as CSV files in folder:\", folder_path)\n",
    "\n",
    "    def load_list_of_dataframes(folder_path):\n",
    "        # initialize an empty list to hold the dataframes\n",
    "        df_list = []\n",
    "\n",
    "        try:\n",
    "            # iterate over all files in the folder\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith('.csv'):\n",
    "                    # read the CSV file into a dataframe and append to the list\n",
    "                    filepath = os.path.join(folder_path, filename)\n",
    "                    df = pd.read_csv(filepath)\n",
    "                    df_list.append(df)\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "        \n",
    "        print(f\"Using stored dataframe from {folder_path}\")\n",
    "        return df_list\n",
    "\n",
    "    def insert_dataframe(main_df, new_df):\n",
    "        # get the index of the second to last column\n",
    "        idx = len(main_df.columns) - 1\n",
    "\n",
    "        # split the original dataframe into two parts\n",
    "        df1 = main_df.iloc[:, :idx]\n",
    "        df2 = main_df.iloc[:, idx:]\n",
    "\n",
    "        # concatenate the two dataframes with the new dataframe in between\n",
    "        return pd.concat([df1, new_df, df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SignalProcessing Class\n",
    "\n",
    "# Using Neurokit2\n",
    "# Documentation can be found here: https://neuropsychology.github.io/NeuroKit/functions/ecg.html                      \n",
    "class SignalProcessing():\n",
    "    # database_percentage - if smaller value used, model will be quicker but model less generalized\n",
    "    def __init__(self, ECG_df, database_percentage=100, multithreading=True):\n",
    "        index = int(len(ECG_df) * database_percentage/100)\n",
    "        index = 2 if (index < 2) else index\n",
    "        self.ECG_df = ECG_df[0:]\n",
    "        self.multithreading = multithreading\n",
    "\n",
    "    def clean(self):\n",
    "        print('Cleaning data...')\n",
    "        # try loading existing df if available\n",
    "        file_path = f'Data/{database}/StoredDataFrames/Cleaned'\n",
    "        ECG_df = Utilities.load_list_of_dataframes(file_path)\n",
    "        if ECG_df:\n",
    "            self.ECG_df = ECG_df\n",
    "            return self.ECG_df\n",
    "        \n",
    "        # otherwise, create df from scratch, overwriting ECG_df with cleaned version \n",
    "        ECG_vect = []\n",
    "        for index, ECG_particpant_df in enumerate(self.ECG_df):\n",
    "            # using method 'neurokit' (0.5 Hz high-pass butterworth filter (order = 5), followed by powerline filtering) but can be changed to other cleaning methods\n",
    "            ECG_vect = nk.ecg_clean(ECG_particpant_df['ECG'], sampling_rate, method='neurokit')\n",
    "            Utilities.progress_bar(index+1, len(self.ECG_df))\n",
    "            self.ECG_df[index]['ECG'] = ECG_vect\n",
    "        \n",
    "        # save dataframe for nextime\n",
    "        Utilities.save_list_of_dataframes(self.ECG_df, file_path)\n",
    "\n",
    "    # Extracts features from ECG. Recommended to clean data first.\n",
    "    def feature_extraction(self, HRV=True, EDR=True):\n",
    "        # try loading existing df if available\n",
    "        file_path = f'Data/{database}/StoredDataFrames/AllFeatures'\n",
    "        ECG_df = Utilities.load_list_of_dataframes(file_path)\n",
    "        if ECG_df:\n",
    "            self.ECG_df = ECG_df\n",
    "            return self.ECG_df\n",
    "        \n",
    "        # Extract R-R peaks\n",
    "        print('Extracting R-R peaks...')\n",
    "        # np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "        \n",
    "        r_peaks_df = []\n",
    "        for index, ECG_particpant_df in enumerate(self.ECG_df):\n",
    "            # extract R-R peaks\n",
    "            r_peaks_df.append(nk.ecg_peaks(ECG_particpant_df['ECG'], sampling_rate=sampling_rate, correct_artifacts=True)[0])\n",
    "            Utilities.progress_bar(index+1, len(self.ECG_df))\n",
    "\n",
    "        # Extract HRV features from R-R peaks, see https://neuropsychology.github.io/NeuroKit/functions/hrv.html \n",
    "        if HRV:\n",
    "            print(\"Extracting HRV...\")\n",
    "            for index, ECG_particpant_df in enumerate(self.ECG_df):\n",
    "                # compute HRV - time, frequency and nonlinear indices.\n",
    "                if index == 0:\n",
    "                    HRV_df = nk.hrv(r_peaks_df[index], sampling_rate=sampling_rate, show=True)\n",
    "                else: \n",
    "                    HRV_df = nk.hrv(r_peaks_df[index], sampling_rate=sampling_rate, show=False)\n",
    "                self.ECG_df[index] = Utilities.insert_dataframe(self.ECG_df[index], HRV_df)\n",
    "                Utilities.progress_bar(index+1, len(self.ECG_df))\n",
    "        \n",
    "        if EDR:\n",
    "            print(\"Extracting EDR...\")\n",
    "            for index, ECG_particpant_df in enumerate(self.ECG_df):\n",
    "                # Get ECG Derived Respiration (EDR) and add to the data\n",
    "                ecg_rate = nk.signal_rate(r_peaks_df[index], sampling_rate=sampling_rate, desired_length=len(r_peaks_df))\n",
    "                EDR = nk.ecg_rsp(ecg_rate, sampling_rate=sampling_rate)\n",
    "                self.ECG_df[index]['EDR'] = EDR\n",
    "                Utilities.progress_bar(index+1, len(self.ECG_df))\n",
    "\n",
    "        # save dataframe for nextime\n",
    "        Utilities.save_list_of_dataframes(self.ECG_df, file_path)\n",
    "\n",
    "    # select features for model using exisiting dataframe\n",
    "    def select_features(self, list):       \n",
    "        # print current headers to console\n",
    "        print(\"Current headers:\", list(self.ECG_df[0].columns))\n",
    "        # prompt user to select headers for new dataframe\n",
    "        selected_headers = input(\"Enter features to include in new dataframe (separated by commas and no spaces): \").strip().split(\",\")\n",
    "\n",
    "        print(\"Selecting features...\")\n",
    "        for index, ECG_particpant_df in enumerate(self.ECG_df):\n",
    "            # create a new dataframe with selected columns (always include label stressed)\n",
    "            self.ECG_df[index] = self.ECG_df[index].loc[:, selected_headers + ['Stressed']]\n",
    "            Utilities.progress_bar(index+1, len(self.ECG_df))\n",
    "        \n",
    "        # save dataframe for nextime\n",
    "        file_path = f'Data/{database}/StoredDataFrames/SelectedFeatures'\n",
    "        Utilities.save_list_of_dataframes(self.ECG_df, file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction and Pre-processing\n",
    "* Downloads data, normalizes timeframe and puts data into a dataframe dictionary of all partcipant data - `ECG_df`.\n",
    "* Cleans data using Neurokit's 5th Order Butterworth filter.\n",
    "* Extracts features such as HRV time, frequency and non-linear domain, EDR etc. \n",
    "\n",
    "This will take a while if you haven't previously ran this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-downloaded database\n",
      "Sorting data...\n",
      "Using stored dataframe from Data/Spider/StoredDataFrames/Sorted\n",
      "Cleaning data...\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/u/g/wfp21/Documents/Stress Classifier Using ECG/ECG-Stress-Classifier.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Pass dataframe to be cleaned and features to be extracted\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m sp \u001b[39m=\u001b[39m SignalProcessing(ECG_df, database_percentage\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, multithreading\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m sp\u001b[39m.\u001b[39;49mclean()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m sp\u001b[39m.\u001b[39mfeature_extraction(HRV\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, EDR\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Select desired features for model (prompt)\u001b[39;00m\n",
      "\u001b[1;32m/u/g/wfp21/Documents/Stress Classifier Using ECG/ECG-Stress-Classifier.ipynb Cell 9\u001b[0m in \u001b[0;36mSignalProcessing.clean\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# try loading existing df if available\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m file_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mData/\u001b[39m\u001b[39m{\u001b[39;00mdatabase\u001b[39m}\u001b[39;00m\u001b[39m/StoredDataFrames/Cleaned\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m ECG_df \u001b[39m=\u001b[39m Utilities\u001b[39m.\u001b[39;49mload_list_of_dataframes(file_path)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mif\u001b[39;00m ECG_df:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mECG_df \u001b[39m=\u001b[39m ECG_df\n",
      "\u001b[1;32m/u/g/wfp21/Documents/Stress Classifier Using ECG/ECG-Stress-Classifier.ipynb Cell 9\u001b[0m in \u001b[0;36mUtilities.load_list_of_dataframes\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m         \u001b[39mif\u001b[39;00m filename\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m             \u001b[39m# read the CSV file into a dataframe and append to the list\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m             filepath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_path, filename)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m             df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(filepath)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m             df_list\u001b[39m.\u001b[39mappend(df)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    580\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 581\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1254\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1252\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1253\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1254\u001b[0m     index, columns, col_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(nrows)\n\u001b[1;32m   1255\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1256\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 225\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[1;32m    226\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    227\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:1960\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "# Download and sort data to create dataframe\n",
    "de = DataExtraction()\n",
    "de.download_data()\n",
    "ECG_df = de.sort_data()\n",
    "\n",
    "# Pass dataframe to be cleaned and features to be extracted\n",
    "sp = SignalProcessing(ECG_df, database_percentage=5, multithreading=False)\n",
    "sp.clean()\n",
    "sp.feature_extraction(HRV=True, EDR=False)\n",
    "\n",
    "# Select desired features for model (prompt)\n",
    "sp.select_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
