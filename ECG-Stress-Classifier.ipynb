{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stress Classifier\n",
    "## Overview\n",
    "* Data Extraction: Downloads and sorts through database\n",
    "* Signal processing: \n",
    "    + Pre-processing - filtering and signal cleaning\n",
    "    + Feature Extraction - R-R peaks, PQRST peaks, EDR, in addition to mean, kurtosis etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and Globals\n",
    "Modify settings to select database, model etc. and tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:56.740564Z",
     "iopub.status.busy": "2023-03-28T14:15:56.740124Z",
     "iopub.status.idle": "2023-03-28T14:15:56.746295Z",
     "shell.execute_reply": "2023-03-28T14:15:56.745553Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# SETTINGS\n",
    "\n",
    "# Database:\n",
    "database = \"Spider\" # database = \"BrainPatch\"\n",
    "\n",
    "# Preprocessing / Feature Extraction:\n",
    "window_length = 40 # window length in seconds\n",
    "overlap = 0.1 # overlap percentage for rolling window (increasing will result in more overlapped samples)\n",
    "\n",
    "if database == 'Spider':\n",
    "    sampling_rate = 100\n",
    "    number_of_participants = 56\n",
    "\n",
    "directory = f'Data/{database}/StoredDataFrames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:56.748730Z",
     "iopub.status.busy": "2023-03-28T14:15:56.748407Z",
     "iopub.status.idle": "2023-03-28T14:15:57.623281Z",
     "shell.execute_reply": "2023-03-28T14:15:57.623003Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import subprocess\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.624839Z",
     "iopub.status.busy": "2023-03-28T14:15:57.624743Z",
     "iopub.status.idle": "2023-03-28T14:15:57.631111Z",
     "shell.execute_reply": "2023-03-28T14:15:57.630859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Useful Utilities\n",
    "class Utilities():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def progress_bar(current, total, bar_length=20):\n",
    "        fraction = current / total\n",
    "\n",
    "        arrow = int(fraction * bar_length - 1) * '-' + '>'\n",
    "        padding = int(bar_length - len(arrow)) * ' '\n",
    "\n",
    "        ending = '\\n' if current == total else '\\r'\n",
    "\n",
    "        print(f'Progress: [{arrow}{padding}] {int(fraction*100)}%', end=ending)\n",
    "\n",
    "\n",
    "    def print_overwrite(msg):\n",
    "        # add padding to line \n",
    "        padding = ' ' * 20\n",
    "        # Print the padded message and remember it as the previous message\n",
    "        print(msg+padding, end=\"\\r\")\n",
    "\n",
    "\n",
    "    def check_csv_exists(folder_path, index):\n",
    "        # read the CSV file into a dataframe and append to the list\n",
    "        filename = os.path.join(folder_path, f'df_{index}.csv')\n",
    "        try:\n",
    "            df = pd.read_csv(filename)\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "        return filename\n",
    "\n",
    "\n",
    "    def load_dataframe(filename):\n",
    "        # read the CSV file into a dataframe and append to the list\n",
    "        df = pd.read_csv(filename)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def save_dataframe(df, folder_path, index=-1, name=''):\n",
    "        # create directoy if necessary\n",
    "        os.makedirs(folder_path, exist_ok=True) \n",
    "        # either store as an index or store as name\n",
    "        if index!=-1:\n",
    "            filename = os.path.join(folder_path, f'df_{index}.csv')\n",
    "        else:\n",
    "            filename = os.path.join(folder_path, f'{name}.csv')\n",
    "        # if filename already exists, concatenate vertically with it.\n",
    "        if os.path.exists(filename):\n",
    "            df_new = pd.concat([Utilities.load_dataframe(filename), df], axis=0, ignore_index=True)\n",
    "            df_new.to_csv(filename, index=False)\n",
    "        # otherwise, save as new file\n",
    "        else:\n",
    "            df.to_csv(filename, index=False)\n",
    "\n",
    "    \n",
    "    def save_list_of_dataframes(df_list, folder_path):\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        # save each dataframe as a CSV file in the folder\n",
    "        for i, df in enumerate(df_list):\n",
    "            filename = os.path.join(folder_path, f'df_{i}.csv')\n",
    "            df.to_csv(filename, index=False)\n",
    "            Utilities.progress_bar(i+1, len(df_list))\n",
    "\n",
    "    \n",
    "# for interactive matplotlib:\n",
    "# %matplotlib widget \n",
    "# Bigger plots\n",
    "plt.rcParams['figure.figsize'] = [10, 6]  \n",
    "plt.rcParams['font.size']= 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.632504Z",
     "iopub.status.busy": "2023-03-28T14:15:57.632310Z",
     "iopub.status.idle": "2023-03-28T14:15:57.638674Z",
     "shell.execute_reply": "2023-03-28T14:15:57.638402Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define DataExtraction class\n",
    "class DataExtraction():\n",
    "    def __init__(self, save_to_path):\n",
    "        self.save_to_path = save_to_path\n",
    "\n",
    "\n",
    "    # Extract data and store to file named Data\n",
    "    def download_data(self):\n",
    "        if database == 'Spider':\n",
    "            directory = 'Data/Spider'\n",
    "            url = 'https://physionet.org/files/ecg-spider-clip/1.0.0/'\n",
    "            if not os.path.isdir(directory):\n",
    "                print(\"Downloading database...this may take a while\")\n",
    "                os.makedirs(directory)\n",
    "                cmd = f\"wget -r -N -c -np -P {directory} {url}\"\n",
    "                print(cmd)\n",
    "                try:\n",
    "                    subprocess.run(cmd)\n",
    "                except:\n",
    "                    print(\"Error: Unable to download database\")\n",
    "                    os.rmdir(directory)\n",
    "            else:\n",
    "                print(\"Using pre-downloaded database\")\n",
    "    \n",
    "\n",
    "    # sorts data into a single dataframe for each participant into a collective dataframe list\n",
    "    def sort_data(self):\n",
    "        print(\"Sorting data...\")\n",
    "\n",
    "        # try loading existing df if available\n",
    "        if os.path.isdir(self.save_to_path):\n",
    "            return\n",
    "        \n",
    "        # otherwise create dataframe from scratch\n",
    "        ECG_df = []\n",
    "\n",
    "        if database == 'Spider':\n",
    "            database_directory = 'Data/Spider/physionet.org/files/ecg-spider-clip/1.0.0/'\n",
    "            # Exclude VP70 because of noise\n",
    "            sub_directories = ['VP02', 'VP03','VP05','VP06','VP08','VP09','VP11','VP12','VP14','VP15','VP17','VP18','VP20','VP23','VP24','VP26','VP27',\n",
    "                    'VP29','VP30','VP32','VP33','VP35','VP36','VP38','VP39','VP41','VP42','VP44','VP45','VP47','VP48','VP50','VP51','VP53',\n",
    "                    'VP54','VP56','VP57','VP59','VP61','VP62','VP63','VP64','VP65','VP66','VP68','VP69','VP71','VP72','VP73','VP74',\n",
    "                    'VP75','VP76','VP77','VP78','VP79','VP80']\n",
    "            for index, sub in enumerate(sub_directories):\n",
    "                # set path\n",
    "                ECG_file = f'{database_directory}{sub}/BitalinoECG.txt'\n",
    "                triggers_file = f'{database_directory}{sub}/Triggers.txt'\n",
    "\n",
    "                # append data to dataframe\n",
    "                ECG_participant_df = pd.read_csv(ECG_file, sep='\\t', names = ['ECG','Timestamp','NA'])\n",
    "                ECG_participant_df = ECG_participant_df.drop(columns=['NA'])\n",
    "                \n",
    "                # set the start time to use to normalize the other times\n",
    "                normalized_time = ECG_participant_df.iloc[0,1]\n",
    "                ECG_participant_df.Timestamp = ECG_participant_df.Timestamp-normalized_time\n",
    "\n",
    "                # read in trigger file\n",
    "                triggers_df_temp = pd.read_csv(triggers_file, sep='\\t', names = ['clip','on','off'])\n",
    "                # normalize time series\n",
    "                triggers_df_temp.on = triggers_df_temp.on-normalized_time\n",
    "                triggers_df_temp.off = triggers_df_temp.off-normalized_time\n",
    "\n",
    "                # Create the 'Stressed' (label) column with all zeros\n",
    "                ECG_participant_df[\"Stressed\"] = np.zeros(len(ECG_participant_df))\n",
    "                # This checks which time stamps fall into the time ranges when the clips are delivered (ignoring demo clip), results in a column of \"true\" and \"false\"\n",
    "                conditions = pd.concat([(ECG_participant_df['Timestamp'] >= triggers_df_temp.on[i]) & (ECG_participant_df['Timestamp'] <= triggers_df_temp.off[i]) for i in range(1,17)],axis=1).any(axis=1)\n",
    "                ECG_participant_df[\"Stressed\"] = conditions\n",
    "                # move stressed label to first column\n",
    "                ECG_participant_df.insert(0, \"Stressed\", ECG_participant_df.pop('Stressed'))\n",
    "\n",
    "                # append data to complete df dictionary\n",
    "                ECG_df.append(ECG_participant_df)\n",
    "\n",
    "                Utilities.progress_bar(index, len(sub_directories)-1)\n",
    "\n",
    "        # save dataframe for nextime\n",
    "        print(\"Saving sorted data...\")\n",
    "        Utilities.save_list_of_dataframes(ECG_df, self.save_to_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Data Extraction and sorting\n",
    "* Downloads data, normalizes timeframe, attaches labels, and saves sorted data to Sorted directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-downloaded database\n",
      "Sorting data...\n"
     ]
    }
   ],
   "source": [
    "de = DataExtraction(f'{directory}/Sorted')\n",
    "de.download_data()\n",
    "de.sort_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.639837Z",
     "iopub.status.busy": "2023-03-28T14:15:57.639761Z",
     "iopub.status.idle": "2023-03-28T14:15:57.644906Z",
     "shell.execute_reply": "2023-03-28T14:15:57.644649Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define PreProcessing Class:\n",
    "# Segments data using rolling window \n",
    "# Cleans data using Neurokit2\n",
    "# Documentation can be found here: https://neuropsychology.github.io/NeuroKit/functions/ecg.html       \n",
    "class PreProcessing():\n",
    "    def __init__(self, ECG_df:pd.DataFrame, sampling_rate:int):\n",
    "        self.ECG_df = ECG_df\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "    # interpolates data to achieve sampling rate\n",
    "    def interpolate(self):\n",
    "        if database == 'Spider':\n",
    "            # print(\"Timestamps are not valid for Spider database. Not interpolating.\")\n",
    "            return\n",
    "\n",
    "        # convert timestamp column to a NumPy array\n",
    "        timestamps = self.ECG_df['Timestamp'].to_numpy()\n",
    "\n",
    "        # calculate the time difference between each pair of adjacent timestamps\n",
    "        time_diff = np.diff(timestamps)\n",
    "\n",
    "        # calculate the average sampling rate of the data\n",
    "        sampling_rate = 1 / np.mean(time_diff)\n",
    "\n",
    "        print(f\"Average sampling rate: {sampling_rate}\")\n",
    "\n",
    "        # interpolate the data to obtain 100Hz sampling rate\n",
    "        self.ECG_df['Timestamp'] = np.arange(timestamps[0], timestamps[-1], 1 / self.sampling_rate)\n",
    "        self.ECG_df['ECG'] = np.interp(self.ECG_df['Timestamp'], timestamps, self.ECG_df['ECG'])\n",
    "        \n",
    "\n",
    "    # segments data with overlap using rolling window\n",
    "    def segment(self, window_length, overlap)->tuple:\n",
    "        # convert window_length in seconds to samples\n",
    "        window_samples = window_length * sampling_rate\n",
    "        # Calculate the step_size as the fraction of the total window samples\n",
    "        step_size = int(window_samples * (1-overlap)) \n",
    "\n",
    "        # Save windowed samples to dictionary containing a list of numpy arrays for each label type\n",
    "        self.sampled_ECG = {\"Stressed\": [], \"Not stressed\": []}\n",
    "\n",
    "        # Initialize starting variables\n",
    "        current_index = 0\n",
    "        current_stressed = self.ECG_df['Stressed'][current_index]\n",
    "        \n",
    "        # Loop through the entire dataframe\n",
    "        while current_index < len(self.ECG_df['ECG']):  \n",
    "            # calculate next index and exit if out of bounds          \n",
    "            next_index = current_index + step_size\n",
    "            if (next_index > len(self.ECG_df['ECG'])):\n",
    "                return\n",
    "            # Check if the window overlaps different label in next window\n",
    "            next_stressed = self.ECG_df['Stressed'][next_index]\n",
    "\n",
    "            # If the next window has a different label, update index to start of new label\n",
    "            if next_stressed != current_stressed:\n",
    "                current_index = next_index\n",
    "                current_stressed = next_stressed\n",
    "            else:\n",
    "                # Extract the window into stressed or not stressed dataframes\n",
    "                if current_stressed:\n",
    "                    self.sampled_ECG[\"Stressed\"].append(self.ECG_df['ECG'].iloc[current_index:current_index+window_samples].to_numpy())\n",
    "                else:\n",
    "                    self.sampled_ECG[\"Not stressed\"].append(self.ECG_df['ECG'].iloc[current_index:current_index+window_samples].to_numpy())\n",
    "                # If the next window has the same label, shift the window\n",
    "                current_index += step_size\n",
    "\n",
    "\n",
    "    def clean(self):\n",
    "        # Clean each sample in the stressed and not stressed data (overwrites original data)\n",
    "        # using method 'neurokit' (0.5 Hz high-pass butterworth filter (order = 5), followed by powerline filtering) but can be changed to other cleaning methods\n",
    "        for label in self.sampled_ECG.keys():\n",
    "            for sample in self.sampled_ECG[label]:\n",
    "                sample = nk.ecg_clean(sample, self.sampling_rate, method='neurokit')\n",
    "\n",
    "\n",
    "    # returns dictionary with list of samples - stressed and not stressed\n",
    "    def get_samples(self):\n",
    "        return self.sampled_ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.646170Z",
     "iopub.status.busy": "2023-03-28T14:15:57.646007Z",
     "iopub.status.idle": "2023-03-28T14:15:57.647802Z",
     "shell.execute_reply": "2023-03-28T14:15:57.647608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define parameters for feature extraction\n",
    "class FE_Parameter:\n",
    "    def __init__(self, name:str, min:float=0.0, max:float=9999):\n",
    "        self.name = name\n",
    "        self.min = min\n",
    "        self.max = max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.649031Z",
     "iopub.status.busy": "2023-03-28T14:15:57.648948Z",
     "iopub.status.idle": "2023-03-28T14:15:57.656454Z",
     "shell.execute_reply": "2023-03-28T14:15:57.656197Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define FeatureExtraction Class\n",
    "# Main class that extracts features from a dictionary of sorted dataframes and stores to csv\n",
    "class FeatureExtraction():\n",
    "    # takes in cleaned ECG data\n",
    "    def __init__(self, sampled_ECG:dict=None, show_plot=False):\n",
    "        self.show_plot = show_plot\n",
    "        self.sampled_ECG = sampled_ECG\n",
    "        # copy dictionary keys from sampled_ECG to all_features_ECG and selected_features_ECG\n",
    "        self.all_features_ECG = {label: pd.DataFrame for label in sampled_ECG.keys()} \n",
    "\n",
    "\n",
    "    def add_to_featured_df(self, df, label):\n",
    "        if self.all_features_ECG[label].empty:\n",
    "            self.all_features_ECG[label] = df\n",
    "        else:\n",
    "            self.all_features_ECG[label] = pd.concat([self.all_features_ECG[label], df], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "    # Extracts features from ECG using neurokit.\n",
    "    def get_neurokit_features(self, EDR=False):\n",
    "        for label in self.sampled_ECG.keys():\n",
    "            for index, sample in enumerate(self.sampled_ECG[label]):\n",
    "                # only show plot once\n",
    "                if index!=0:\n",
    "                    self.show_plot = False \n",
    "                # extract R-R peaks\n",
    "                np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "                r_peaks_df = nk.ecg_peaks(sample, sampling_rate=sampling_rate, correct_artifacts=True)[0]\n",
    "\n",
    "                # Extract HRV features from R-R peaks, see https://neuropsychology.github.io/NeuroKit/functions/hrv.html \n",
    "                # compute HRV - time, frequency and nonlinear indices.\n",
    "                warnings.filterwarnings('ignore') # temporarily supress warnings\n",
    "                features = nk.hrv(r_peaks_df, sampling_rate=sampling_rate, show=self.show_plot)\n",
    "                warnings.filterwarnings('default')\n",
    "                # compute Shannon Entropy (SE) using signal symbolization and discretization\n",
    "                # see https://neuropsychology.github.io/NeuroKit/functions/complexity.html#entropy-shannon \n",
    "                SE = nk.entropy_shannon(sample, symbolize='A')[0]\n",
    "                HRV_SE = pd.DataFrame([SE], columns=['HRV_SE'])\n",
    "                # concat to dataframe\n",
    "                features = pd.concat([features, HRV_SE], axis=1)\n",
    "                \n",
    "                if EDR:\n",
    "                    # Get ECG Derived Respiration (EDR) and add to the data\n",
    "                    warnings.filterwarnings('ignore') # temporarily supress warnings\n",
    "                    ecg_rate = nk.signal_rate(r_peaks_df, sampling_rate=sampling_rate, desired_length=len(r_peaks_df))\n",
    "                    warnings.filterwarnings('default')\n",
    "                    EDR_sample = nk.ecg_rsp(ecg_rate, sampling_rate=sampling_rate)\n",
    "                    info = nk.signal_findpeaks(EDR_sample)\n",
    "                    # add feature to dataframe\n",
    "                    # self.add_to_featured_df(pd.DataFrame(EDR, columns=['EDR']), label, index)\n",
    "                    \n",
    "                # concatenate to all_features_ECG dataframe\n",
    "                self.add_to_featured_df(features, label)\n",
    "\n",
    "\n",
    "    # saves selected or all features to concatenated csv\n",
    "    def save_features(self, save_to_path):\n",
    "        for label in self.all_features_ECG.keys():\n",
    "            if database == 'Spider':\n",
    "                self.all_features_ECG[label].loc[:, 'Stressed'] = 1 if label == 'Stressed' else 0\n",
    "            Utilities.save_dataframe(self.all_features_ECG[label], f'{save_to_path}/All', name=f'Featured_ECG_with_{window_length}s_Window_Length')\n",
    "                \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing and Feature Extraction\n",
    "* Cleans data using Neurokit's 5th Order Butterworth filter.\n",
    "* Extracts features such as HRV time, frequency and non-linear domain, EDR etc. and saves them to Feature directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.662551Z",
     "iopub.status.busy": "2023-03-28T14:15:57.662447Z",
     "iopub.status.idle": "2023-03-28T14:15:57.666162Z",
     "shell.execute_reply": "2023-03-28T14:15:57.665929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Feature Extraction\n"
     ]
    }
   ],
   "source": [
    "# For each participant, segment, clean and extract features from ECG recordings\n",
    "# Skip if feature file exists\n",
    "if os.path.exists(f'{directory}/Features/All/Featured_ECG_with_{window_length}s_Window_Length.csv'):\n",
    "    print(\"Skipping Feature Extraction\")\n",
    "else:\n",
    "    print(\"Feature Extraction...\")\n",
    "    for index in range(number_of_participants):\n",
    "        # show plots for first participant only\n",
    "        show_plot = True if index==0 else False\n",
    "\n",
    "        # Extract features using sorted data:\n",
    "        df = Utilities.load_dataframe(Utilities.check_csv_exists(f'{directory}/Sorted', index))\n",
    "        # interpolate, segment using sliding window, and clean data\n",
    "        pp = PreProcessing(df, sampling_rate)\n",
    "        pp.interpolate()\n",
    "        pp.segment(window_length, overlap)\n",
    "        pp.clean()\n",
    "\n",
    "        # extract feautres for each segment\n",
    "        fe = FeatureExtraction(pp.get_samples(), show_plot)\n",
    "        fe.get_neurokit_features(EDR=False)\n",
    "        fe.save_features(f'{directory}/Features')\n",
    "        Utilities.progress_bar(index+1, number_of_participants)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection:\n",
    "* Visualise most useful features\n",
    "* Select desired features, sanity check the values, and save them to Features directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define FeatureSelection class that is used to visualise and select data\n",
    "class FeatureSelection():\n",
    "    def __init__(self, path):\n",
    "        self.save_to_path = path\n",
    "        self.selected_features = Utilities.load_dataframe(f'{path}/All/Featured_ECG_with_{window_length}s_Window_Length.csv')\n",
    "\n",
    "    \n",
    "    def visualise(self, cross_features=False):\n",
    "        if cross_features:\n",
    "            # Visualize the data using seaborn Pairplots\n",
    "            sns.pairplot(self.selected_features, hue = 'Stressed', diag_bandwidth=0.2)\n",
    "        else:\n",
    "            sns.scatterplot(data=self.selected_features, hue = 'Stressed')\n",
    "\n",
    "\n",
    "    # Copies selected features from all feature csv file to selected feature csv \n",
    "    # desired_features is a list of FE_Parameter objects  \n",
    "    def select(self, desired_features:list):\n",
    "        new_selected_features = pd.DataFrame()\n",
    "        for feature in desired_features:\n",
    "            # Sanity check: check if feature exists\n",
    "            if feature.name in self.selected_features.columns:\n",
    "                # Set value to NaN if it falls outside min and max values.\n",
    "                for i, value in enumerate(self.selected_features[feature.name]):\n",
    "                    if (value < feature.min) or (value > feature.max):\n",
    "                        self.selected_features.loc[i, feature.name] = np.nan\n",
    "                # Add column to new selected features\n",
    "                new_selected_features[feature.name] = self.selected_features[feature.name]\n",
    "            else:\n",
    "                print(f'Error: No such feature \"{feature}\" in extracted features')\n",
    "        # add the label column as well\n",
    "        new_selected_features['Stressed'] = self.selected_features['Stressed']\n",
    "        # overwrite selected features with new ones selected\n",
    "        self.selected_features = new_selected_features\n",
    "\n",
    "\n",
    "    # impute missing values in dataset with mean values of column\n",
    "    def impute(self):\n",
    "        self.selected_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        imp = SimpleImputer(strategy='mean')\n",
    "        imp.fit(self.selected_features)\n",
    "        self.dataset = pd.DataFrame(imp.transform(self.selected_features))\n",
    "\n",
    "\n",
    "    def save_features(self):\n",
    "        Utilities.save_dataframe(self.selected_features, f'{self.save_to_path}/Selected', name=f'Featured_ECG_with_{window_length}s_Window_Length')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Desired Features and Visualise Cross Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.657717Z",
     "iopub.status.busy": "2023-03-28T14:15:57.657598Z",
     "iopub.status.idle": "2023-03-28T14:15:57.661341Z",
     "shell.execute_reply": "2023-03-28T14:15:57.661082Z"
    }
   },
   "outputs": [],
   "source": [
    "# See Neurokit2 HRV - https://neuropsychology.github.io/NeuroKit/functions/hrv.html\n",
    "\n",
    "# Minimum and maximum expected HR (beats per min)\n",
    "min_HR = 30\n",
    "max_HR = 200\n",
    "\n",
    "# MinNN: The minimum of the RR intervals (Parent, 2019; Subramaniam, 2022).\n",
    "HRV_MinNN = FE_Parameter('HRV_MinNN', min=60000.0/max_HR, max=60000.0/min_HR)\n",
    "# MaxNN: The maximum of the RR intervals (Parent, 2019; Subramaniam, 2022).\n",
    "HRV_MaxNN = FE_Parameter('HRV_MaxNN', min=60000.0/max_HR, max=60000.0/min_HR)\n",
    "# MeanNN: The mean of the RR intervals.\n",
    "HRV_MeanNN = FE_Parameter('HRV_MeanNN', min=60000.0/max_HR, max=60000.0/min_HR)\n",
    "\n",
    "# SDNN: The standard deviation of the RR intervals.\n",
    "# See https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5624990/ for chosen min and max values.\n",
    "HRV_SDNN = FE_Parameter('HRV_SDNN', min=30, max=150)\n",
    "# RMSSD: The square root of the mean of the squared successive differences between adjacent RR intervals. \n",
    "# # It is equivalent (although on another scale) to SD1, and therefore it is redundant to report correlations with both (Ciccone, 2017).\n",
    "# See https://help.welltory.com/en/articles/4413231-what-normal-ranges-and-measurement-standards-we-use-to-interpret-your-heart-rate-variability for chosen min and max values.\n",
    "HRV_RMSSD = FE_Parameter('HRV_RMSSD', min=13, max=107)\n",
    "# The root mean square of successive differences (RMSSD) divided by the mean of the RR intervals (MeanNN).\n",
    "CVSD = FE_Parameter('HRV_CVSD')\n",
    "# Shannon Entropy\n",
    "HRV_SE = FE_Parameter('HRV_SE')\n",
    "\n",
    "# pNN50: The proportion of RR intervals greater than 20ms, out of the total number of RR intervals.\n",
    "HRV_pNN50 = FE_Parameter('HRV_pNN20')\n",
    "# A geometrical parameter of the HRV, or more specifically, the baseline width of the RR intervals distribution \n",
    "# TINN: obtained by triangular interpolation, where the error of least squares determines the triangle. \n",
    "# It is an approximation of the RR interval distribution.\n",
    "HRV_TINN = FE_Parameter('HRV_TINN')\n",
    "# HTI: The HRV triangular index, measuring the total number of RR intervals divided by the height of the RR intervals histogram.\n",
    "HRV_HTI = FE_Parameter('HRV_HTI')\n",
    "\n",
    "# VLF: The spectral power (W/Hz) of very low frequencies (.0033 to .04 Hz).\n",
    "# HRV_VLF = FE_Parameter('HRV_VLF', min=0.0, max=9) # hidden due to use of 0.5 Hz high-pass butterworth filter\n",
    "# LF: The spectral power (W/Hz) of low frequencies (.04 to .15 Hz).\n",
    "HRV_LF = FE_Parameter('HRV_LF', max=1.00)\n",
    "# HF: The spectral power (W/Hz) of high frequencies (.15 to .4 Hz).\n",
    "HRV_HF = FE_Parameter('HRV_HF', max=1.00)\n",
    "# LFHF: The ratio obtained by dividing the low frequency power by the high frequency power.\n",
    "HRV_LFHF = FE_Parameter('HRV_LFHF', max=1.00)\n",
    "\n",
    "\n",
    "# Append all FE parameters to a list which will be passed to Feature_Extraction's Select method\n",
    "selected_features = []\n",
    "all_variables = dict(globals(), **locals())\n",
    "for name, var in all_variables.items():\n",
    "    if isinstance(var, FE_Parameter):\n",
    "        selected_features.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following variable cannot be assigned with wide-form data: `hue`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/u/g/wfp21/Documents/Stress Classifier Using ECG/ECG-Stress-Classifier.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m fs\u001b[39m.\u001b[39mimpute()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# fs.save_features()\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m fs\u001b[39m.\u001b[39;49mvisualise()\n",
      "\u001b[1;32m/u/g/wfp21/Documents/Stress Classifier Using ECG/ECG-Stress-Classifier.ipynb Cell 18\u001b[0m in \u001b[0;36mFeatureSelection.visualise\u001b[0;34m(self, cross_features)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     sns\u001b[39m.\u001b[39mpairplot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselected_features, hue \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mStressed\u001b[39m\u001b[39m'\u001b[39m, diag_bandwidth\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     sns\u001b[39m.\u001b[39;49mscatterplot(data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselected_features, hue \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mStressed\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:46\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m     37\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPass the following variable\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m as \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39mkeyword arg\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFrom version 0.12, the only valid positional argument \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     )\n\u001b[1;32m     45\u001b[0m kwargs\u001b[39m.\u001b[39mupdate({k: arg \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args)})\n\u001b[0;32m---> 46\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/seaborn/relational.py:808\u001b[0m, in \u001b[0;36mscatterplot\u001b[0;34m(x, y, hue, style, size, data, palette, hue_order, hue_norm, sizes, size_order, size_norm, markers, style_order, x_bins, y_bins, units, estimator, ci, n_boot, alpha, x_jitter, y_jitter, legend, ax, **kwargs)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[39m@_deprecate_positional_args\u001b[39m\n\u001b[1;32m    794\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscatterplot\u001b[39m(\n\u001b[1;32m    795\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    804\u001b[0m     legend\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m, ax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    805\u001b[0m ):\n\u001b[1;32m    807\u001b[0m     variables \u001b[39m=\u001b[39m _ScatterPlotter\u001b[39m.\u001b[39mget_semantics(\u001b[39mlocals\u001b[39m())\n\u001b[0;32m--> 808\u001b[0m     p \u001b[39m=\u001b[39m _ScatterPlotter(\n\u001b[1;32m    809\u001b[0m         data\u001b[39m=\u001b[39;49mdata, variables\u001b[39m=\u001b[39;49mvariables,\n\u001b[1;32m    810\u001b[0m         x_bins\u001b[39m=\u001b[39;49mx_bins, y_bins\u001b[39m=\u001b[39;49my_bins,\n\u001b[1;32m    811\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator, ci\u001b[39m=\u001b[39;49mci, n_boot\u001b[39m=\u001b[39;49mn_boot,\n\u001b[1;32m    812\u001b[0m         alpha\u001b[39m=\u001b[39;49malpha, x_jitter\u001b[39m=\u001b[39;49mx_jitter, y_jitter\u001b[39m=\u001b[39;49my_jitter, legend\u001b[39m=\u001b[39;49mlegend,\n\u001b[1;32m    813\u001b[0m     )\n\u001b[1;32m    815\u001b[0m     p\u001b[39m.\u001b[39mmap_hue(palette\u001b[39m=\u001b[39mpalette, order\u001b[39m=\u001b[39mhue_order, norm\u001b[39m=\u001b[39mhue_norm)\n\u001b[1;32m    816\u001b[0m     p\u001b[39m.\u001b[39mmap_size(sizes\u001b[39m=\u001b[39msizes, order\u001b[39m=\u001b[39msize_order, norm\u001b[39m=\u001b[39msize_norm)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/seaborn/relational.py:587\u001b[0m, in \u001b[0;36m_ScatterPlotter.__init__\u001b[0;34m(self, data, variables, x_bins, y_bins, estimator, ci, n_boot, alpha, x_jitter, y_jitter, legend)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    572\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m,\n\u001b[1;32m    573\u001b[0m     data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, variables\u001b[39m=\u001b[39m{},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[39m# the kind of plot to draw, but for the time being we need to set\u001b[39;00m\n\u001b[1;32m    582\u001b[0m     \u001b[39m# this information so the SizeMapping can use it\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_size_range \u001b[39m=\u001b[39m (\n\u001b[1;32m    584\u001b[0m         np\u001b[39m.\u001b[39mr_[\u001b[39m.5\u001b[39m, \u001b[39m2\u001b[39m] \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msquare(mpl\u001b[39m.\u001b[39mrcParams[\u001b[39m\"\u001b[39m\u001b[39mlines.markersize\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    585\u001b[0m     )\n\u001b[0;32m--> 587\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(data\u001b[39m=\u001b[39;49mdata, variables\u001b[39m=\u001b[39;49mvariables)\n\u001b[1;32m    589\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha \u001b[39m=\u001b[39m alpha\n\u001b[1;32m    590\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlegend \u001b[39m=\u001b[39m legend\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/seaborn/_core.py:605\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, variables\u001b[39m=\u001b[39m{}):\n\u001b[0;32m--> 605\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massign_variables(data, variables)\n\u001b[1;32m    607\u001b[0m     \u001b[39mfor\u001b[39;00m var, \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_semantic_mappings\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    608\u001b[0m \n\u001b[1;32m    609\u001b[0m         \u001b[39m# Create the mapping function\u001b[39;00m\n\u001b[1;32m    610\u001b[0m         map_func \u001b[39m=\u001b[39m partial(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmap, plotter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/seaborn/_core.py:663\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    662\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_format \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwide\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 663\u001b[0m     plot_data, variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_assign_variables_wideform(\n\u001b[1;32m    664\u001b[0m         data, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mvariables,\n\u001b[1;32m    665\u001b[0m     )\n\u001b[1;32m    666\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    667\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_format \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlong\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/seaborn/_core.py:712\u001b[0m, in \u001b[0;36mVectorPlotter._assign_variables_wideform\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m     err \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe following variable\u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m}\u001b[39;00m\u001b[39m cannot be assigned with wide-form data: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    711\u001b[0m     err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`\u001b[39m\u001b[39m{\u001b[39;00mv\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m assigned)\n\u001b[0;32m--> 712\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err)\n\u001b[1;32m    714\u001b[0m \u001b[39m# Determine if the data object actually has any data in it\u001b[39;00m\n\u001b[1;32m    715\u001b[0m empty \u001b[39m=\u001b[39m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(data)\n",
      "\u001b[0;31mValueError\u001b[0m: The following variable cannot be assigned with wide-form data: `hue`"
     ]
    }
   ],
   "source": [
    "# Skip if feature file exists\n",
    "if os.path.exists(f'{directory}/Features/Selected/Featured_ECG_with_{window_length}s_Window_Length.csv'):\n",
    "    print(\"Skipping Feature Selection\")\n",
    "else:\n",
    "    fs = FeatureSelection(f'{directory}/Features')\n",
    "    fs.select(selected_features)\n",
    "    fs.impute()\n",
    "    # fs.save_features()\n",
    "    fs.visualise()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Classification\n",
    "* Tunes\n",
    "* Classifies using LDA, Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.678855Z",
     "iopub.status.busy": "2023-03-28T14:15:57.678628Z",
     "iopub.status.idle": "2023-03-28T14:15:57.685391Z",
     "shell.execute_reply": "2023-03-28T14:15:57.685101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define LinearML method, which implements different linear classification methods\n",
    "class Linear_ML():\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    \n",
    "    # randomizes, standardizes, and splits into test and training data.\n",
    "    def prepare(self, test_split:float, dimension_reudction=False):       \n",
    "        # split into test and training data\n",
    "        dataset_size = len(self.dataset.columns)\n",
    "        features = self.dataset.iloc[:, 0:dataset_size-1].values\n",
    "        label = self.dataset.iloc[:,dataset_size-1].values\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(features, label, test_size=test_split, shuffle=True)\n",
    "\n",
    "        # scale \n",
    "        sc = StandardScaler()\n",
    "        self.X_train = sc.fit_transform(self.X_train)\n",
    "        self.X_test = sc.transform(self.X_test)\n",
    "\n",
    "        # LDA\n",
    "        if dimension_reudction:\n",
    "            lda = LinearDiscriminantAnalysis()\n",
    "            self.X_train = lda.fit_transform(self.X_train, self.y_train)\n",
    "            self.X_test = lda.transform(self.X_test)\n",
    "            # Define model evaluation method (k-fold cross-validation)\n",
    "            cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=None)\n",
    "\n",
    "            #evaluate model\n",
    "            scores = cross_val_score(lda, features, label, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "            # summarize result\n",
    "            print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "\n",
    "    # Tunning Random Forest\n",
    "    def rf_tuner(self):\n",
    "        n_estimators = 100\n",
    "        max_features = [1, 'sqrt', 'log2']\n",
    "        max_depths = [None, 2, 3, 4, 5]\n",
    "        for f, d in product(max_features, max_depths): # with product we can iterate through all possible combinations\n",
    "            rf = RandomForestClassifier(n_estimators=n_estimators, \n",
    "                                        criterion='entropy', \n",
    "                                        max_features=f, \n",
    "                                        max_depth=d, \n",
    "                                        n_jobs=2,\n",
    "                                        random_state=1337)\n",
    "            rf.fit(self.X_train, self.y_train)\n",
    "            self.y_pred = rf.predict(X=self.X_test)\n",
    "            print('Classification accuracy on test set with max features = {} and max_depth = {}: {:.3f}'.format(f, d, accuracy_score(self.y_test,self.y_pred)))\n",
    "            self.plot_confustion_matrix()\n",
    "\n",
    "\n",
    "    # Pre-tuned Random Forests Classifier\n",
    "    def rf_classifier(self):\n",
    "        # source: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "        rf = RandomForestClassifier(n_estimators=100, criterion='entropy')\n",
    "        rf.fit(self.X_train, self.y_train)\n",
    "        self.y_pred = rf.predict(X=self.X_test)\n",
    "\n",
    "        # Accuracy on Test\n",
    "        print(\"Training Accuracy is: \", rf.score(self.X_train, self.y_train))\n",
    "        # Accuracy on Train\n",
    "        print(\"Testing Accuracy is: \", rf.score(self.X_test, self.y_test))\n",
    "\n",
    "        self.plot_confustion_matrix()\n",
    "\n",
    "    \n",
    "    def plot_confustion_matrix(self):\n",
    "        # Confusion Matrix\n",
    "        ConfusionMatrixDisplay.from_predictions(self.y_test, self.y_pred, display_labels=['Negative', 'Positive'])\n",
    "\n",
    "\n",
    "    # main method to conduct LDA\n",
    "    def model(self, classification_type):\n",
    "        classification_types = {\n",
    "            \"RF\": self.rf_classifier\n",
    "        }\n",
    "        classification_types.get(classification_type, lambda: print(\"Invalid input\"))()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.686647Z",
     "iopub.status.busy": "2023-03-28T14:15:57.686556Z",
     "iopub.status.idle": "2023-03-28T14:15:58.903839Z",
     "shell.execute_reply": "2023-03-28T14:15:58.903584Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retreive dataset of pre-processed and featured data from csv \n",
    "dataset_path = f'{directory}/Features/Selected/Featured_ECG_with_{window_length}s_Window_Length.csv'\n",
    "dataset = Utilities.load_dataframe(dataset_path)\n",
    "\n",
    "# pass to LinearML class\n",
    "lml = Linear_ML(dataset)\n",
    "# prepare dataset, split to a 33% test split and use LDA for dimension reduction\n",
    "lml.prepare(0.33, dimension_reudction=False)\n",
    "# run Random Forest Tuner\n",
    "lml.rf_tuner()\n",
    "\n",
    "#lml.model('RF')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
