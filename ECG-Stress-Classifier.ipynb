{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stress Classifier\n",
    "## Overview\n",
    "* Data Extraction: Downloads and sorts through database\n",
    "* Signal processing: \n",
    "    + Pre-processing - filtering and signal cleaning\n",
    "    + Feature Extraction - R-R peaks, PQRST peaks, EDR, in addition to mean, kurtosis etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and Globals\n",
    "Modify settings to select database, model etc. and tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:56.740564Z",
     "iopub.status.busy": "2023-03-28T14:15:56.740124Z",
     "iopub.status.idle": "2023-03-28T14:15:56.746295Z",
     "shell.execute_reply": "2023-03-28T14:15:56.745553Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# SETTINGS\n",
    "\n",
    "# Database:\n",
    "database = \"Spider\" # database = \"BrainPatch\"\n",
    "\n",
    "# Preprocessing / Feature Extraction:\n",
    "window_length = 40 # window length in seconds\n",
    "overlap = 0.1 # overlap percentage for rolling window (increasing will result in more overlapped samples)\n",
    "\n",
    "if database == 'Spider':\n",
    "    sampling_rate = 100\n",
    "    number_of_participants = 56\n",
    "\n",
    "directory = f'Data/{database}/StoredDataFrames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:56.748730Z",
     "iopub.status.busy": "2023-03-28T14:15:56.748407Z",
     "iopub.status.idle": "2023-03-28T14:15:57.623281Z",
     "shell.execute_reply": "2023-03-28T14:15:57.623003Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import subprocess\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.624839Z",
     "iopub.status.busy": "2023-03-28T14:15:57.624743Z",
     "iopub.status.idle": "2023-03-28T14:15:57.631111Z",
     "shell.execute_reply": "2023-03-28T14:15:57.630859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Useful Utilities\n",
    "class Utilities():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def progress_bar(current, total, bar_length=20):\n",
    "        fraction = current / total\n",
    "\n",
    "        arrow = int(fraction * bar_length - 1) * '-' + '>'\n",
    "        padding = int(bar_length - len(arrow)) * ' '\n",
    "\n",
    "        ending = '\\n' if current == total else '\\r'\n",
    "\n",
    "        print(f'Progress: [{arrow}{padding}] {int(fraction*100)}%', end=ending)\n",
    "\n",
    "\n",
    "    def print_overwrite(msg):\n",
    "        # add padding to line \n",
    "        padding = ' ' * 20\n",
    "        # Print the padded message and remember it as the previous message\n",
    "        print(msg+padding, end=\"\\r\")\n",
    "\n",
    "\n",
    "    def check_csv_exists(folder_path, index):\n",
    "        # read the CSV file into a dataframe and append to the list\n",
    "        filename = os.path.join(folder_path, f'df_{index}.csv')\n",
    "        try:\n",
    "            df = pd.read_csv(filename)\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "        return filename\n",
    "\n",
    "\n",
    "    def load_dataframe(filename):\n",
    "        # read the CSV file into a dataframe and append to the list\n",
    "        df = pd.read_csv(filename)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def save_dataframe(df, folder_path, index=-1, name=''):\n",
    "        # create directoy if necessary\n",
    "        os.makedirs(folder_path, exist_ok=True) \n",
    "        # either store as an index or store as name\n",
    "        if index!=-1:\n",
    "            filename = os.path.join(folder_path, f'df_{index}.csv')\n",
    "        else:\n",
    "            filename = os.path.join(folder_path, f'{name}.csv')\n",
    "        # if filename already exists, concatenate vertically with it.\n",
    "        if os.path.exists(filename):\n",
    "            df_new = pd.concat([Utilities.load_dataframe(filename), df], axis=0, ignore_index=True)\n",
    "            df_new.to_csv(filename, index=False)\n",
    "        # otherwise, save as new file\n",
    "        else:\n",
    "            df.to_csv(filename, index=False)\n",
    "\n",
    "    \n",
    "    def save_list_of_dataframes(df_list, folder_path):\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        # save each dataframe as a CSV file in the folder\n",
    "        for i, df in enumerate(df_list):\n",
    "            filename = os.path.join(folder_path, f'df_{i}.csv')\n",
    "            df.to_csv(filename, index=False)\n",
    "            Utilities.progress_bar(i+1, len(df_list))\n",
    "\n",
    "    \n",
    "# for interactive matplotlib:\n",
    "# %matplotlib widget \n",
    "# Bigger plots\n",
    "plt.rcParams['figure.figsize'] = [10, 6]  \n",
    "plt.rcParams['font.size']= 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.632504Z",
     "iopub.status.busy": "2023-03-28T14:15:57.632310Z",
     "iopub.status.idle": "2023-03-28T14:15:57.638674Z",
     "shell.execute_reply": "2023-03-28T14:15:57.638402Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define DataExtraction class\n",
    "class DataExtraction():\n",
    "    def __init__(self, save_to_path):\n",
    "        self.save_to_path = save_to_path\n",
    "\n",
    "\n",
    "    # Extract data and store to file named Data\n",
    "    def download_data(self):\n",
    "        if database == 'Spider':\n",
    "            directory = 'Data/Spider'\n",
    "            url = 'https://physionet.org/files/ecg-spider-clip/1.0.0/'\n",
    "            if not os.path.isdir(directory):\n",
    "                print(\"Downloading database...this may take a while\")\n",
    "                os.makedirs(directory)\n",
    "                cmd = f\"wget -r -N -c -np -P {directory} {url}\"\n",
    "                print(cmd)\n",
    "                try:\n",
    "                    subprocess.run(cmd)\n",
    "                except:\n",
    "                    print(\"Error: Unable to download database\")\n",
    "                    os.rmdir(directory)\n",
    "            else:\n",
    "                print(\"Using pre-downloaded database\")\n",
    "    \n",
    "\n",
    "    # sorts data into a single dataframe for each participant into a collective dataframe list\n",
    "    def sort_data(self):\n",
    "        print(\"Sorting data...\")\n",
    "\n",
    "        # try loading existing df if available\n",
    "        if os.path.isdir(self.save_to_path):\n",
    "            return\n",
    "        \n",
    "        # otherwise create dataframe from scratch\n",
    "        ECG_df = []\n",
    "\n",
    "        if database == 'Spider':\n",
    "            database_directory = 'Data/Spider/physionet.org/files/ecg-spider-clip/1.0.0/'\n",
    "            # Exclude VP70 because of noise\n",
    "            sub_directories = ['VP02', 'VP03','VP05','VP06','VP08','VP09','VP11','VP12','VP14','VP15','VP17','VP18','VP20','VP23','VP24','VP26','VP27',\n",
    "                    'VP29','VP30','VP32','VP33','VP35','VP36','VP38','VP39','VP41','VP42','VP44','VP45','VP47','VP48','VP50','VP51','VP53',\n",
    "                    'VP54','VP56','VP57','VP59','VP61','VP62','VP63','VP64','VP65','VP66','VP68','VP69','VP71','VP72','VP73','VP74',\n",
    "                    'VP75','VP76','VP77','VP78','VP79','VP80']\n",
    "            for index, sub in enumerate(sub_directories):\n",
    "                # set path\n",
    "                ECG_file = f'{database_directory}{sub}/BitalinoECG.txt'\n",
    "                triggers_file = f'{database_directory}{sub}/Triggers.txt'\n",
    "\n",
    "                # append data to dataframe\n",
    "                ECG_participant_df = pd.read_csv(ECG_file, sep='\\t', names = ['ECG','Timestamp','NA'])\n",
    "                ECG_participant_df = ECG_participant_df.drop(columns=['NA'])\n",
    "                \n",
    "                # set the start time to use to normalize the other times\n",
    "                normalized_time = ECG_participant_df.iloc[0,1]\n",
    "                ECG_participant_df.Timestamp = ECG_participant_df.Timestamp-normalized_time\n",
    "\n",
    "                # read in trigger file\n",
    "                triggers_df_temp = pd.read_csv(triggers_file, sep='\\t', names = ['clip','on','off'])\n",
    "                # normalize time series\n",
    "                triggers_df_temp.on = triggers_df_temp.on-normalized_time\n",
    "                triggers_df_temp.off = triggers_df_temp.off-normalized_time\n",
    "\n",
    "                # Create the 'Stressed' (label) column with all zeros\n",
    "                ECG_participant_df[\"Stressed\"] = np.zeros(len(ECG_participant_df))\n",
    "                # This checks which time stamps fall into the time ranges when the clips are delivered (ignoring demo clip), results in a column of \"true\" and \"false\"\n",
    "                conditions = pd.concat([(ECG_participant_df['Timestamp'] >= triggers_df_temp.on[i]) & (ECG_participant_df['Timestamp'] <= triggers_df_temp.off[i]) for i in range(1,17)],axis=1).any(axis=1)\n",
    "                ECG_participant_df[\"Stressed\"] = conditions\n",
    "                # move stressed label to first column\n",
    "                ECG_participant_df.insert(0, \"Stressed\", ECG_participant_df.pop('Stressed'))\n",
    "\n",
    "                # append data to complete df dictionary\n",
    "                ECG_df.append(ECG_participant_df)\n",
    "\n",
    "                Utilities.progress_bar(index, len(sub_directories)-1)\n",
    "\n",
    "        # save dataframe for nextime\n",
    "        print(\"Saving sorted data...\")\n",
    "        Utilities.save_list_of_dataframes(ECG_df, self.save_to_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Data Extraction and sorting\n",
    "* Downloads data, normalizes timeframe, attaches labels, and saves sorted data to Sorted directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-downloaded database\n",
      "Sorting data...\n"
     ]
    }
   ],
   "source": [
    "de = DataExtraction(f'{directory}/Sorted')\n",
    "de.download_data()\n",
    "de.sort_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.639837Z",
     "iopub.status.busy": "2023-03-28T14:15:57.639761Z",
     "iopub.status.idle": "2023-03-28T14:15:57.644906Z",
     "shell.execute_reply": "2023-03-28T14:15:57.644649Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define PreProcessing Class:\n",
    "# Segments data using rolling window \n",
    "# Cleans data using Neurokit2\n",
    "# Documentation can be found here: https://neuropsychology.github.io/NeuroKit/functions/ecg.html       \n",
    "class PreProcessing():\n",
    "    def __init__(self, ECG_df:pd.DataFrame, sampling_rate:int):\n",
    "        self.ECG_df = ECG_df\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "    # interpolates data to achieve sampling rate\n",
    "    def interpolate(self):\n",
    "        if database == 'Spider':\n",
    "            # print(\"Timestamps are not valid for Spider database. Not interpolating.\")\n",
    "            return\n",
    "\n",
    "        # convert timestamp column to a NumPy array\n",
    "        timestamps = self.ECG_df['Timestamp'].to_numpy()\n",
    "\n",
    "        # calculate the time difference between each pair of adjacent timestamps\n",
    "        time_diff = np.diff(timestamps)\n",
    "\n",
    "        # calculate the average sampling rate of the data\n",
    "        sampling_rate = 1 / np.mean(time_diff)\n",
    "\n",
    "        print(f\"Average sampling rate: {sampling_rate}\")\n",
    "\n",
    "        # interpolate the data to obtain 100Hz sampling rate\n",
    "        self.ECG_df['Timestamp'] = np.arange(timestamps[0], timestamps[-1], 1 / self.sampling_rate)\n",
    "        self.ECG_df['ECG'] = np.interp(self.ECG_df['Timestamp'], timestamps, self.ECG_df['ECG'])\n",
    "        \n",
    "\n",
    "    # segments data with overlap using rolling window\n",
    "    def segment(self, window_length, overlap)->tuple:\n",
    "        # convert window_length in seconds to samples\n",
    "        window_samples = window_length * sampling_rate\n",
    "        # Calculate the step_size as the fraction of the total window samples\n",
    "        step_size = int(window_samples * (1-overlap)) \n",
    "\n",
    "        # Save windowed samples to dictionary containing a list of numpy arrays for each label type\n",
    "        self.sampled_ECG = {\"Stressed\": [], \"Not stressed\": []}\n",
    "\n",
    "        # Initialize starting variables\n",
    "        current_index = 0\n",
    "        current_stressed = self.ECG_df['Stressed'][current_index]\n",
    "        \n",
    "        # Loop through the entire dataframe\n",
    "        while current_index < len(self.ECG_df['ECG']):  \n",
    "            # calculate next index and exit if out of bounds          \n",
    "            next_index = current_index + step_size\n",
    "            if (next_index > len(self.ECG_df['ECG'])):\n",
    "                return\n",
    "            # Check if the window overlaps different label in next window\n",
    "            next_stressed = self.ECG_df['Stressed'][next_index]\n",
    "\n",
    "            # If the next window has a different label, update index to start of new label\n",
    "            if next_stressed != current_stressed:\n",
    "                current_index = next_index\n",
    "                current_stressed = next_stressed\n",
    "            else:\n",
    "                # Extract the window into stressed or not stressed dataframes\n",
    "                if current_stressed:\n",
    "                    self.sampled_ECG[\"Stressed\"].append(self.ECG_df['ECG'].iloc[current_index:current_index+window_samples].to_numpy())\n",
    "                else:\n",
    "                    self.sampled_ECG[\"Not stressed\"].append(self.ECG_df['ECG'].iloc[current_index:current_index+window_samples].to_numpy())\n",
    "                # If the next window has the same label, shift the window\n",
    "                current_index += step_size\n",
    "\n",
    "\n",
    "    def clean(self):\n",
    "        # Clean each sample in the stressed and not stressed data (overwrites original data)\n",
    "        # using method 'neurokit' (0.5 Hz high-pass butterworth filter (order = 5), followed by powerline filtering) but can be changed to other cleaning methods\n",
    "        for label in self.sampled_ECG.keys():\n",
    "            for sample in self.sampled_ECG[label]:\n",
    "                sample = nk.ecg_clean(sample, self.sampling_rate, method='neurokit')\n",
    "\n",
    "\n",
    "    # returns dictionary with list of samples - stressed and not stressed\n",
    "    def get_samples(self):\n",
    "        return self.sampled_ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.646170Z",
     "iopub.status.busy": "2023-03-28T14:15:57.646007Z",
     "iopub.status.idle": "2023-03-28T14:15:57.647802Z",
     "shell.execute_reply": "2023-03-28T14:15:57.647608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define parameters for feature extraction\n",
    "class FE_Parameter:\n",
    "    def __init__(self, name:str, min:float=0.0, max:float=9999):\n",
    "        self.name = name\n",
    "        self.min = min\n",
    "        self.max = max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.649031Z",
     "iopub.status.busy": "2023-03-28T14:15:57.648948Z",
     "iopub.status.idle": "2023-03-28T14:15:57.656454Z",
     "shell.execute_reply": "2023-03-28T14:15:57.656197Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define FeatureExtraction Class\n",
    "# Main class that extracts features from a dictionary of sorted dataframes and stores to csv\n",
    "class FeatureExtraction():\n",
    "    # takes in cleaned ECG data\n",
    "    def __init__(self, sampled_ECG:dict=None, show_plot=False):\n",
    "        self.show_plot = show_plot\n",
    "        self.sampled_ECG = sampled_ECG\n",
    "        # copy dictionary keys from sampled_ECG to all_features_ECG and selected_features_ECG\n",
    "        self.all_features_ECG = {label: pd.DataFrame for label in sampled_ECG.keys()} \n",
    "\n",
    "\n",
    "    def add_to_featured_df(self, df, label):\n",
    "        if self.all_features_ECG[label].empty:\n",
    "            self.all_features_ECG[label] = df\n",
    "        else:\n",
    "            self.all_features_ECG[label] = pd.concat([self.all_features_ECG[label], df], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "    # Extracts features from ECG using neurokit.\n",
    "    def get_neurokit_features(self, EDR=False):\n",
    "        for label in self.sampled_ECG.keys():\n",
    "            for index, sample in enumerate(self.sampled_ECG[label]):\n",
    "                # only show plot once\n",
    "                if index!=0:\n",
    "                    self.show_plot = False \n",
    "                # extract R-R peaks\n",
    "                np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "                r_peaks_df = nk.ecg_peaks(sample, sampling_rate=sampling_rate, correct_artifacts=True)[0]\n",
    "\n",
    "                # Extract HRV features from R-R peaks, see https://neuropsychology.github.io/NeuroKit/functions/hrv.html \n",
    "                # compute HRV - time, frequency and nonlinear indices.\n",
    "                warnings.filterwarnings('ignore') # temporarily supress warnings\n",
    "                features = nk.hrv(r_peaks_df, sampling_rate=sampling_rate, show=self.show_plot)\n",
    "                warnings.filterwarnings('default')\n",
    "                # compute Shannon Entropy (SE) using signal symbolization and discretization\n",
    "                # see https://neuropsychology.github.io/NeuroKit/functions/complexity.html#entropy-shannon \n",
    "                SE = nk.entropy_shannon(sample, symbolize='A')[0]\n",
    "                HRV_SE = pd.DataFrame([SE], columns=['HRV_SE'])\n",
    "                # concat to dataframe\n",
    "                features = pd.concat([features, HRV_SE], axis=1)\n",
    "                \n",
    "                if EDR:\n",
    "                    # Get ECG Derived Respiration (EDR) and add to the data\n",
    "                    warnings.filterwarnings('ignore') # temporarily supress warnings\n",
    "                    ecg_rate = nk.signal_rate(r_peaks_df, sampling_rate=sampling_rate, desired_length=len(r_peaks_df))\n",
    "                    warnings.filterwarnings('default')\n",
    "                    EDR_sample = nk.ecg_rsp(ecg_rate, sampling_rate=sampling_rate)\n",
    "                    info = nk.signal_findpeaks(EDR_sample)\n",
    "                    # add feature to dataframe\n",
    "                    # self.add_to_featured_df(pd.DataFrame(EDR, columns=['EDR']), label, index)\n",
    "                    \n",
    "                # concatenate to all_features_ECG dataframe\n",
    "                self.add_to_featured_df(features, label)\n",
    "\n",
    "\n",
    "    # saves selected or all features to concatenated csv\n",
    "    def save_features(self, save_to_path):\n",
    "        for label in self.all_features_ECG.keys():\n",
    "            if database == 'Spider':\n",
    "                self.all_features_ECG[label].loc[:, 'Stressed'] = 1 if label == 'Stressed' else 0\n",
    "            Utilities.save_dataframe(self.all_features_ECG[label], f'{save_to_path}/All', name=f'Featured_ECG_with_{window_length}s_Window_Length')\n",
    "                \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing and Feature Extraction\n",
    "* Cleans data using Neurokit's 5th Order Butterworth filter.\n",
    "* Extracts features such as HRV time, frequency and non-linear domain, EDR etc. and saves them to Feature directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.662551Z",
     "iopub.status.busy": "2023-03-28T14:15:57.662447Z",
     "iopub.status.idle": "2023-03-28T14:15:57.666162Z",
     "shell.execute_reply": "2023-03-28T14:15:57.665929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Feature Extraction\n"
     ]
    }
   ],
   "source": [
    "# For each participant, segment, clean and extract features from ECG recordings\n",
    "# Skip if feature file exists\n",
    "if os.path.exists(f'{directory}/Features/All/Featured_ECG_with_{window_length}s_Window_Length.csv'):\n",
    "    print(\"Skipping Feature Extraction\")\n",
    "else:\n",
    "    print(\"Feature Extraction...\")\n",
    "    for index in range(number_of_participants):\n",
    "        # show plots for first participant only\n",
    "        show_plot = True if index==0 else False\n",
    "\n",
    "        # Extract features using sorted data:\n",
    "        df = Utilities.load_dataframe(Utilities.check_csv_exists(f'{directory}/Sorted', index))\n",
    "        # interpolate, segment using sliding window, and clean data\n",
    "        pp = PreProcessing(df, sampling_rate)\n",
    "        pp.interpolate()\n",
    "        pp.segment(window_length, overlap)\n",
    "        pp.clean()\n",
    "\n",
    "        # extract feautres for each segment\n",
    "        fe = FeatureExtraction(pp.get_samples(), show_plot)\n",
    "        fe.get_neurokit_features(EDR=False)\n",
    "        fe.save_features(f'{directory}/Features')\n",
    "        Utilities.progress_bar(index+1, number_of_participants)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection:\n",
    "* Visualise most useful features\n",
    "* Select desired features, sanity check the values, and save them to Features directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define FeatureSelection class that is used to visualise and select data\n",
    "class FeatureSelection():\n",
    "    def __init__(self, path):\n",
    "        self.save_to_path = path\n",
    "        self.selected_features = Utilities.load_dataframe(f'{path}/All/Featured_ECG_with_{window_length}s_Window_Length.csv')\n",
    "\n",
    "    \n",
    "    def visualise(self, plot_type='pairplot'):\n",
    "        print(\"Generating plot...\")\n",
    "        if plot_type == 'pairplot':         \n",
    "            sns.pairplot(data = self.selected_features, hue = 'Stressed')\n",
    "        elif plot_type == 'kdeplot':\n",
    "            # Create a figure with subplots for each feature\n",
    "            subplot_size = math.ceil(math.sqrt(len(self.selected_features.columns) - 1))\n",
    "            fig = plt.figure(figsize=(20, 8*subplot_size))\n",
    "\n",
    "            # Loop through each feature and add it to a subplot\n",
    "            for i, feature in enumerate(self.selected_features.loc[:, self.selected_features.columns != 'Stressed']):\n",
    "                fig.add_subplot(subplot_size, subplot_size, i+1)\n",
    "                sns.kdeplot(x=feature, data=self.selected_features, hue='Stressed', common_norm=False, warn_singular=False)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Plot type not recognised. Please choose between pairplot, kdeplot\")\n",
    "\n",
    "\n",
    "    # Copies selected features from all feature csv file to selected feature csv \n",
    "    # desired_features is a list of FE_Parameter objects  \n",
    "    def select(self, desired_features:list):\n",
    "        new_selected_features = pd.DataFrame()\n",
    "        for feature in desired_features:\n",
    "            # Sanity check: check if feature exists\n",
    "            if feature.name in self.selected_features.columns:\n",
    "                # Set value to NaN if it falls outside min and max values.\n",
    "                for i, value in enumerate(self.selected_features[feature.name]):\n",
    "                    if (value < feature.min) or (value > feature.max):\n",
    "                        self.selected_features.loc[i, feature.name] = np.nan\n",
    "                # Add column to new selected features\n",
    "                new_selected_features[feature.name] = self.selected_features[feature.name]\n",
    "            else:\n",
    "                print(f'Error: No such feature \"{feature}\" in extracted features')\n",
    "        # add the label column as well\n",
    "        new_selected_features['Stressed'] = self.selected_features['Stressed']\n",
    "        # overwrite selected features with new ones selected\n",
    "        self.selected_features = new_selected_features\n",
    "\n",
    "\n",
    "    # impute missing values in dataset with mean values of column\n",
    "    def impute(self):\n",
    "        self.selected_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        imp = SimpleImputer(strategy='mean')\n",
    "        imp.fit(self.selected_features)\n",
    "        self.selected_features = pd.DataFrame(imp.transform(self.selected_features))\n",
    "\n",
    "\n",
    "    def save_features(self):\n",
    "        Utilities.save_dataframe(self.selected_features, f'{self.save_to_path}/Selected', name=f'Featured_ECG_with_{window_length}s_Window_Length')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Desired Features and Visualise Cross Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.657717Z",
     "iopub.status.busy": "2023-03-28T14:15:57.657598Z",
     "iopub.status.idle": "2023-03-28T14:15:57.661341Z",
     "shell.execute_reply": "2023-03-28T14:15:57.661082Z"
    }
   },
   "outputs": [],
   "source": [
    "# See Neurokit2 HRV - https://neuropsychology.github.io/NeuroKit/functions/hrv.html\n",
    "\n",
    "# Minimum and maximum expected HR (beats per min)\n",
    "min_HR = 30\n",
    "max_HR = 200\n",
    "\n",
    "# MinNN: The minimum of the RR intervals (Parent, 2019; Subramaniam, 2022).\n",
    "HRV_MinNN = FE_Parameter('HRV_MinNN', min=60000.0/max_HR, max=60000.0/min_HR)\n",
    "# MaxNN: The maximum of the RR intervals (Parent, 2019; Subramaniam, 2022).\n",
    "HRV_MaxNN = FE_Parameter('HRV_MaxNN', min=60000.0/max_HR, max=60000.0/min_HR)\n",
    "# MeanNN: The mean of the RR intervals.\n",
    "HRV_MeanNN = FE_Parameter('HRV_MeanNN', min=60000.0/max_HR, max=60000.0/min_HR)\n",
    "\n",
    "# SDNN: The standard deviation of the RR intervals.\n",
    "# See https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5624990/ for chosen min and max values.\n",
    "HRV_SDNN = FE_Parameter('HRV_SDNN', min=30, max=150)\n",
    "# RMSSD: The square root of the mean of the squared successive differences between adjacent RR intervals. \n",
    "# # It is equivalent (although on another scale) to SD1, and therefore it is redundant to report correlations with both (Ciccone, 2017).\n",
    "# See https://help.welltory.com/en/articles/4413231-what-normal-ranges-and-measurement-standards-we-use-to-interpret-your-heart-rate-variability for chosen min and max values.\n",
    "HRV_RMSSD = FE_Parameter('HRV_RMSSD', min=13, max=107)\n",
    "# The root mean square of successive differences (RMSSD) divided by the mean of the RR intervals (MeanNN).\n",
    "CVSD = FE_Parameter('HRV_CVSD')\n",
    "# Shannon Entropy\n",
    "HRV_SE = FE_Parameter('HRV_SE')\n",
    "\n",
    "# pNN50: The proportion of RR intervals greater than 20ms, out of the total number of RR intervals.\n",
    "HRV_pNN50 = FE_Parameter('HRV_pNN20')\n",
    "# A geometrical parameter of the HRV, or more specifically, the baseline width of the RR intervals distribution \n",
    "# TINN: obtained by triangular interpolation, where the error of least squares determines the triangle. \n",
    "# It is an approximation of the RR interval distribution.\n",
    "HRV_TINN = FE_Parameter('HRV_TINN')\n",
    "# HTI: The HRV triangular index, measuring the total number of RR intervals divided by the height of the RR intervals histogram.\n",
    "HRV_HTI = FE_Parameter('HRV_HTI')\n",
    "\n",
    "# VLF: The spectral power (W/Hz) of very low frequencies (.0033 to .04 Hz).\n",
    "# HRV_VLF = FE_Parameter('HRV_VLF', min=0.0, max=9) # hidden due to use of 0.5 Hz high-pass butterworth filter\n",
    "# LF: The spectral power (W/Hz) of low frequencies (.04 to .15 Hz).\n",
    "HRV_LF = FE_Parameter('HRV_LF', max=1.00)\n",
    "# HF: The spectral power (W/Hz) of high frequencies (.15 to .4 Hz).\n",
    "HRV_HF = FE_Parameter('HRV_HF', max=1.00)\n",
    "# LFHF: The ratio obtained by dividing the low frequency power by the high frequency power.\n",
    "HRV_LFHF = FE_Parameter('HRV_LFHF', max=1.00)\n",
    "\n",
    "\n",
    "# Append all FE parameters to a list which will be passed to Feature_Extraction's Select method\n",
    "selected_features = []\n",
    "all_variables = dict(globals(), **locals())\n",
    "for name, var in all_variables.items():\n",
    "    if isinstance(var, FE_Parameter):\n",
    "        selected_features.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Feature Selection\n"
     ]
    }
   ],
   "source": [
    "# Skip if feature file exists\n",
    "if os.path.exists(f'{directory}/Features/Selected/Featured_ECG_with_{window_length}s_Window_Length.csv'):\n",
    "    print(\"Skipping Feature Selection\")\n",
    "else:\n",
    "    fs = FeatureSelection(f'{directory}/Features')\n",
    "    fs.select(selected_features)\n",
    "    fs.impute()\n",
    "    # fs.visualise(plot_type='kdeplot')\n",
    "    # fs.visualise(plot_type='pairplot')\n",
    "    fs.save_features()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Classification\n",
    "* Tunes\n",
    "* Classifies using LDA, Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.678855Z",
     "iopub.status.busy": "2023-03-28T14:15:57.678628Z",
     "iopub.status.idle": "2023-03-28T14:15:57.685391Z",
     "shell.execute_reply": "2023-03-28T14:15:57.685101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define LinearML method, which implements different linear classification methods\n",
    "class Linear_ML():\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    \n",
    "    # randomizes, standardizes, and splits into test and training data.\n",
    "    def prepare(self, test_split:float, dimension_reudction=False):       \n",
    "        dataset_size = len(self.dataset.columns)\n",
    "        features = self.dataset.iloc[:, 0:dataset_size-1].values\n",
    "        label = self.dataset.iloc[:,dataset_size-1].values\n",
    "        # split into test, training and validation data (hold-out validation)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(features, label, train_size = 0.8, test_size=0.2, random_state=15)\n",
    "        self.X_train, self.X_valid, self.y_train, self.y_valid = train_test_split(self.X_train, self.y_train, train_size = 0.9, test_size=0.1, random_state=15)\n",
    "        \n",
    "        # check distribution of stress to not stressed is even for each split:\n",
    "        stressed_dataset = (np.count_nonzero(label == 1) / (np.count_nonzero(label == 1) + np.count_nonzero(label == 0))) * 100\n",
    "        stressed_y_test = (np.count_nonzero(self.y_test == 1) / (np.count_nonzero(self.y_test == 1) + np.count_nonzero(self.y_test == 0))) * 100\n",
    "        stressed_y_train = (np.count_nonzero(self.y_train == 1) / (np.count_nonzero(self.y_train == 1) + np.count_nonzero(self.y_train == 0))) * 100\n",
    "        stressed_y_valid = (np.count_nonzero(self.y_valid == 1) / (np.count_nonzero(self.y_valid == 1) + np.count_nonzero(self.y_valid == 0))) * 100\n",
    "\n",
    "        print(f'Stressed Percentage for dataset is {stressed_dataset:.2f}%')\n",
    "        print(f'Stressed Percentage for test data is {stressed_y_test:.2f}%')\n",
    "        print(f'Stressed Percentage for train data is {stressed_y_train:.2f}%')\n",
    "        print(f'Stressed Percentage for valid data is {stressed_y_valid:.2f}%')\n",
    "\n",
    "        # scale \n",
    "        sc = StandardScaler()\n",
    "        self.X_train = sc.fit_transform(self.X_train)\n",
    "        self.X_test = sc.transform(self.X_test)\n",
    "\n",
    "        # LDA\n",
    "        if dimension_reudction:\n",
    "            lda = LinearDiscriminantAnalysis()\n",
    "            self.X_train = lda.fit_transform(self.X_train, self.y_train)\n",
    "            self.X_test = lda.transform(self.X_test)\n",
    "            # Define model evaluation method (k-fold cross-validation)\n",
    "            cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=None)\n",
    "\n",
    "            #evaluate model\n",
    "            scores = cross_val_score(lda, features, label, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "            # summarize result\n",
    "            print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "\n",
    "    # Tunning Random Forest\n",
    "    def rf_tuner(self):\n",
    "        for e in range(0, 1000, 10):\n",
    "            n_estimators = 100\n",
    "            max_features = [1, 'sqrt', 'log2']\n",
    "            max_depths = [None, 2, 3, 4, 5]\n",
    "            for f, d in product(max_features, max_depths): # with product we can iterate through all possible combinations\n",
    "                rf = RandomForestClassifier(n_estimators=n_estimators, \n",
    "                                            criterion='entropy', \n",
    "                                            max_features=f, \n",
    "                                            max_depth=d, \n",
    "                                            n_jobs=-1,\n",
    "                                            random_state=1337)\n",
    "                rf.fit(self.X_train, self.y_train)\n",
    "                self.y_pred = rf.predict(X=self.X_test)\n",
    "                print('Classification accuracy on test set with estimators = {} max features = {} and max_depth = {}: {:.3f}'.format(e, f, d, accuracy_score(self.y_test,self.y_pred)))\n",
    "                # self.plot_confustion_matrix()\n",
    "\n",
    "\n",
    "    # Pre-tuned Random Forests Classifier\n",
    "    def rf_classifier(self):\n",
    "        # source: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "        rf = RandomForestClassifier(n_estimators=100, criterion='entropy')\n",
    "        rf.fit(self.X_train, self.y_train)\n",
    "        self.y_pred = rf.predict(X=self.X_test)\n",
    "\n",
    "        # Accuracy on Test\n",
    "        print(\"Training Accuracy is: \", rf.score(self.X_train, self.y_train))\n",
    "        # Accuracy on Train\n",
    "        print(\"Testing Accuracy is: \", rf.score(self.X_test, self.y_test))\n",
    "\n",
    "        self.plot_confustion_matrix()\n",
    "\n",
    "    \n",
    "    def plot_confustion_matrix(self):\n",
    "        # Confusion Matrix\n",
    "        ConfusionMatrixDisplay.from_predictions(self.y_test, self.y_pred, display_labels=['Negative', 'Positive'])\n",
    "\n",
    "\n",
    "    # main method to conduct LDA\n",
    "    def model(self, classification_type):\n",
    "        classification_types = {\n",
    "            \"RF\": self.rf_classifier\n",
    "        }\n",
    "        classification_types.get(classification_type, lambda: print(\"Invalid input\"))()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.686647Z",
     "iopub.status.busy": "2023-03-28T14:15:57.686556Z",
     "iopub.status.idle": "2023-03-28T14:15:58.903839Z",
     "shell.execute_reply": "2023-03-28T14:15:58.903584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stressed Percentage for dataset is 39.82%\n",
      "Stressed Percentage for test data is 38.67%\n",
      "Stressed Percentage for train data is 39.47%\n",
      "Stressed Percentage for valid data is 45.83%\n",
      "Classification accuracy on test set with estimators = 0 max features = 1 and max_depth = None: 0.642\n",
      "Classification accuracy on test set with estimators = 0 max features = 1 and max_depth = 2: 0.613\n",
      "Classification accuracy on test set with estimators = 0 max features = 1 and max_depth = 3: 0.613\n",
      "Classification accuracy on test set with estimators = 0 max features = 1 and max_depth = 4: 0.613\n",
      "Classification accuracy on test set with estimators = 0 max features = 1 and max_depth = 5: 0.611\n",
      "Classification accuracy on test set with estimators = 0 max features = sqrt and max_depth = None: 0.609\n",
      "Classification accuracy on test set with estimators = 0 max features = sqrt and max_depth = 2: 0.613\n",
      "Classification accuracy on test set with estimators = 0 max features = sqrt and max_depth = 3: 0.613\n",
      "Classification accuracy on test set with estimators = 0 max features = sqrt and max_depth = 4: 0.609\n",
      "Classification accuracy on test set with estimators = 0 max features = sqrt and max_depth = 5: 0.622\n",
      "Classification accuracy on test set with estimators = 0 max features = log2 and max_depth = None: 0.609\n",
      "Classification accuracy on test set with estimators = 0 max features = log2 and max_depth = 2: 0.613\n",
      "Classification accuracy on test set with estimators = 0 max features = log2 and max_depth = 3: 0.613\n",
      "Classification accuracy on test set with estimators = 0 max features = log2 and max_depth = 4: 0.609\n",
      "Classification accuracy on test set with estimators = 0 max features = log2 and max_depth = 5: 0.622\n",
      "Classification accuracy on test set with estimators = 10 max features = 1 and max_depth = None: 0.642\n",
      "Classification accuracy on test set with estimators = 10 max features = 1 and max_depth = 2: 0.613\n",
      "Classification accuracy on test set with estimators = 10 max features = 1 and max_depth = 3: 0.613\n",
      "Classification accuracy on test set with estimators = 10 max features = 1 and max_depth = 4: 0.613\n",
      "Classification accuracy on test set with estimators = 10 max features = 1 and max_depth = 5: 0.611\n",
      "Classification accuracy on test set with estimators = 10 max features = sqrt and max_depth = None: 0.609\n",
      "Classification accuracy on test set with estimators = 10 max features = sqrt and max_depth = 2: 0.613\n",
      "Classification accuracy on test set with estimators = 10 max features = sqrt and max_depth = 3: 0.613\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/u/g/wfp21/Documents/Stress Classifier Using ECG/ECG-Stress-Classifier.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m lml\u001b[39m.\u001b[39mprepare(\u001b[39m0.33\u001b[39m, dimension_reudction\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# run Random Forest Tuner\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m lml\u001b[39m.\u001b[39;49mrf_tuner()\n",
      "\u001b[1;32m/u/g/wfp21/Documents/Stress Classifier Using ECG/ECG-Stress-Classifier.ipynb Cell 21\u001b[0m in \u001b[0;36mLinear_ML.rf_tuner\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mfor\u001b[39;00m f, d \u001b[39min\u001b[39;00m product(max_features, max_depths): \u001b[39m# with product we can iterate through all possible combinations\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     rf \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39mn_estimators, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m                                 criterion\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mentropy\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m                                 max_features\u001b[39m=\u001b[39mf, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m                                 max_depth\u001b[39m=\u001b[39md, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m                                 n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m                                 random_state\u001b[39m=\u001b[39m\u001b[39m1337\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m     rf\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mX_train, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my_train)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_pred \u001b[39m=\u001b[39m rf\u001b[39m.\u001b[39mpredict(X\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX_test)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mClassification accuracy on test set with estimators = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m max features = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and max_depth = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(e, f, d, accuracy_score(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_test,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_pred)))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:450\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    439\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[1;32m    440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m    441\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    442\u001b[0m ]\n\u001b[1;32m    444\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[1;32m    451\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[1;32m    452\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    453\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_joblib_parallel_args(prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    454\u001b[0m )(\n\u001b[1;32m    455\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[1;32m    456\u001b[0m         t,\n\u001b[1;32m    457\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    458\u001b[0m         X,\n\u001b[1;32m    459\u001b[0m         y,\n\u001b[1;32m    460\u001b[0m         sample_weight,\n\u001b[1;32m    461\u001b[0m         i,\n\u001b[1;32m    462\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[1;32m    463\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    464\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m    465\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[1;32m    466\u001b[0m     )\n\u001b[1;32m    467\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[1;32m    468\u001b[0m )\n\u001b[1;32m    470\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1057\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    936\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    573\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 574\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Retreive dataset of pre-processed and featured data from csv \n",
    "dataset_path = f'{directory}/Features/Selected/Featured_ECG_with_{window_length}s_Window_Length.csv'\n",
    "dataset = Utilities.load_dataframe(dataset_path)\n",
    "\n",
    "# pass to LinearML class\n",
    "lml = Linear_ML(dataset)\n",
    "# prepare dataset, split to a 33% test split and use LDA for dimension reduction\n",
    "lml.prepare(0.33, dimension_reudction=False)\n",
    "# run Random Forest Tuner\n",
    "lml.rf_tuner()\n",
    "\n",
    "#lml.model('RF')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
