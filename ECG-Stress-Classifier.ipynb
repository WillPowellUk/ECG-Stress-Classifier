{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stress Classifier\n",
    "## Overview\n",
    "* Data Extraction: Downloads and sorts through database\n",
    "* Signal processing: \n",
    "    + Pre-processing - filtering and signal cleaning\n",
    "    + Feature Extraction - R-R peaks, PQRST peaks, EDR, in addition to mean, kurtosis etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and Globals\n",
    "Modify settings to select database, model etc. and tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SETTINGS\n",
    "\n",
    "# Database:\n",
    "database = \"Spider\" # database = \"BrainPatch\"\n",
    "\n",
    "# Preprocessing / Feature Extraction:\n",
    "window_length = 30 # window length in seconds\n",
    "overlap = 0.1 # overlap percentage for rolling window (increasing will result in more overlapped samples)\n",
    "\n",
    "if database == 'Spider':\n",
    "    sampling_rate = 100\n",
    "    number_of_participants = 56\n",
    "\n",
    "directory = f'Data/{database}/StoredDataFrames/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Utilities\n",
    "class Utilities():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def progress_bar(current, total, bar_length=20):\n",
    "        fraction = current / total\n",
    "\n",
    "        arrow = int(fraction * bar_length - 1) * '-' + '>'\n",
    "        padding = int(bar_length - len(arrow)) * ' '\n",
    "\n",
    "        ending = '\\n' if current == total else '\\r'\n",
    "\n",
    "        print(f'Progress: [{arrow}{padding}] {int(fraction*100)}%', end=ending)\n",
    "\n",
    "\n",
    "    def print_overwrite(msg):\n",
    "        # Calculate the length of the previous message\n",
    "        prev_msg_length = len(Utilities.print_overwrite.prev_msg) if hasattr(Utilities.print_overwrite, \"prev_msg\") else 0\n",
    "        \n",
    "        # Pad the new message with spaces to the length of the previous message\n",
    "        padded_msg = msg.ljust(prev_msg_length)\n",
    "        \n",
    "        # Print the padded message and remember it as the previous message\n",
    "        print(padded_msg, end=\"\\r\")\n",
    "        Utilities.print_overwrite.prev_msg = padded_msg\n",
    "\n",
    "\n",
    "    def save_dataframe(df, folder_path, index):\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        filename = os.path.join(folder_path, f'df_{index}.csv')\n",
    "        df.to_csv(filename, index=False)\n",
    "\n",
    "    \n",
    "    def save_list_of_dataframes(df_list, folder_path):\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        # save each dataframe as a CSV file in the folder\n",
    "        for i, df in enumerate(df_list):\n",
    "            filename = os.path.join(folder_path, f'df_{i}.csv')\n",
    "            df.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "    def check_csv_exists(folder_path, index):\n",
    "        # read the CSV file into a dataframe and append to the list\n",
    "        filename = os.path.join(folder_path, f'df_{index}.csv')\n",
    "        try:\n",
    "            df = pd.read_csv(filename)\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "        return filename\n",
    "\n",
    "\n",
    "    def load_dataframe(filename):\n",
    "        # read the CSV file into a dataframe and append to the list\n",
    "        df = pd.read_csv(filename)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def merge_dataset_columns(df1, df2):\n",
    "        # concatenate the two dataframes with the new dataframe in between\n",
    "        return pd.concat([df1, df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataExtraction class\n",
    "class DataExtraction():\n",
    "    def __init__(self, save_to_path):\n",
    "        self.save_to_path = save_to_path\n",
    "\n",
    "\n",
    "    # Extract data and store to file named Data\n",
    "    def download_data(self):\n",
    "        if database == 'Spider':\n",
    "            directory = 'Data/Spider'\n",
    "            url = 'https://physionet.org/files/ecg-spider-clip/1.0.0/'\n",
    "            if not os.path.isdir(directory):\n",
    "                print(\"Downloading database...this may take a while\")\n",
    "                os.makedirs(directory)\n",
    "                cmd = f\"wget -r -N -c -np -P {directory} {url}\"\n",
    "                print(cmd)\n",
    "                try:\n",
    "                    subprocess.run(cmd)\n",
    "                except:\n",
    "                    print(\"Error: Unable to download database\")\n",
    "                    os.rmdir(directory)\n",
    "            else:\n",
    "                print(\"Using pre-downloaded database\")\n",
    "    \n",
    "\n",
    "    # sorts data into a single dataframe for each participant into a collective dataframe list\n",
    "    def sort_data(self):\n",
    "        print(\"Sorting data...\")\n",
    "\n",
    "        # try loading existing df if available\n",
    "        if os.path.isdir(self.save_to_path):\n",
    "            return\n",
    "        \n",
    "        # otherwise create dataframe from scratch\n",
    "        ECG_df = []\n",
    "\n",
    "        if database == 'Spider':\n",
    "            database_directory = 'Data/Spider/physionet.org/files/ecg-spider-clip/1.0.0/'\n",
    "            # Exclude VP70 because of noise\n",
    "            sub_directories = ['VP02', 'VP03','VP05','VP06','VP08','VP09','VP11','VP12','VP14','VP15','VP17','VP18','VP20','VP23','VP24','VP26','VP27',\n",
    "                    'VP29','VP30','VP32','VP33','VP35','VP36','VP38','VP39','VP41','VP42','VP44','VP45','VP47','VP48','VP50','VP51','VP53',\n",
    "                    'VP54','VP56','VP57','VP59','VP61','VP62','VP63','VP64','VP65','VP66','VP68','VP69','VP71','VP72','VP73','VP74',\n",
    "                    'VP75','VP76','VP77','VP78','VP79','VP80']\n",
    "            for index, sub in enumerate(sub_directories):\n",
    "                # set path\n",
    "                ECG_file = f'{database_directory}{sub}/BitalinoECG.txt'\n",
    "                triggers_file = f'{database_directory}{sub}/Triggers.txt'\n",
    "\n",
    "                # append data to dataframe\n",
    "                ECG_participant_df = pd.read_csv(ECG_file, sep='\\t', names = ['ECG','Timestamp','NA'])\n",
    "                ECG_participant_df = ECG_participant_df.drop(columns=['NA'])\n",
    "                \n",
    "                # set the start time to use to normalize the other times\n",
    "                normalized_time = ECG_participant_df.iloc[0,1]\n",
    "                ECG_participant_df.Timestamp = ECG_participant_df.Timestamp-normalized_time\n",
    "\n",
    "                # read in trigger file\n",
    "                triggers_df_temp = pd.read_csv(triggers_file, sep='\\t', names = ['clip','on','off'])\n",
    "                triggers_df_temp.on = triggers_df_temp.on-normalized_time\n",
    "                triggers_df_temp.off = triggers_df_temp.off-normalized_time\n",
    "\n",
    "                # Create the 'Stressed' (label) column with all zeros\n",
    "                ECG_participant_df[\"Stressed\"] = np.zeros(len(ECG_participant_df))\n",
    "                # This checks which time stamps fall into the time ranges when the clips are delivered, results in a column of \"true\" and \"false\"\n",
    "                conditions = pd.concat([(ECG_participant_df['Timestamp'] >= triggers_df_temp.on[i]) & (ECG_participant_df['Timestamp'] <= triggers_df_temp.off[i]) for i in range(0,17)],axis=1).any(axis=1)\n",
    "                ECG_participant_df[\"Stressed\"] = conditions\n",
    "                # move stressed label to first column\n",
    "                ECG_participant_df.insert(0, \"Stressed\", ECG_participant_df.pop('Stressed'))\n",
    "\n",
    "                # append data to complete df dictionary\n",
    "                ECG_df.append(ECG_participant_df)\n",
    "\n",
    "                Utilities.progress_bar(index, len(sub_directories)-1)\n",
    "\n",
    "        # save dataframe for nextime\n",
    "        Utilities.save_list_of_dataframes(ECG_df, self.save_to_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PreProcessing Class:\n",
    "# Segments data using rolling window \n",
    "# Cleans data using Neurokit2\n",
    "# Documentation can be found here: https://neuropsychology.github.io/NeuroKit/functions/ecg.html       \n",
    "class PreProcessing():\n",
    "    def __init__(self, ECG_df, sampling_rate):\n",
    "        self.ECG_df = ECG_df\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "\n",
    "    def segment(self, window_length, overlap)->tuple:\n",
    "        Utilities.print_overwrite(\"Segmenting data...\")\n",
    "        # convert window_length in seconds to samples\n",
    "        window_samples = window_length * sampling_rate\n",
    "        # Calculate the step_size as the fraction of the total window samples\n",
    "        step_size = int(window_samples * (1-overlap)) \n",
    "\n",
    "        # Save windowed samples to numpy array\n",
    "        self.stressed_ECG = []\n",
    "        self.not_stressed_ECG = []\n",
    "\n",
    "        # Initialize starting variables\n",
    "        current_index = 0\n",
    "        current_stressed = self.ECG_df['Stressed'][current_index]\n",
    "        \n",
    "        # Loop through the entire dataframe\n",
    "        while current_index < len(self.ECG_df['ECG']):  \n",
    "            # calculate next index and exit if out of bounds          \n",
    "            next_index = current_index + step_size\n",
    "            if (next_index > len(self.ECG_df['ECG'])):\n",
    "                return\n",
    "            # Check if the window overlaps different label in next window\n",
    "            next_stressed = self.ECG_df['Stressed'][next_index]\n",
    "\n",
    "            # If the next window has a different label, update index to start of new label\n",
    "            if next_stressed != current_stressed:\n",
    "                current_index = next_index\n",
    "                current_stressed = next_stressed\n",
    "            else:\n",
    "                # Extract the window into stressed or not stressed dataframes\n",
    "                if current_stressed:\n",
    "                    self.stressed_ECG.append(self.ECG_df['ECG'].iloc[current_index:current_index+window_samples].to_numpy())\n",
    "                else:\n",
    "                    self.not_stressed_ECG.append(self.ECG_df['ECG'].iloc[current_index:current_index+window_samples].to_numpy())\n",
    "                # If the next window has the same label, shift the window\n",
    "                current_index += step_size\n",
    "\n",
    "\n",
    "    def clean(self):\n",
    "        Utilities.print_overwrite(\"Cleaning data...\")\n",
    "\n",
    "        # Clean each sample in the stressed and not stressed data by overwriting\n",
    "        # using method 'neurokit' (0.5 Hz high-pass butterworth filter (order = 5), followed by powerline filtering) but can be changed to other cleaning methods\n",
    "        for sample in self.stressed_ECG:\n",
    "            sample = nk.ecg_clean(sample, self.sampling_rate, method='neurokit')\n",
    "        for sample in self.not_stressed_ECG:\n",
    "            sample = nk.ecg_clean(sample, self.sampling_rate, method='neurokit')\n",
    "\n",
    "    # returns list of samples - stressed and not stressed\n",
    "    def get_samples(self):\n",
    "        return (self.stressed_ECG, self.not_stressed_ECG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define FeatureExtraction Class               \n",
    "class FeatureExtraction():\n",
    "    # takes in cleaned ECG data\n",
    "    def __init__(self, sampled_ECG):\n",
    "        self.sampled_ECG = sampled_ECG\n",
    "\n",
    "\n",
    "    # Extracts features from ECG using neurokit.\n",
    "    def neurokit_features(self, HRV=True, EDR=True):  \n",
    "        for labelled_samples in self.sampled_ECG:  \n",
    "            for sample in labelled_samples:    \n",
    "                # extract R-R peaks\n",
    "                np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "                r_peaks_df = nk.ecg_peaks(sample, sampling_rate=sampling_rate, correct_artifacts=True)[0]\n",
    "\n",
    "                # Extract HRV features from R-R peaks, see https://neuropsychology.github.io/NeuroKit/functions/hrv.html \n",
    "                if HRV:\n",
    "                    # compute HRV - time, frequency and nonlinear indices.\n",
    "                    HRV_df = nk.hrv(r_peaks_df, sampling_rate=sampling_rate, show=True)\n",
    "                    # concatenate with original data and save to file\n",
    "                    self.ECG_df = Utilities.merge_dataset_columns(self.ECG_df, HRV_df)\n",
    "                \n",
    "                if EDR:\n",
    "                    # Get ECG Derived Respiration (EDR) and add to the data\n",
    "                    ecg_rate = nk.signal_rate(r_peaks_df, sampling_rate=sampling_rate, desired_length=len(r_peaks_df))\n",
    "                    EDR = nk.ecg_rsp(ecg_rate, sampling_rate=sampling_rate)\n",
    "                    self.ECG_df['EDR'] = EDR\n",
    "\n",
    "\n",
    "    # # select features for model using exisiting dataframe\n",
    "    # def select_features(self, list):       \n",
    "    #     # print current headers to console\n",
    "    #     print(\"Current headers:\", list(self.ECG_df[0].columns))\n",
    "    #     # prompt user to select headers for new dataframe\n",
    "    #     selected_headers = input(\"Enter features to include in new dataframe (separated by commas and no spaces): \").strip().split(\",\")\n",
    "\n",
    "    #     print(\"Selecting features...\")\n",
    "    #     # create a new dataframe with selected columns (always include label stressed)\n",
    "    #     self.ECG_df[index] = self.ECG_df[index].loc[:, selected_headers + ['Stressed']]\n",
    "    #     Utilities.progress_bar(index+1, len(self.ECG_df))\n",
    "        \n",
    "    #     # save dataframe for nextime\n",
    "    #     file_path = f'Data/{database}/StoredDataFrames/SelectedFeatures'\n",
    "    #     Utilities.save_list_of_dataframes(self.ECG_df, file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction and Pre-processing\n",
    "* Downloads data, normalizes timeframe and puts data into a dataframe dictionary of all partcipant data - `ECG_df`.\n",
    "* Cleans data using Neurokit's 5th Order Butterworth filter.\n",
    "* Extracts features such as HRV time, frequency and non-linear domain, EDR etc. \n",
    "\n",
    "This will take a while if you haven't previously ran this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-downloaded database\n",
      "Sorting data...\n",
      "Pre-processing Data...\n",
      "Cleaning data...  \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/u/g/wfp21/Documents/Stress Classifier Using ECG/ECG-Stress-Classifier.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     pp\u001b[39m.\u001b[39msegment(window_length, overlap)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     pp\u001b[39m.\u001b[39mclean()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m fe \u001b[39m=\u001b[39m FeatureExtraction(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mdirectory\u001b[39m}\u001b[39;49;00m\u001b[39mPreProcessed\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mdirectory\u001b[39m}\u001b[39;49;00m\u001b[39mFeaturesExtracted\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m fe\u001b[39m.\u001b[39mfeature_extraction(HRV\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, EDR\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux.bath.ac.uk/u/g/wfp21/Documents/Stress%20Classifier%20Using%20ECG/ECG-Stress-Classifier.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m Utilities\u001b[39m.\u001b[39mprogress_bar(index\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, number_of_participants)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# Download and sort database to Sorted Directory\n",
    "de = DataExtraction(f'{directory}Sorted')\n",
    "de.download_data()\n",
    "de.sort_data()\n",
    "\n",
    "# For each participant, segment, clean and extract data\n",
    "print(\"Pre-processing data...\")\n",
    "for index in range(number_of_participants):\n",
    "    # skip if features already extracted, otherwise use extract using sorted data\n",
    "    if not Utilities.check_csv_exists(f'{directory}FeaturesExtracted', index):\n",
    "        df = Utilities.load_dataframe(Utilities.check_csv_exists(f'{directory}Sorted', index))\n",
    "\n",
    "        # segment using sliding window and clean data\n",
    "        pp = PreProcessing(df, sampling_rate)\n",
    "        pp.segment(window_length, overlap)\n",
    "        pp.clean()\n",
    "        sampled_ECG = pp.get_samples()\n",
    "        \n",
    "        # extract feautres for each segment\n",
    "        fe = FeatureExtraction(sampled_ECG)\n",
    "        fe.extract(HRV=True, EDR=False)\n",
    "        Utilities.progress_bar(index+1, number_of_participants)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
