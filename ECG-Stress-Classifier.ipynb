{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stress Classifier\n",
    "## Overview\n",
    "* Data Extraction: Downloads and sorts through database\n",
    "* Signal processing: \n",
    "    + Pre-processing - filtering and signal cleaning\n",
    "    + Feature Extraction - R-R peaks, PQRST peaks, EDR, in addition to mean, kurtosis etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and Globals\n",
    "Modify settings to select database, model etc. and tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SETTINGS\n",
    "\n",
    "# Database:\n",
    "database = \"Spider\" # database = \"BrainPatch\"\n",
    "\n",
    "# Preprocessing / Feature Extraction:\n",
    "window_length = 40 # window length in seconds\n",
    "overlap = 0.1 # overlap percentage for rolling window (increasing will result in more overlapped samples)\n",
    "\n",
    "if database == 'Spider':\n",
    "    sampling_rate = 100\n",
    "    number_of_participants = 56\n",
    "\n",
    "directory = f'Data/{database}/StoredDataFrames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neurokit2 as nk\n",
    "# for interactive matplotlib\n",
    "# %matplotlib widget \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import subprocess\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Utilities\n",
    "class Utilities():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def progress_bar(current, total, bar_length=20):\n",
    "        fraction = current / total\n",
    "\n",
    "        arrow = int(fraction * bar_length - 1) * '-' + '>'\n",
    "        padding = int(bar_length - len(arrow)) * ' '\n",
    "\n",
    "        ending = '\\n' if current == total else '\\r'\n",
    "\n",
    "        print(f'Progress: [{arrow}{padding}] {int(fraction*100)}%', end=ending)\n",
    "\n",
    "\n",
    "    def print_overwrite(msg):\n",
    "        # add padding to line \n",
    "        padding = ' ' * 20\n",
    "        # Print the padded message and remember it as the previous message\n",
    "        print(msg+padding, end=\"\\r\")\n",
    "\n",
    "\n",
    "    def save_dataframe(df, folder_path, index):\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        filename = os.path.join(folder_path, f'df_{index}.csv')\n",
    "        df.to_csv(filename, index=False)\n",
    "\n",
    "    \n",
    "    def save_list_of_dataframes(df_list, folder_path):\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        # save each dataframe as a CSV file in the folder\n",
    "        for i, df in enumerate(df_list):\n",
    "            filename = os.path.join(folder_path, f'df_{i}.csv')\n",
    "            df.to_csv(filename, index=False)\n",
    "            Utilities.progress_bar(i+1, len(df_list))\n",
    "\n",
    "\n",
    "    def check_csv_exists(folder_path, index):\n",
    "        # read the CSV file into a dataframe and append to the list\n",
    "        filename = os.path.join(folder_path, f'df_{index}.csv')\n",
    "        try:\n",
    "            df = pd.read_csv(filename)\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "        return filename\n",
    "\n",
    "\n",
    "    def load_dataframe(filename):\n",
    "        # read the CSV file into a dataframe and append to the list\n",
    "        df = pd.read_csv(filename)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def merge_dataset_columns(df1, df2):\n",
    "        # concatenate the two dataframes with the new dataframe in between\n",
    "        return pd.concat([df1, df2], axis=1)\n",
    "    \n",
    "# Bigger plots\n",
    "plt.rcParams['figure.figsize'] = [10, 6]  \n",
    "plt.rcParams['font.size']= 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataExtraction class\n",
    "class DataExtraction():\n",
    "    def __init__(self, save_to_path):\n",
    "        self.save_to_path = save_to_path\n",
    "\n",
    "\n",
    "    # Extract data and store to file named Data\n",
    "    def download_data(self):\n",
    "        if database == 'Spider':\n",
    "            directory = 'Data/Spider'\n",
    "            url = 'https://physionet.org/files/ecg-spider-clip/1.0.0/'\n",
    "            if not os.path.isdir(directory):\n",
    "                print(\"Downloading database...this may take a while\")\n",
    "                os.makedirs(directory)\n",
    "                cmd = f\"wget -r -N -c -np -P {directory} {url}\"\n",
    "                print(cmd)\n",
    "                try:\n",
    "                    subprocess.run(cmd)\n",
    "                except:\n",
    "                    print(\"Error: Unable to download database\")\n",
    "                    os.rmdir(directory)\n",
    "            else:\n",
    "                print(\"Using pre-downloaded database\")\n",
    "    \n",
    "\n",
    "    # sorts data into a single dataframe for each participant into a collective dataframe list\n",
    "    def sort_data(self):\n",
    "        print(\"Sorting data...\")\n",
    "\n",
    "        # try loading existing df if available\n",
    "        if os.path.isdir(self.save_to_path):\n",
    "            return\n",
    "        \n",
    "        # otherwise create dataframe from scratch\n",
    "        ECG_df = []\n",
    "\n",
    "        if database == 'Spider':\n",
    "            database_directory = 'Data/Spider/physionet.org/files/ecg-spider-clip/1.0.0/'\n",
    "            # Exclude VP70 because of noise\n",
    "            sub_directories = ['VP02', 'VP03','VP05','VP06','VP08','VP09','VP11','VP12','VP14','VP15','VP17','VP18','VP20','VP23','VP24','VP26','VP27',\n",
    "                    'VP29','VP30','VP32','VP33','VP35','VP36','VP38','VP39','VP41','VP42','VP44','VP45','VP47','VP48','VP50','VP51','VP53',\n",
    "                    'VP54','VP56','VP57','VP59','VP61','VP62','VP63','VP64','VP65','VP66','VP68','VP69','VP71','VP72','VP73','VP74',\n",
    "                    'VP75','VP76','VP77','VP78','VP79','VP80']\n",
    "            for index, sub in enumerate(sub_directories):\n",
    "                # set path\n",
    "                ECG_file = f'{database_directory}{sub}/BitalinoECG.txt'\n",
    "                triggers_file = f'{database_directory}{sub}/Triggers.txt'\n",
    "\n",
    "                # append data to dataframe\n",
    "                ECG_participant_df = pd.read_csv(ECG_file, sep='\\t', names = ['ECG','Timestamp','NA'])\n",
    "                ECG_participant_df = ECG_participant_df.drop(columns=['NA'])\n",
    "                \n",
    "                # set the start time to use to normalize the other times\n",
    "                normalized_time = ECG_participant_df.iloc[0,1]\n",
    "                ECG_participant_df.Timestamp = ECG_participant_df.Timestamp-normalized_time\n",
    "\n",
    "                # read in trigger file\n",
    "                triggers_df_temp = pd.read_csv(triggers_file, sep='\\t', names = ['clip','on','off'])\n",
    "                # normalize time series\n",
    "                triggers_df_temp.on = triggers_df_temp.on-normalized_time\n",
    "                triggers_df_temp.off = triggers_df_temp.off-normalized_time\n",
    "\n",
    "                # Create the 'Stressed' (label) column with all zeros\n",
    "                ECG_participant_df[\"Stressed\"] = np.zeros(len(ECG_participant_df))\n",
    "                # This checks which time stamps fall into the time ranges when the clips are delivered (ignoring demo clip), results in a column of \"true\" and \"false\"\n",
    "                conditions = pd.concat([(ECG_participant_df['Timestamp'] >= triggers_df_temp.on[i]) & (ECG_participant_df['Timestamp'] <= triggers_df_temp.off[i]) for i in range(1,17)],axis=1).any(axis=1)\n",
    "                ECG_participant_df[\"Stressed\"] = conditions\n",
    "                # move stressed label to first column\n",
    "                ECG_participant_df.insert(0, \"Stressed\", ECG_participant_df.pop('Stressed'))\n",
    "\n",
    "                # append data to complete df dictionary\n",
    "                ECG_df.append(ECG_participant_df)\n",
    "\n",
    "                Utilities.progress_bar(index, len(sub_directories)-1)\n",
    "\n",
    "        # save dataframe for nextime\n",
    "        print(\"Saving sorted data...\")\n",
    "        Utilities.save_list_of_dataframes(ECG_df, self.save_to_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PreProcessing Class:\n",
    "# Segments data using rolling window \n",
    "# Cleans data using Neurokit2\n",
    "# Documentation can be found here: https://neuropsychology.github.io/NeuroKit/functions/ecg.html       \n",
    "class PreProcessing():\n",
    "    def __init__(self, ECG_df:pd.DataFrame, sampling_rate:int):\n",
    "        self.ECG_df = ECG_df\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "    # interpolates data to achieve sampling rate\n",
    "    def interpolate(self):\n",
    "        if database == 'Spider':\n",
    "            # print(\"Timestamps are not valid for Spider database. Not interpolating.\")\n",
    "            return\n",
    "\n",
    "        # convert timestamp column to a NumPy array\n",
    "        timestamps = self.ECG_df['Timestamp'].to_numpy()\n",
    "\n",
    "        # calculate the time difference between each pair of adjacent timestamps\n",
    "        time_diff = np.diff(timestamps)\n",
    "\n",
    "        # calculate the average sampling rate of the data\n",
    "        sampling_rate = 1 / np.mean(time_diff)\n",
    "\n",
    "        print(f\"Average sampling rate: {sampling_rate}\")\n",
    "\n",
    "        # interpolate the data to obtain 100Hz sampling rate\n",
    "        self.ECG_df['Timestamp'] = np.arange(timestamps[0], timestamps[-1], 1 / self.sampling_rate)\n",
    "        self.ECG_df['ECG'] = np.interp(self.ECG_df['Timestamp'], timestamps, self.ECG_df['ECG'])\n",
    "        \n",
    "\n",
    "    # segments data with overlap using rolling window\n",
    "    def segment(self, window_length, overlap)->tuple:\n",
    "        Utilities.print_overwrite(\"Segmenting data...\")\n",
    "        # convert window_length in seconds to samples\n",
    "        window_samples = window_length * sampling_rate\n",
    "        # Calculate the step_size as the fraction of the total window samples\n",
    "        step_size = int(window_samples * (1-overlap)) \n",
    "\n",
    "        # Save windowed samples to dictionary containing a list of numpy arrays for each label type\n",
    "        self.sampled_ECG = {\"Stressed\": [], \"Not stressed\": []}\n",
    "\n",
    "        # Initialize starting variables\n",
    "        current_index = 0\n",
    "        current_stressed = self.ECG_df['Stressed'][current_index]\n",
    "        \n",
    "        # Loop through the entire dataframe\n",
    "        while current_index < len(self.ECG_df['ECG']):  \n",
    "            # calculate next index and exit if out of bounds          \n",
    "            next_index = current_index + step_size\n",
    "            if (next_index > len(self.ECG_df['ECG'])):\n",
    "                return\n",
    "            # Check if the window overlaps different label in next window\n",
    "            next_stressed = self.ECG_df['Stressed'][next_index]\n",
    "\n",
    "            # If the next window has a different label, update index to start of new label\n",
    "            if next_stressed != current_stressed:\n",
    "                current_index = next_index\n",
    "                current_stressed = next_stressed\n",
    "            else:\n",
    "                # Extract the window into stressed or not stressed dataframes\n",
    "                if current_stressed:\n",
    "                    self.sampled_ECG[\"Stressed\"].append(self.ECG_df['ECG'].iloc[current_index:current_index+window_samples].to_numpy())\n",
    "                else:\n",
    "                    self.sampled_ECG[\"Not stressed\"].append(self.ECG_df['ECG'].iloc[current_index:current_index+window_samples].to_numpy())\n",
    "                # If the next window has the same label, shift the window\n",
    "                current_index += step_size\n",
    "\n",
    "\n",
    "    def clean(self):\n",
    "        Utilities.print_overwrite(\"Cleaning data...\")\n",
    "        # Clean each sample in the stressed and not stressed data (overwrites original data)\n",
    "        # using method 'neurokit' (0.5 Hz high-pass butterworth filter (order = 5), followed by powerline filtering) but can be changed to other cleaning methods\n",
    "        for label in self.sampled_ECG.keys():\n",
    "            for sample in self.sampled_ECG[label]:\n",
    "                sample = nk.ecg_clean(sample, self.sampling_rate, method='neurokit')\n",
    "\n",
    "\n",
    "    # returns dictionary with list of samples - stressed and not stressed\n",
    "    def get_samples(self):\n",
    "        return self.sampled_ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for feature extraction\n",
    "class FE_Parameter:\n",
    "    def __init__(self, name:str, min:float=0.0, max:float=9999):\n",
    "        self.name = name\n",
    "        self.min = min\n",
    "        self.max = max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define FeatureExtraction Class\n",
    "# Main class that extracts features from a dictionary of sorted dataframes and stores to csv\n",
    "class FeatureExtraction():\n",
    "    # takes in cleaned ECG data\n",
    "    def __init__(self, sampled_ECG:dict, show_plot=False):\n",
    "        self.show_plot = show_plot\n",
    "        self.sampled_ECG = sampled_ECG\n",
    "        # copy dictionary keys and size from sampled_ECG to all_features_ECG and selected_features_ECG\n",
    "        self.all_features_ECG = {label: [] for label in sampled_ECG.keys()} \n",
    "        self.selected_features_ECG = {label: [] for label in sampled_ECG.keys()} \n",
    "\n",
    "\n",
    "    def add_to_featured_df(self, df, label, index, selected=False):\n",
    "        if selected:\n",
    "            # add dataframe to dictionary if df does not exist \n",
    "            try:\n",
    "                self.selected_features_ECG[label][index]\n",
    "            except IndexError:\n",
    "                self.selected_features_ECG[label].insert(index, df)\n",
    "                return\n",
    "            # otherwise if it does exist, concatenate with previous dataframe\n",
    "            self.selected_features_ECG[label][index] = pd.concat([self.selected_features_ECG[label][index], df], axis=1)\n",
    "        else:\n",
    "            # add dataframe to dictionary if df does not exist \n",
    "            try:\n",
    "                self.all_features_ECG[label][index]\n",
    "            except IndexError:\n",
    "                self.all_features_ECG[label].insert(index, df)\n",
    "                return\n",
    "            # otherwise if it does exist, concatenate with previous dataframe\n",
    "            self.all_features_ECG[label][index] = pd.concat([self.all_features_ECG[label][index], df], axis=1)\n",
    "\n",
    "\n",
    "    # Extracts features from ECG using neurokit.\n",
    "    def get_neurokit_features(self, HRV=True, EDR=True):\n",
    "        for label in self.sampled_ECG.keys():\n",
    "            for index, sample in enumerate(self.sampled_ECG[label]):\n",
    "                # only show plot once\n",
    "                if index!=0:\n",
    "                    self.show_plot = False \n",
    "                # extract R-R peaks\n",
    "                np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "                r_peaks_df = nk.ecg_peaks(sample, sampling_rate=sampling_rate, correct_artifacts=True)[0]\n",
    "\n",
    "                # Extract HRV features from R-R peaks, see https://neuropsychology.github.io/NeuroKit/functions/hrv.html \n",
    "                if HRV:\n",
    "                    Utilities.print_overwrite(\"HRV Extraction...\")    \n",
    "                    # compute HRV - time, frequency and nonlinear indices.\n",
    "                    warnings.filterwarnings('ignore') # temporarily supress warnings\n",
    "                    HRV_df = nk.hrv(r_peaks_df, sampling_rate=sampling_rate, show=self.show_plot)\n",
    "                    warnings.filterwarnings('default')\n",
    "                    # compute Shannon Entropy (SE) using signal symbolization and discretization\n",
    "                    # see https://neuropsychology.github.io/NeuroKit/functions/complexity.html#entropy-shannon \n",
    "                    SE = nk.entropy_shannon(sample, symbolize='A')[0]\n",
    "                    HRV_SE = pd.DataFrame([SE], columns=['HRV_SE'])\n",
    "                    # add features to dataframe\n",
    "                    self.add_to_featured_df(HRV_df, label, index)\n",
    "                    self.add_to_featured_df(HRV_SE, label, index)\n",
    "\n",
    "                if EDR:\n",
    "                    Utilities.print_overwrite(\"EDR Extraction...\")    \n",
    "                    # Get ECG Derived Respiration (EDR) and add to the data\n",
    "                    warnings.filterwarnings('ignore') # temporarily supress warnings\n",
    "                    ecg_rate = nk.signal_rate(r_peaks_df, sampling_rate=sampling_rate, desired_length=len(r_peaks_df))\n",
    "                    warnings.filterwarnings('default')\n",
    "                    EDR_sample = nk.ecg_rsp(ecg_rate, sampling_rate=sampling_rate)\n",
    "                    info = nk.signal_findpeaks(EDR_sample)\n",
    "                    # add feature to dataframe\n",
    "                    # self.add_to_featured_df(pd.DataFrame(EDR, columns=['EDR']), label, index)\n",
    "    \n",
    "\n",
    "    # Copies selected features from all feature dataframe to selected feature dataframe \n",
    "    # desired_features is a list of FE_Parameter objects  \n",
    "    def select(self, desired_features:list):\n",
    "        for label in self.sampled_ECG.keys():\n",
    "            for index in range(len(self.sampled_ECG[label])):\n",
    "                for feature in desired_features:\n",
    "                    # Sanity check: check if feature exists and ignore if it's range falls outside min and max values.\n",
    "                    if feature.name in self.all_features_ECG[label][index].columns:\n",
    "                        value = self.all_features_ECG[label][index][feature.name][0]\n",
    "                        if (value < feature.min) or (value > feature.max):\n",
    "                            self.all_features_ECG[label][index][feature.name][0] = np.NaN\n",
    "                        self.add_to_featured_df(self.all_features_ECG[label][index][feature.name], label, index, selected=True)\n",
    "                    else:\n",
    "                        print(f'No such feature \"{feature}\" in extracted features')\n",
    "\n",
    "\n",
    "    def save_features(self, save_to_path, features='All'):\n",
    "        for label in self.sampled_ECG.keys():\n",
    "            if features == 'All':\n",
    "                Utilities.save_list_of_dataframes(self.all_features_ECG[label], f'{save_to_path}/{label}')\n",
    "            elif features == 'Selected':\n",
    "                Utilities.save_list_of_dataframes(self.selected_features_ECG[label], f'{save_to_path}/{label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to Extract:\n",
    "# See Neurokit2 HRV - https://neuropsychology.github.io/NeuroKit/functions/hrv.html\n",
    "\n",
    "# Minimum and maximum expected HR (beats per min)\n",
    "min_HR = 30\n",
    "max_HR = 200\n",
    "\n",
    "# MinNN: The minimum of the RR intervals (Parent, 2019; Subramaniam, 2022).\n",
    "HRV_MinNN = FE_Parameter('HRV_MinNN', min=60000.0/max_HR, max=60000.0/min_HR)\n",
    "# MaxNN: The maximum of the RR intervals (Parent, 2019; Subramaniam, 2022).\n",
    "HRV_MaxNN = FE_Parameter('HRV_MaxNN', min=60000.0/max_HR, max=60000.0/min_HR)\n",
    "# MeanNN: The mean of the RR intervals.\n",
    "HRV_MeanNN = FE_Parameter('HRV_MeanNN', min=60000.0/max_HR, max=60000.0/min_HR)\n",
    "\n",
    "# SDNN: The standard deviation of the RR intervals.\n",
    "# See https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5624990/ for chosen min and max values.\n",
    "HRV_SDNN = FE_Parameter('HRV_SDNN', min=30, max=150)\n",
    "# RMSSD: The square root of the mean of the squared successive differences between adjacent RR intervals. \n",
    "# # It is equivalent (although on another scale) to SD1, and therefore it is redundant to report correlations with both (Ciccone, 2017).\n",
    "# See https://help.welltory.com/en/articles/4413231-what-normal-ranges-and-measurement-standards-we-use-to-interpret-your-heart-rate-variability for chosen min and max values.\n",
    "HRV_RMSSD = FE_Parameter('HRV_RMSSD', min=13, max=107)\n",
    "HRV_SE = FE_Parameter('HRV_SE')\n",
    "\n",
    "# VLF: The spectral power (W/Hz) of very low frequencies (.0033 to .04 Hz).\n",
    "# HRV_VLF = FE_Parameter('HRV_VLF', min=0.0, max=9) # hidden due to use of 0.5 Hz high-pass butterworth filter\n",
    "# LF: The spectral power (W/Hz) of low frequencies (.04 to .15 Hz).\n",
    "HRV_LF = FE_Parameter('HRV_LF', max=1.00)\n",
    "# HF: The spectral power (W/Hz) of high frequencies (.15 to .4 Hz).\n",
    "HRV_HF = FE_Parameter('HRV_HF', max=1.00)\n",
    "# LFHF: The ratio obtained by dividing the low frequency power by the high frequency power.\n",
    "HRV_LFHF = FE_Parameter('HRV_LFHF', max=1.00)\n",
    "\n",
    "# pNN50: The proportion of RR intervals greater than 50ms, out of the total number of RR intervals.\n",
    "HRV_pNN50 = FE_Parameter('HRV_pNN50', max=1.00)\n",
    "\n",
    "# Append all FE parameters to a list which will be passed to Feature_Extraction's Select method\n",
    "selected_features = []\n",
    "all_variables = dict(globals(), **locals())\n",
    "for name, var in all_variables.items():\n",
    "    if isinstance(var, FE_Parameter):\n",
    "        selected_features.append(var)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction and Pre-Processing\n",
    "* Downloads data, normalizes timeframe and puts data into a dataframe dictionary of all partcipant data - `ECG_df`.\n",
    "* Cleans data using Neurokit's 5th Order Butterworth filter.\n",
    "* Extracts features such as HRV time, frequency and non-linear domain, EDR etc. \n",
    "\n",
    "This will take a while if you haven't previously ran this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-downloaded database\n",
      "Sorting data...\n",
      "Pre-processing data...\n",
      "EDR Extraction...                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/g/wfp21/.local/lib/python3.9/site-packages/neurokit2/signal/signal_interpolate.py:102: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if np.all(x_values == x_new):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDR Extraction...                    \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/g/wfp21/.local/lib/python3.9/site-packages/neurokit2/signal/signal_interpolate.py:102: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if np.all(x_values == x_new):\n"
     ]
    }
   ],
   "source": [
    "# Download and sort database to Sorted Directory\n",
    "de = DataExtraction(f'{directory}/Sorted')\n",
    "de.download_data()\n",
    "de.sort_data()\n",
    "\n",
    "# For each participant, segment, clean and extract data\n",
    "print(\"Pre-processing data...\")\n",
    "for index in range(1): # number_of_participants\n",
    "    # show plots for first participant only\n",
    "    show_plot = True if index==0 else False\n",
    "\n",
    "    # skip if features already extracted, otherwise use extract using sorted data\n",
    "    if not Utilities.check_csv_exists(f'{directory}/All Features', index):\n",
    "        df = Utilities.load_dataframe(Utilities.check_csv_exists(f'{directory}/Sorted', index))\n",
    "        # interpolate, segment using sliding window, and clean data\n",
    "        pp = PreProcessing(df, sampling_rate)\n",
    "        pp.interpolate()\n",
    "        pp.segment(window_length, overlap)\n",
    "        pp.clean()\n",
    "\n",
    "        # extract feautres for each segment\n",
    "        fe = FeatureExtraction(pp.get_samples(), show_plot)\n",
    "        fe.get_neurokit_features(HRV=True, EDR=True)\n",
    "        fe.save_features(f'{directory}/All Features', features='All')\n",
    "        fe.select(selected_features)\n",
    "        fe.save_features(f'{directory}/Selected Features', features='Selected')\n",
    "        Utilities.progress_bar(index+1, number_of_participants)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
