{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stress Classifier\n",
    "## Overview\n",
    "1. [Select Database](#1-select-database)\n",
    "2. [Import Libraries and Load Useful Utilities](#2-import-libraries-and-load-useful-utilities)\n",
    "3. [Data Extraction](#3-data-extraction): Downloads and sorts through databases.\n",
    "4. [Pre-processing](#4-pre-processing): filtering and signal cleaning.\n",
    "5. [Feature Extraction](#5-feature-extraction): R-R peaks, PQRST peaks, EDR, in addition to mean, kurtosis etc.\n",
    "6. [Feature Selection](#6-feature-selection): Visualise labelled feature distribution, select desired labels etc.\n",
    "7. [Traditional Machine Learning Methods](#7-traditional-machine-learning-methods): Linear classification including Random Forests, LDA, Bagged Trees and linear/non-linear SVM.\n",
    "8. [Convolutional Neural Network](#8-convolutional-neural-network): 1-D convolutional neural network with automatic feature extraction "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select Database\n",
    "Select between:\n",
    "* [Spider-Fearful ECG](https://doi.org/10.1371/journal.pone.0231517) \n",
    "* BrainPatch ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:56.740564Z",
     "iopub.status.busy": "2023-03-28T14:15:56.740124Z",
     "iopub.status.idle": "2023-03-28T14:15:56.746295Z",
     "shell.execute_reply": "2023-03-28T14:15:56.745553Z"
    }
   },
   "outputs": [],
   "source": [
    "# DATABASE = 'Spider'\n",
    "DATABASE = 'BrainPatch'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Load Useful Utilities\n",
    "Please run `pip install -r requirements.txt` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:56.748730Z",
     "iopub.status.busy": "2023-03-28T14:15:56.748407Z",
     "iopub.status.idle": "2023-03-28T14:15:57.623281Z",
     "shell.execute_reply": "2023-03-28T14:15:57.623003Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, CategoricalNB, ComplementNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import subprocess\n",
    "import warnings\n",
    "from itertools import product\n",
    "from typing import List\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.624839Z",
     "iopub.status.busy": "2023-03-28T14:15:57.624743Z",
     "iopub.status.idle": "2023-03-28T14:15:57.631111Z",
     "shell.execute_reply": "2023-03-28T14:15:57.630859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Useful Utilities\n",
    "class Utilities():\n",
    "    def __init__(self):    \n",
    "        pass\n",
    "\n",
    "    def progress_bar(current_message, current, total, bar_length=20):\n",
    "        fraction = current / total\n",
    "        arrow = int(fraction * bar_length - 1) * '-' + '>'\n",
    "        padding = int(bar_length - len(arrow)) * ' '\n",
    "        ending = '\\n' if current == total else '\\r'\n",
    "        print(f'{current_message}: [{arrow}{padding}] {int(fraction*100)}%', end=ending)\n",
    "\n",
    "\n",
    "    def check_csv_exists(folder_path, sample_index):\n",
    "        # read the CSV file into a dataframe and append to the list\n",
    "        filename = os.path.join(folder_path, f'df_{index}.csv')\n",
    "        try:\n",
    "            df = pd.read_csv(filename)\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "        return filename\n",
    "\n",
    "\n",
    "    def load_dataframe(filename):\n",
    "        # read the CSV file into a dataframe and append to the list\n",
    "        df = pd.read_csv(filename)\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def save_dataframe_list(list_of_dfs:List[pd.DataFrame], folder_path:str, file_name:str):\n",
    "        # create directoy if necessary\n",
    "        os.makedirs(folder_path, exist_ok=True) \n",
    "        for i, df in enumerate(list_of_dfs):\n",
    "            file_path = f\"{folder_path}/{file_name}_{i}.csv\"\n",
    "            df.to_csv(file_path, index=False)\n",
    "\n",
    "\n",
    "    def save_dataframe(df:pd.DataFrame, folder_path:str, file_name:str):\n",
    "        print(f\"Saving Dataframe to: {folder_path}/{file_name}.csv...\", end='')\n",
    "        # create directoy if necessary\n",
    "        os.makedirs(folder_path, exist_ok=True) \n",
    "        df.to_csv(f'{folder_path}/{file_name}.csv', index=False)\n",
    "        print(\"Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For bigger plots\n",
    "plt.rcParams['figure.figsize'] = [20, 12]  \n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Extraction\n",
    "* Downloads data, normalizes timeframe, attaches labels, and saves sorted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.632504Z",
     "iopub.status.busy": "2023-03-28T14:15:57.632310Z",
     "iopub.status.idle": "2023-03-28T14:15:57.638674Z",
     "shell.execute_reply": "2023-03-28T14:15:57.638402Z"
    }
   },
   "outputs": [],
   "source": [
    "# This class handles downloading and sorting the Spider database.\n",
    "# Method from this paper https://doi.org/10.1371/journal.pone.0231517\n",
    "class SpiderDataExtraction():\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory + 'Spider'\n",
    "        self.sampling_frequency = 100\n",
    "        self.sorted_ECG = pd.DataFrame(columns=['Timestamp', 'ECG', 'Stress Level'])\n",
    "\n",
    "    \n",
    "    # Downloads and extracts database \n",
    "    def download_data(self):\n",
    "        if sys.platform == 'linux':\n",
    "            url = 'https://physionet.org/files/ecg-spider-clip/1.0.0/'\n",
    "            print(\"Downloading database...this may take a while\")\n",
    "            os.makedirs(self.directory, exist_ok=True)\n",
    "            cmd = f\"wget -r -N -c -np -P {self.directory} {url}\"\n",
    "            subprocess.run(cmd)\n",
    "        else:\n",
    "            sys.exit(\"Unable to download database. If you are running Windows/Mac please download manually via https://physionet.org/content/ecg-spider-clip/1.0.0/\")\n",
    "    \n",
    "\n",
    "    # sorts data from each participant, labelling each ECG recording and appends to one dataframe.\n",
    "    # Following the SB approach in the study.\n",
    "    def sort_data(self):       \n",
    "        directory = self.directory + '/physionet.org/files/ecg-spider-clip/1.0.0/'\n",
    "        \n",
    "        # Exclude VP70 because of noise\n",
    "        sub_directories = ['VP02', 'VP03','VP05','VP06','VP08','VP09','VP11','VP12','VP14','VP15','VP17','VP18','VP20','VP23','VP24','VP26','VP27',\n",
    "                'VP29','VP30','VP32','VP33','VP35','VP36','VP38','VP39','VP41','VP42','VP44','VP45','VP47','VP48','VP50','VP51','VP53',\n",
    "                'VP54','VP56','VP57','VP59','VP61','VP62','VP63','VP64','VP65','VP66','VP68','VP69','VP71','VP72','VP73','VP74',\n",
    "                'VP75','VP76','VP77','VP78','VP79','VP80']\n",
    "        \n",
    "        # Path to Ratings file for all particpants\n",
    "        subjective_ratings_file = f'{self.directory}/Subjective Ratings.txt'\n",
    "\n",
    "        # Read in the subject ratings file (ignore arousal markers, interested in angst)\n",
    "        ratings_df = (\n",
    "            pd.read_csv(subjective_ratings_file, sep='\\t', names=['Subject','Group','Session', '4', '8', '12', '16', 'NA1', 'NA2', 'NA3', 'NA4'], encoding='UTF-16')\n",
    "            .drop(columns=['NA1', 'NA2', 'NA3', 'NA4'])\n",
    "            .iloc[1:]\n",
    "            .reset_index(drop=True)\n",
    "            .astype(int)\n",
    "        )\n",
    "\n",
    "        for index, sub_directory in enumerate(sub_directories):\n",
    "            Utilities.progress_bar('Sorting database', index, len(sub_directories)-1)\n",
    "\n",
    "            # set participant data paths\n",
    "            ECG_file = f'{directory}{sub_directory}/BitalinoECG.txt'\n",
    "            triggers_file = f'{directory}{sub_directory}/Triggers.txt'\n",
    "\n",
    "            # Get participant number\n",
    "            participant_no = int(sub_directory[2:])\n",
    "\n",
    "            # read in particpant ECG raw data file and reorder to get columns Timestamp, ECG \n",
    "            raw_df = pd.read_csv(ECG_file, sep='\\t', names = ['ECG','Timestamp','NA'])\n",
    "            raw_df = raw_df.drop(columns=['NA'])\n",
    "            raw_df = raw_df[['Timestamp', 'ECG']]\n",
    "            \n",
    "            # Read in participant trigger file\n",
    "            triggers_df = pd.read_csv(triggers_file, sep='\\t', names = ['Clip','On','Off'])\n",
    "\n",
    "            # Determine stress levels by correspoding the raw ecg data with the triggers file and the Subjective Ratings file.\n",
    "            # Iterate through the 16 stress clips (first clip is a demo):\n",
    "            for i in range(1, 17):\n",
    "                # Determine row in ratings file\n",
    "                row = ratings_df.loc[ratings_df['Subject'] == participant_no].index[0]\n",
    "\n",
    "                # find stress for the clip in the ratings file\n",
    "                stress_level = ratings_df.iloc[row]['4'] if i <= 4 else ratings_df.iloc[row]['8'] if i <= 8 else ratings_df.iloc[row]['12'] if i <= 12 else ratings_df.iloc[row]['16']\n",
    "\n",
    "                # convert stress level to Low, Medium or High (1-3)\n",
    "                stress_level = 2 if (stress_level <= 2) else 3\n",
    "\n",
    "                # Get 60 second slice of ECG data for that clip\n",
    "                clip_start_time = triggers_df.iloc[i]['On']\n",
    "                start_index = raw_df.index[raw_df['Timestamp']>clip_start_time].tolist()[0]\n",
    "                clip_df = raw_df.iloc[start_index:start_index + (self.sampling_frequency * 60)].copy(deep=False)\n",
    "                clip_df['Stress Level'] = stress_level\n",
    "                self.sorted_ECG = pd.concat([self.sorted_ECG, clip_df], axis=0, ignore_index=True)\n",
    "\n",
    "            # Add the last 3 minute resting phase (stress level Low) to the data\n",
    "            rest_start_time = triggers_df.iloc[-1]['On']\n",
    "            start_index = (raw_df['Timestamp'] > rest_start_time).idxmin() + (self.sampling_frequency * 120)\n",
    "            rest_df = raw_df.iloc[start_index: start_index + (self.sampling_frequency * 180)].copy(deep=False)\n",
    "            rest_df.loc[:, 'Stress Level'] = 1\n",
    "            self.sorted_ECG = pd.concat([self.sorted_ECG, rest_df], axis=0, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class handles sorting through the data collected using the BrainPatch ECG Recorder.\n",
    "class BPDataExtraction():\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory + 'BrainPatch/'\n",
    "        self.sorted_ECG = pd.DataFrame(columns=['Timestamp', 'ECG', 'Stress Level'])\n",
    "\n",
    "    \n",
    "    # interpolates data to achieve sampling rate\n",
    "    def interpolate(self, sample:pd.DataFrame) -> pd.DataFrame:\n",
    "        # convert timestamp column to a NumPy array\n",
    "        timestamps = sample['Timestamp'].to_numpy()\n",
    "\n",
    "        # calculate the time difference between each pair of adjacent timestamps\n",
    "        time_diff = np.diff(timestamps)\n",
    "\n",
    "        # calculate the average sampling rate of the data\n",
    "        sampling_frequency = (1 / np.mean(time_diff))*1e6\n",
    "\n",
    "        interpolated_sample = pd.DataFrame(columns=['Timestamp', 'ECG'])\n",
    "\n",
    "        # interpolate the data to obtain desired sampling rate\n",
    "        interpolated_sample['ECG'] = nk.signal_resample(sample['ECG'], sampling_rate=sampling_frequency, desired_sampling_rate=self.sampling_frequency)\n",
    "        time_interval_us = (1/self.sampling_frequency)*1e6\n",
    "        interpolated_sample['Timestamp'] = [i*time_interval_us for i in range(len(interpolated_sample['ECG']))]\n",
    "\n",
    "        return interpolated_sample.astype('int64')\n",
    "\n",
    "\n",
    "    # define a function to map stress levels to labels\n",
    "    def map_stress_level(self, level):\n",
    "        if level >= 7:\n",
    "            return 3\n",
    "        elif level >= 5:\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "\n",
    "     # Sorts data from each participant, interpolates it (downsampling), labelling each ECG recording and appends to one dataframe.\n",
    "    def sort_data(self, sampling_frequency:int=1000):\n",
    "        self.sampling_frequency = sampling_frequency\n",
    "        # list the contents of the directory and filter out subfolders\n",
    "        recordings = [f for f in os.listdir(self.directory) if os.path.isfile(os.path.join(self.directory, f))]\n",
    "\n",
    "        for index, recording in enumerate(recordings):\n",
    "            Utilities.progress_bar('Sorting database', index, len(recordings)-1)\n",
    "            # get recording number\n",
    "            split_string = recording.split(' ')\n",
    "            second_element = split_string[1]\n",
    "            second_split = second_element.split('.')\n",
    "            recording_number = int(second_split[0])\n",
    "\n",
    "            # extract recording dataframe\n",
    "            recording_df = pd.read_csv(self.directory + recording, skiprows=2, usecols=[0, 1], header=None, names=['Timestamp', 'ECG'])\n",
    "            # interpolate it to desired sampling frequency\n",
    "            recording_df = self.interpolate(recording_df)\n",
    "            # load events containing the stress level markers\n",
    "            events_df = pd.read_csv(self.directory + 'Events/Events.csv')\n",
    "            # extract events for that recording\n",
    "            events_df = events_df[events_df['Recording No'] == recording_number].dropna(how='all', axis=1)\n",
    "            # convert string values to datetime objects\n",
    "            events_df['Timestamp'] = pd.to_datetime(events_df['Timestamp'], format='%H:%M')\n",
    "            # normalise timestamps \n",
    "            events_df['Timestamp'] = events_df['Timestamp'] - events_df['Timestamp'].iloc[0]\n",
    "            # convert from nanoseconds to microseconds\n",
    "            events_df['Timestamp'] = (events_df['Timestamp']/ 1e3).astype('int64')\n",
    "            # label recording with Events label\n",
    "            recording_df = pd.merge_asof(recording_df, events_df, on='Timestamp', direction='backward')\n",
    "            recording_df = recording_df.drop('Recording No', axis=1)\n",
    "            # convert stress level to Low(1-4), Medium(5-6) or High(7-10)\n",
    "            recording_df['Stress Level'] = recording_df['Stress Level'].apply(self.map_stress_level)\n",
    "            # concatentate to main dataframe\n",
    "            self.sorted_ECG = pd.concat([self.sorted_ECG, recording_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/data/myenv/Data/'\n",
    "\n",
    "if DATABASE == 'Spider':\n",
    "    sde = SpiderDataExtraction(data_path)\n",
    "    # sde.download_data()\n",
    "    sde.sort_data()\n",
    "    dataframe_path = f'{data_path}Spider/Dataframes'\n",
    "    Utilities.save_dataframe(sde.sorted_ECG, dataframe_path, 'Sorted')\n",
    "\n",
    "elif DATABASE == 'BrainPatch':\n",
    "    bpde = BPDataExtraction(data_path)\n",
    "    bpde.sort_data(sampling_frequency=1000) # downsamples to 1kHz\n",
    "    dataframe_path = f'{data_path}BrainPatch/Dataframes'\n",
    "    Utilities.save_dataframe(bpde.sorted_ECG, dataframe_path, 'Sorted')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pre-Processing\n",
    "* Interpolates data to sample rate using timestamps \n",
    "* Semgents data using rolling window with overlap\n",
    "* Cleans data using Neurokit's 5th Order Butterworth filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.639837Z",
     "iopub.status.busy": "2023-03-28T14:15:57.639761Z",
     "iopub.status.idle": "2023-03-28T14:15:57.644906Z",
     "shell.execute_reply": "2023-03-28T14:15:57.644649Z"
    }
   },
   "outputs": [],
   "source": [
    "class PreProcessing():\n",
    "    def __init__(self, sorted_ECG:pd.DataFrame, sampling_frequency:int):\n",
    "        self.sorted_ECG = sorted_ECG\n",
    "        self.sampling_frequency = sampling_frequency\n",
    "       \n",
    "\n",
    "    # segments data with overlap using rolling window, if semgnet_heartbeats true, then segment is centralised around the heartbeat (R peak)\n",
    "    def segment(self, window_length_s:float, overlap:float, segment_hearbeats=False):\n",
    "        # convert window_length in seconds to samples\n",
    "        self.window_samples = int(window_length_s * self.sampling_frequency)\n",
    "        # Calculate the step_size as the fraction of the total window samples\n",
    "        step_size = int(self.window_samples * (1-overlap)) \n",
    "\n",
    "        # Initialize starting variables\n",
    "        current_index = 0\n",
    "        current_stressed = self.sorted_ECG['Stress Level'][current_index]\n",
    "        self.preprocessed_ECG = pd.DataFrame(columns=['Timestamp', 'ECG', 'Stress Level'])\n",
    "\n",
    "        # faster to concatenate at the end \n",
    "        preprocessed_ECG_list = []\n",
    "\n",
    "        # get all R peaks index if required\n",
    "        if segment_hearbeats:\n",
    "            r_peaks = nk.ecg_peaks(self.sorted_ECG['ECG'], sampling_rate=self.sampling_frequency)\n",
    "        \n",
    "        # Loop through the entire dataframe\n",
    "        while current_index < len(self.sorted_ECG['ECG']):  \n",
    "            Utilities.progress_bar('Segmenting data', current_index, len(self.sorted_ECG))\n",
    "            # calculate end index in window and exit if out of bounds          \n",
    "            end_index = current_index + self.window_samples\n",
    "            if (end_index > len(self.sorted_ECG['ECG'])):\n",
    "                break\n",
    "            \n",
    "            # Check if the window overlaps a different label\n",
    "            end_stressed = self.sorted_ECG['Stress Level'][end_index]\n",
    "\n",
    "            # If the next window has a different label, skip to next start of next label\n",
    "            if end_stressed != current_stressed:\n",
    "                while (current_stressed == self.sorted_ECG['Stress Level'][current_index]):\n",
    "                    current_index += 1\n",
    "                current_stressed = end_stressed\n",
    "\n",
    "            # otherwise, add segment to list of pre-processed ECG\n",
    "            else:\n",
    "                if segment_hearbeats:\n",
    "                    # get index of next r peak\n",
    "                    while not bool(r_peaks[0]['ECG_R_Peaks'][current_index]):\n",
    "                        current_index += 1\n",
    "                    # append segment centred on r-peak to dataframe\n",
    "                    preprocessed_ECG_list.append(self.sorted_ECG.iloc[(current_index - (self.window_samples//2)):(current_index + (self.window_samples//2))].astype('Float64'))\n",
    "                    # shift the window to next non r-peak index\n",
    "                    current_index += 1\n",
    "                else:\n",
    "                    # append segment to dataframe\n",
    "                    preprocessed_ECG_list.append(self.sorted_ECG.iloc[current_index:current_index + self.window_samples].astype('Float64'))\n",
    "                    # Shift the window\n",
    "                    current_index += step_size\n",
    "            \n",
    "        self.preprocessed_ECG = pd.concat(preprocessed_ECG_list, axis=0, ignore_index=True).astype('Float64')\n",
    "        Utilities.progress_bar('Segmenting data', current_index, current_index)\n",
    "    \n",
    "    \n",
    "    def create_2d(self):\n",
    "        # convert the pandas DataFrame into a 2D pandas where each row has the size of window and the corresponding label (stress level)\n",
    "        \n",
    "        # Calculate the number of rows required\n",
    "        num_rows = len(self.preprocessed_ECG['ECG']) // self.window_samples\n",
    "\n",
    "        # Create an empty dataframe to hold the reshaped data\n",
    "        df_reshaped = pd.DataFrame(index=range(num_rows), columns=[f\"ECG {i}\" for i in range(self.window_samples)])\n",
    "\n",
    "        # Reshape the data\n",
    "        for i in range(num_rows):\n",
    "            start_idx = i * self.window_samples\n",
    "            end_idx = (i + 1) * self.window_samples\n",
    "            values = self.preprocessed_ECG['ECG'].iloc[start_idx:end_idx].values\n",
    "            df_reshaped.iloc[i, :] = values\n",
    "        \n",
    "        self.preprocessed_ECG_2d = df_reshaped\n",
    "        self.preprocessed_ECG_2d['Stress Level'] = self.preprocessed_ECG['Stress Level'][::self.window_samples].reset_index(drop=True)\n",
    "\n",
    "\n",
    "    def clean(self):\n",
    "        # Clean each sample in the stressed and not stressed data (overwrites original data)\n",
    "        # using method 'neurokit' (0.5 Hz high-pass butterworth filter (order = 5), followed by powerline filtering) but can be changed to other cleaning methods\n",
    "        print(\"Cleaning data...\")\n",
    "        self.preprocessed_ECG['ECG'] = pd.Series(nk.ecg_clean(self.preprocessed_ECG['ECG'], self.sampling_frequency, method='neurokit')).astype('Float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length_s = 30 # window length for each segment\n",
    "overlap = 0.1 # overlap for sliding window\n",
    "\n",
    "if DATABASE == 'Spider':\n",
    "    pp = PreProcessing(sde.sorted_ECG, sde.sampling_frequency)\n",
    "    pp.segment(window_length_s, overlap)\n",
    "    pp.clean()\n",
    "    Utilities.save_dataframe(pp.preprocessed_ECG, dataframe_path, 'Preprocessed')\n",
    "\n",
    "elif DATABASE == 'BrainPatch':\n",
    "    pp = PreProcessing(bpde.sorted_ECG, bpde.sampling_frequency) # the data will be downsampled to 1000Hz\n",
    "    pp.segment(window_length_s, overlap)\n",
    "    pp.clean()\n",
    "    Utilities.save_dataframe(pp.preprocessed_ECG, dataframe_path, 'Preprocessed')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction\n",
    "* Extracts features such as HRV time, frequency and non-linear domain, EDR etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.649031Z",
     "iopub.status.busy": "2023-03-28T14:15:57.648948Z",
     "iopub.status.idle": "2023-03-28T14:15:57.656454Z",
     "shell.execute_reply": "2023-03-28T14:15:57.656197Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main class that extracts features from a dictionary of sorted dataframes and stores to csv\n",
    "class FeatureExtraction():\n",
    "    # takes in cleaned ECG data\n",
    "    def __init__(self, preprocessed_ECG:pd.DataFrame, window_samples:int, sampling_frequency:int):\n",
    "        self.preprocessed_ECG = preprocessed_ECG\n",
    "        self.window_samples = window_samples\n",
    "        self.sampling_frequency = sampling_frequency\n",
    "        self.feature_extracted_ECG = pd.DataFrame()\n",
    "\n",
    "\n",
    "    def plot_segment(self, segment, ECG_processed=None, peaks=None, colors=['r', 'g', 'c', 'm', 'y', 'k']):\n",
    "        # Define time array\n",
    "        t = np.arange(len(segment)) / self.sampling_frequency\n",
    "\n",
    "        # plot fft if no peaks are given\n",
    "        if isinstance(peaks, type(None)):\n",
    "            # Compute FFT\n",
    "            fft = np.fft.fft(segment)\n",
    "            freq = np.fft.fftfreq(len(segment), d=1/self.sampling_frequency)\n",
    "\n",
    "            # max frequency to plot\n",
    "            max_freq = 100\n",
    "\n",
    "            # Filter out negative frequencies and frequencies above max_freq\n",
    "            mask = (freq >= 0) & (freq <= max_freq)\n",
    "            freq = freq[mask]\n",
    "            fft = fft[mask]\n",
    "\n",
    "            # Calculate PSD\n",
    "            psd = ((np.abs(fft) ** 2) / len(segment))\n",
    "            psd = 10 * np.log10(psd)\n",
    "            psd -= psd.max()\n",
    "\n",
    "            # Plot raw ECG segment\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(t, segment)\n",
    "\n",
    "            ax.set_xlabel('Time (s)')\n",
    "            ax.set_ylabel('Amplitude')\n",
    "\n",
    "            # Hide y-axis units\n",
    "            ax.set_yticklabels([])\n",
    "\n",
    "            # Crop the plot to the first 25% of the x-axis\n",
    "            ax.set_xlim(t.max()*0, t.max()*0.25)\n",
    "\n",
    "            # Add subplot\n",
    "            subax = fig.add_axes([0.68, 0.65, 0.2, 0.2])\n",
    "            subax.plot(freq, psd)\n",
    "\n",
    "            # Limit x-axis to positive frequencies between 0 and max_freq\n",
    "            subax.set_xlim(0, max_freq)\n",
    "\n",
    "            # add labels\n",
    "            subax.set_xlabel('Frequency (Hz)')\n",
    "            subax.set_ylabel('PSD (dB)')\n",
    "\n",
    "        # otherwise, plot peaks\n",
    "        else:\n",
    "            # Plot raw ECG segment\n",
    "            plt.figure()\n",
    "            plt.plot(t, segment)\n",
    "\n",
    "            # Create Line2D objects for each peak type with corresponding color\n",
    "            lines = [Line2D([0], [0], linestyle='--', color=colors[i]) for i in range(len(peaks))]\n",
    "\n",
    "            # Plot peaks\n",
    "            for i, peak in enumerate(peaks):\n",
    "                peak_inds = np.where(ECG_processed[peak] == 1)[0]\n",
    "                for ind in peak_inds:\n",
    "                    plt.axvline(x=t[ind], linestyle='--', color=colors[i])\n",
    "\n",
    "            # Add legend with the created Line2D objects and corresponding labels\n",
    "            plt.legend(handles=lines, labels=peaks, loc='lower right')\n",
    "\n",
    "            plt.xlabel('Time (s)')\n",
    "            plt.ylabel('Amplitude')\n",
    "            # Hide y-axis units\n",
    "            plt.gca().set_yticklabels([])\n",
    "            # Crop the plot to the first 25% of the x-axis\n",
    "            plt.xlim(t.max()*0, t.max()*0.1)\n",
    "\n",
    "            # Show plot\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def wave_analysis(self, segment:pd.DataFrame, plot=False) -> pd.DataFrame:\n",
    "        ECG_processed, info = nk.ecg_process(segment.to_numpy(dtype='float64'), sampling_rate=self.sampling_frequency, method='neurokit')\n",
    "\n",
    "        # calculate the mean and SD of the peak intervals\n",
    "        peaks = ['ECG_P_Peaks', 'ECG_Q_Peaks', 'ECG_R_Peaks', 'ECG_S_Peaks', 'ECG_T_Peaks']\n",
    "        # Minimum and maximum expected HR (beats per min)\n",
    "        min_HR = 30\n",
    "        max_HR = 200\n",
    "        min_interval = 60e6/max_HR\n",
    "        max_interval = 60e6/min_HR\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for peak in peaks:\n",
    "            intervals = np.diff(np.where(np.array(ECG_processed[peak]==1))) * self.sampling_frequency\n",
    "            # Remove any intervals that are out of range\n",
    "            intervals = intervals[(intervals >= min_interval) & (intervals <= max_interval)]\n",
    "\n",
    "            df[f'{peak}_Interval_Mean'] = [np.mean(intervals)]\n",
    "            df[f'{peak}_Interval_SD'] = [np.std(intervals)]\n",
    "        \n",
    "        if plot:\n",
    "            self.plot_segment(segment, ECG_processed, peaks)\n",
    "\n",
    "        # calculate the average length of each wave\n",
    "        waves = ['P', 'R', 'T']\n",
    "        max_duration = [120000, 120000, 200000]\n",
    "        for w, wave in enumerate(waves):\n",
    "            onsets = np.where(np.array(ECG_processed[f'ECG_{wave}_Onsets']==1))[0]\n",
    "            offsets = np.where(np.array(ECG_processed[f'ECG_{wave}_Offsets']==1))[0]\n",
    "            # find index of first element in offsets that is >= first element in onsets\n",
    "            idx_offset = np.where(offsets >= onsets[0])[0][0]\n",
    "            # find size of smallest array\n",
    "            duration_size = min(onsets.size, offsets.size)\n",
    "            # slice offsets array to start at same index as onset\n",
    "            offsets = offsets[idx_offset:duration_size]\n",
    "            # set onset to same length\n",
    "            onsets = onsets[:duration_size]\n",
    "            # calculate durations taking into account missed onset detection\n",
    "            durations = []\n",
    "            # iterate over elements of both arrays\n",
    "            i = 0\n",
    "            j = 0\n",
    "            while i < len(offsets) and j < len(onsets):\n",
    "                diff = offsets[i] - onsets[j]\n",
    "                if diff < 0:\n",
    "                    i += 1\n",
    "                else:\n",
    "                    durations.append(diff)\n",
    "                    i += 1\n",
    "                    j += 1\n",
    "            durations = np.array(durations * self.sampling_frequency)\n",
    "            # Remove any intervals that are out of range\n",
    "            durations = durations[(durations <= max_duration[w])]\n",
    "\n",
    "            duration_mean = np.mean(durations)\n",
    "            duration_SD = np.std(durations)\n",
    "            df[f'ECG_{wave}_Duration_Mean'] = duration_mean\n",
    "            df[f'ECG_{wave}_Duration_SD'] = duration_SD\n",
    "\n",
    "        wave_onsets_offsets = []\n",
    "        for wave in waves:\n",
    "            wave_onsets_offsets.append(f'ECG_{wave}_Onsets')\n",
    "            wave_onsets_offsets.append(f'ECG_{wave}_Offsets')\n",
    "        \n",
    "        if plot:\n",
    "            self.plot_segment(segment, ECG_processed, wave_onsets_offsets, colors = ['r', 'r', 'g', 'g', 'b', 'b'])\n",
    "\n",
    "        return df    \n",
    "\n",
    "    \n",
    "    def calc_PSD(self, segment:pd.DataFrame) -> pd.DataFrame:\n",
    "        # Sum the power across 10 Hz bands from 0 to 200 Hz\n",
    "        # Compute the power spectrum using a Fast Fourier Transform\n",
    "        PSD = nk.signal_psd(segment.to_list(), sampling_rate=self.sampling_frequency, method=\"welch\", min_frequency=0.5, max_frequency=200)\n",
    "        # Create an empty list to store the binned power values\n",
    "        binned_power = []\n",
    "        # Set the initial frequency and bin range values\n",
    "        frequency = 0\n",
    "        bin_range = 10\n",
    "        nyquist_frequency = self.sampling_frequency//2\n",
    "\n",
    "        # Loop through the frequency ranges of 10Hz\n",
    "        while bin_range <= nyquist_frequency:\n",
    "            # Initialize the total power for the current bin\n",
    "            total_power = 0\n",
    "            \n",
    "            # Loop through the rows of the original dataframe\n",
    "            for index, row in PSD.iterrows():\n",
    "                # Check if the frequency falls within the current bin range\n",
    "                if row['Frequency'] >= frequency and row['Frequency'] < bin_range:\n",
    "                    # Add the power value to the total power\n",
    "                    total_power += row['Power']\n",
    "            \n",
    "            # Calculate the logarithm of the total power for the current bin and append it to the binned_power list\n",
    "            if total_power > 0:\n",
    "                binned_power.append(np.log10(total_power))\n",
    "            else:\n",
    "                binned_power.append(-np.inf)\n",
    "            \n",
    "            # Increment the frequency and bin range values for the next iteration\n",
    "            frequency += 10\n",
    "            bin_range += 10\n",
    "\n",
    "        # Create a new dataframe with the binned power values and the frequency ranges as the index\n",
    "        binned_PSD = pd.DataFrame({'Power': binned_power})\n",
    "        binned_PSD['Frequency Band'] = list(range(10, nyquist_frequency+1, 10))\n",
    "        # Convert to columns\n",
    "        ECG_Frequencies = pd.DataFrame(columns=[f\"ECG_FQ_{i}\" for i in range(10, nyquist_frequency+1, 10)])\n",
    "        for i, column in enumerate(ECG_Frequencies.columns):\n",
    "            ECG_Frequencies[column] = [binned_PSD.iloc[i]['Power']]\n",
    "        \n",
    "        return ECG_Frequencies\n",
    "    \n",
    "\n",
    "    def calc_collective_ECG_features(self) -> pd.DataFrame:\n",
    "        print(\"Extracting Collective Features...\")\n",
    "        warnings.filterwarnings('ignore') # temporarily supress warnings\n",
    "        # automated pipeline for preprocessing an ECG signal\n",
    "        ECG_processed, info = nk.ecg_process(self.preprocessed_ECG['ECG'].to_numpy(dtype='float64'), sampling_rate=self.sampling_frequency, method='neurokit')\n",
    "        events = np.arange(self.window_samples, self.preprocessed_ECG.shape[0], self.window_samples)\n",
    "        epochs = nk.epochs_create(ECG_processed, events=events, sampling_rate=self.sampling_frequency)\n",
    "        # calculate ECG Features such as pqrstu intevals etc.\n",
    "        ECG_events = nk.ecg_analyze(epochs, sampling_rate=self.sampling_frequency, method='event-related')\n",
    "        warnings.filterwarnings('default')\n",
    "        return ECG_events\n",
    "\n",
    "    \n",
    "    def calc_HRV_features(self, r_peaks_df, segment, show_plot=False):\n",
    "        np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "        # skip segment if insufficient peaks are detected (otherwise will cause NK error)\n",
    "        if int(r_peaks_df[r_peaks_df == 1].sum().iloc[0]) < 4:\n",
    "            return\n",
    "\n",
    "        # Extract HRV features from R-R peaks, see https://neuropsychology.github.io/NeuroKit/functions/hrv.html \n",
    "        # compute HRV - time, frequency and nonlinear indices.\n",
    "        warnings.filterwarnings('ignore') # temporarily supress warnings\n",
    "        HRV_time = nk.hrv_time(r_peaks_df, sampling_rate=self.sampling_frequency, show=show_plot)\n",
    "        HRV_frequency = nk.hrv_frequency(r_peaks_df, sampling_rate=self.sampling_frequency, show=show_plot)\n",
    "        warnings.filterwarnings('default')\n",
    "\n",
    "        # compute Shannon Entropy (SE) using signal symbolization and discretization\n",
    "        # see https://neuropsychology.github.io/NeuroKit/functions/complexity.html#entropy-shannon \n",
    "        SE = nk.entropy_shannon(segment, symbolize='A')[0]\n",
    "        HRV_ShanEn = pd.DataFrame([SE], columns=['HRV_ShanEn'])\n",
    "        # concat to feature dataframe\n",
    "        return HRV_time, HRV_frequency, HRV_ShanEn\n",
    "\n",
    "    \n",
    "    def calc_EDR(self, r_peaks_df, segment, show_plot) -> pd.DataFrame:\n",
    "        # Get ECG Derived Respiration (EDR) and add to the data\n",
    "        warnings.filterwarnings('ignore') # temporarily supress warnings\n",
    "        ecg_rate = nk.signal_rate(r_peaks_df, sampling_rate=self.sampling_frequency, desired_length=len(r_peaks_df))\n",
    "        EDR_sample = nk.ecg_rsp(ecg_rate, sampling_rate=self.sampling_frequency)\n",
    "        if show_plot:\n",
    "            nk.signal_plot(segment)\n",
    "            nk.signal_plot(EDR_sample)\n",
    "        EDR_Distances = nk.signal_findpeaks(EDR_sample)[\"Distance\"]\n",
    "        EDR_Distance = pd.DataFrame([np.average(EDR_Distances)], columns=['EDR_Distance'])\n",
    "        diff = np.diff(EDR_Distances)\n",
    "        diff_squared = diff**2\n",
    "        mean_diff_squared = np.mean(diff_squared)\n",
    "        rmssd = np.sqrt(mean_diff_squared)\n",
    "        EDR_RMSSD = pd.Series(rmssd)\n",
    "        warnings.filterwarnings('default')\n",
    "\n",
    "        return EDR_Distance, EDR_RMSSD\n",
    "\n",
    "\n",
    "    # Main method to extracts features from ECG using neurokit\n",
    "    def extract(self, ECG:bool=False, EDR:bool=False, show_plot:bool=False):\n",
    "        # collective epoch analysis\n",
    "        if ECG:\n",
    "          ECG_events = self.calc_collective_ECG_features()\n",
    "\n",
    "        # individual epoch analysis\n",
    "        sample_index = 0\n",
    "        epoch_index = 0\n",
    "        while sample_index < (len(self.preprocessed_ECG['ECG']) - self.window_samples):\n",
    "            Utilities.progress_bar('Extracting Individual Features', sample_index, len(self.preprocessed_ECG['ECG']))\n",
    "\n",
    "            # get segment ECG and stress level from dataframe \n",
    "            segment = self.preprocessed_ECG.iloc[sample_index:sample_index + self.window_samples]['ECG']\n",
    "            stress_level = self.preprocessed_ECG.iloc[sample_index]['Stress Level']\n",
    "            features = pd.DataFrame({'Stress Level': [stress_level]})\n",
    "            # extract R-R peaks\n",
    "            r_peaks_df = nk.ecg_peaks(segment, sampling_rate=self.sampling_frequency, correct_artifacts=True)[0]\n",
    "            \n",
    "            sample_index += self.window_samples\n",
    "\n",
    "            if ECG:\n",
    "                warnings.filterwarnings('ignore') # temporarily supress warnings\n",
    "                ECG_processed_segment, info = nk.ecg_process(segment.to_numpy(dtype='float64'), sampling_rate=self.sampling_frequency, method='neurokit')\n",
    "                if show_plot:\n",
    "                    # nk.ecg_plot(ECG_processed_segment)\n",
    "                    self.plot_segment(segment)\n",
    "\n",
    "                # Calculate ECG_HRV - different heart rate variability metrices.\n",
    "                HRV_intervals = nk.ecg_intervalrelated(ECG_processed_segment, sampling_rate=self.sampling_frequency)\n",
    "\n",
    "                # calculate waveform intervals (PQRSTU)\n",
    "                ECG_intervals = self.wave_analysis(segment, show_plot)\n",
    "\n",
    "                # get the binned power spectrum frequencies from the ECG segment\n",
    "                ECG_frequencies = self.calc_PSD(segment)\n",
    "                \n",
    "                # add ECG_event to dataframe (ECG Features such as pqrstu intevals etc.))\n",
    "                ECG_event = ECG_events.iloc[epoch_index].to_frame().transpose().reset_index()\n",
    "                ECG_event = ECG_event.drop(['index', 'Label', 'Event_Onset'], axis=1)\n",
    "                epoch_index += 1\n",
    "\n",
    "                # concat to dataframe\n",
    "                features = pd.concat([features, HRV_intervals, ECG_intervals, ECG_event, ECG_frequencies], axis=1)\n",
    "\n",
    "            # If not all ECG features desired, obtain HRV and Shannon Entropy\n",
    "            else:\n",
    "                # skip segments that do not yield HRV features\n",
    "                try:\n",
    "                    HRV_time, HRV_frequency, HRV_ShanEn = self.calc_HRV_features(r_peaks_df, segment, show_plot=show_plot)\n",
    "                except TypeError:\n",
    "                    continue\n",
    "                # concat to feature dataframe\n",
    "                features = pd.concat([features, HRV_time, HRV_frequency, HRV_ShanEn], axis=1)\n",
    "                \n",
    "            if EDR:\n",
    "                EDR_Distance, EDR_RMSSD = self.calc_EDR(r_peaks_df, segment, show_plot)\n",
    "\n",
    "                # concat to dataframe\n",
    "                features = pd.concat([features, EDR_Distance, EDR_RMSSD], axis=1)\n",
    "\n",
    "            # concat features to main dataframe\n",
    "            self.feature_extracted_ECG = pd.concat([self.feature_extracted_ECG, features], axis=0, ignore_index=True)\n",
    "        Utilities.progress_bar('Extracting Neurokit Features', sample_index, sample_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.662551Z",
     "iopub.status.busy": "2023-03-28T14:15:57.662447Z",
     "iopub.status.idle": "2023-03-28T14:15:57.666162Z",
     "shell.execute_reply": "2023-03-28T14:15:57.665929Z"
    }
   },
   "outputs": [],
   "source": [
    "fe = FeatureExtraction(pp.preprocessed_ECG, pp.window_samples, pp.sampling_frequency)\n",
    "fe.extract(ECG=True, EDR=True, show_plot=False)\n",
    "Utilities.save_dataframe(fe.feature_extracted_ECG, dataframe_path, 'Feature Extracted BP EDR 30s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Selection:\n",
    "* Select desired features, sanity check the values, and save them to Features directory\n",
    "* Visualise most feature and cross-feature distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.646170Z",
     "iopub.status.busy": "2023-03-28T14:15:57.646007Z",
     "iopub.status.idle": "2023-03-28T14:15:57.647802Z",
     "shell.execute_reply": "2023-03-28T14:15:57.647608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Each feature will be an object of FEParameter\n",
    "class FEParameter:\n",
    "    def __init__(self, name:str, min:float=-999999, max:float=999999):\n",
    "        self.name = name\n",
    "        self.min = min\n",
    "        self.max = max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define FeatureSelection class that is used to visualise and select data\n",
    "class FeatureSelection():\n",
    "    def __init__(self, feature_extracted_ECG):\n",
    "        self.feature_extracted_ECG = feature_extracted_ECG\n",
    "\n",
    "\n",
    "    # desired_features is a list of FEParameter objects  \n",
    "    def select(self, desired_features:List[FEParameter]):\n",
    "        self.selected_features_ECG = self.feature_extracted_ECG[['Stress Level']]\n",
    "        \n",
    "        for feature in desired_features:\n",
    "            out_of_range_count = 0\n",
    "            # Sanity check: check if feature exists\n",
    "            if feature.name in self.feature_extracted_ECG.columns:\n",
    "                # Set value to NaN if it falls outside min and max values.\n",
    "                for i, value in enumerate(self.feature_extracted_ECG[feature.name]):\n",
    "                    if (value < feature.min) or (value > feature.max):\n",
    "                        out_of_range_count += 1\n",
    "                        self.feature_extracted_ECG.loc[i, feature.name] = np.nan\n",
    "                # Add column to new selected features\n",
    "                pd.options.mode.chained_assignment = None\n",
    "                self.selected_features_ECG[feature.name] = self.feature_extracted_ECG[[feature.name]].copy()\n",
    "                pd.options.mode.chained_assignment = 'warn'\n",
    "            else:\n",
    "                print(f'Error: No such feature \"{feature}\" in extracted features')\n",
    "            if out_of_range_count != 0:\n",
    "                print(f'Feature: {feature.name} is out of range {out_of_range_count}/{len(self.feature_extracted_ECG[feature.name])} segments')\n",
    "\n",
    "        \n",
    "    # impute missing values in dataset with mean values of column\n",
    "    def impute(self):\n",
    "        # switch infs to NaNs\n",
    "        pd.options.mode.chained_assignment = None\n",
    "        self.selected_features_ECG.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        pd.options.mode.chained_assignment = 'warn'\n",
    "        # check for columns with only NaNs and delete if necessary \n",
    "        drop_cols = [col for col in self.selected_features_ECG.columns if self.selected_features_ECG[col].isnull().all()]\n",
    "        if drop_cols:\n",
    "            self.selected_features_ECG = self.selected_features_ECG.drop(drop_cols, axis=1)\n",
    "        imp = SimpleImputer(strategy='mean')\n",
    "        imp.fit(self.selected_features_ECG)\n",
    "        self.selected_features_ECG = pd.DataFrame(imp.transform(self.selected_features_ECG), columns=self.selected_features_ECG.columns)\n",
    "\n",
    "\n",
    "    def visualise(self, plot_type='pairplot', single_feature=None):\n",
    "        print(\"Generating plot...\")\n",
    "        if plot_type == 'pairplot':         \n",
    "            sns.pairplot(data = self.selected_features_ECG, hue='Stress Level', palette=['green', 'yellow', 'red'])\n",
    "\n",
    "        elif plot_type == 'kdeplot':\n",
    "            if single_feature is None:\n",
    "                for i, feature in enumerate(self.selected_features_ECG):\n",
    "                    fig = plt.figure(figsize=(8,6))\n",
    "                    sns.kdeplot(x=feature, data=self.selected_features_ECG, hue='Stress Level', common_norm=False, warn_singular=False, palette=['green', 'yellow', 'red'])\n",
    "            else:\n",
    "                sns.kdeplot(x=single_feature, data=self.selected_features_ECG, hue='Stress Level', common_norm=False, warn_singular=False, palette=['green', 'yellow', 'red'], clip=(0.0, 150))\n",
    "\n",
    "\n",
    "        elif plot_type == 'kdesubplot':\n",
    "            # Create a figure with subplots for each feature\n",
    "            subplot_size = math.ceil(math.sqrt(len(self.selected_features_ECG.columns)))\n",
    "            fig = plt.figure(figsize=(20, 8*subplot_size))\n",
    "\n",
    "            # Loop through each feature and add it to a subplot\n",
    "            for i, feature in enumerate(self.selected_features_ECG):\n",
    "                fig.add_subplot(subplot_size, subplot_size, i+1)\n",
    "                sns.kdeplot(x=feature, data=self.selected_features_ECG, hue='Stress Level', common_norm=False, warn_singular=False, palette=['green', 'yellow', 'red'])\n",
    "            plt.show()\n",
    "\n",
    "        else:\n",
    "            print(\"Plot type not recognised. Please choose between pairplot, kdeplot, kdesubplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.657717Z",
     "iopub.status.busy": "2023-03-28T14:15:57.657598Z",
     "iopub.status.idle": "2023-03-28T14:15:57.661341Z",
     "shell.execute_reply": "2023-03-28T14:15:57.661082Z"
    }
   },
   "outputs": [],
   "source": [
    "# Desired Features to be selected from Feature Extracted ECG.\n",
    "# See Neurokit2 HRV for more information - https://neuropsychology.github.io/NeuroKit/functions/hrv.html\n",
    "\n",
    "# Minimum and maximum expected HR (beats per min)\n",
    "min_HR = 30\n",
    "max_HR = 200\n",
    "\n",
    "# MinNN: The minimum of the RR intervals (Parent, 2019; Subramaniam, 2022).\n",
    "HRV_MinNN = FEParameter('HRV_MinNN', min=60000.0/max_HR, max=60000.0/min_HR)\n",
    "# MaxNN: The maximum of the RR intervals (Parent, 2019; Subramaniam, 2022).\n",
    "HRV_MaxNN = FEParameter('HRV_MaxNN', min=60000.0/max_HR, max=60000.0/min_HR)\n",
    "# MeanNN: The mean of the RR intervals.\n",
    "HRV_MeanNN = FEParameter('HRV_MeanNN', min=60000.0/max_HR, max=60000.0/min_HR)\n",
    "\n",
    "\n",
    "# pNN20: The proportion of RR intervals greater than 20ms, out of the total number of RR intervals.\n",
    "HRV_pNN20 = FEParameter('HRV_pNN20')\n",
    "# pNN50: The proportion of RR intervals greater than 50ms, out of the total number of RR intervals.\n",
    "HRV_pNN50 = FEParameter('HRV_pNN50')\n",
    "# A geometrical parameter of the HRV, or more specifically, the baseline width of the RR intervals distribution \n",
    "# TINN: obtained by triangular interpolation, where the error of least squares determines the triangle. \n",
    "# It is an approximation of the RR interval distribution.\n",
    "HRV_TINN = FEParameter('HRV_TINN')\n",
    "# HTI: The HRV triangular index, measuring the total number of RR intervals divided by the height of the RR intervals histogram.\n",
    "HRV_HTI = FEParameter('HRV_HTI')\n",
    "\n",
    "# VLF: The spectral power (W/Hz) of very low frequencies (.0033 to .04 Hz).\n",
    "HRV_VLF = FEParameter('HRV_VLF', min=0.0, max=9) # hidden due to use of 0.5 Hz high-pass butterworth filter\n",
    "# LF: The spectral power (W/Hz) of low frequencies (.04 to .15 Hz).\n",
    "HRV_LF = FEParameter('HRV_LF', max=1.00)\n",
    "# HF: The spectral power (W/Hz) of high frequencies (.15 to .4 Hz).\n",
    "HRV_HF = FEParameter('HRV_HF', max=1.00)\n",
    "# LFHF: The ratio obtained by dividing the low frequency power by the high frequency power.\n",
    "HRV_LFHF = FEParameter('HRV_LFHF', max=1.00)\n",
    "\n",
    "# SDNN: The standard deviation of the RR intervals.\n",
    "# See https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5624990/ for chosen max value.\n",
    "HRV_SDNN = FEParameter('HRV_SDNN', max=200)\n",
    "# RMSSD: The square root of the mean of the squared successive differences between adjacent RR intervals. \n",
    "# # It is equivalent (although on another scale) to SD1, and therefore it is redundant to report correlations with both (Ciccone, 2017).\n",
    "# See https://help.welltory.com/en/articles/4413231-what-normal-ranges-and-measurement-standards-we-use-to-interpret-your-heart-rate-variability for chosen max value.\n",
    "HRV_RMSSD = FEParameter('HRV_RMSSD', max=200)\n",
    "# The root mean square of successive differences (RMSSD) divided by the mean of the RR intervals (MeanNN).\n",
    "cv_foldsSD = FEParameter('HRV_cv_foldsSD')\n",
    "# Shannon Entropy\n",
    "HRV_ShanEn = FEParameter('HRV_ShanEn')\n",
    "# Sample Entropy\n",
    "HRV_SampEn = FEParameter('HRV_SampEn')\n",
    "# DFA_alpha1: The monofractal detrended fluctuation analysis of the HR signal, corresponding to short-term correlations.\n",
    "HRV_DFA_alpha1 = FEParameter('HRV_DFA_alpha1')\n",
    "\n",
    "# ECG Intervals and Durations\n",
    "parameter_names = ['ECG_S_Peaks_Interval_Mean',\n",
    "                   'ECG_S_Peaks_Interval_SD',\n",
    "                   'ECG_T_Peaks_Interval_Mean',\n",
    "                   'ECG_T_Peaks_Interval_SD',\n",
    "                   'ECG_P_Duration_Mean',\n",
    "                   'ECG_P_Duration_SD',\n",
    "                   'ECG_T_Duration_Mean',\n",
    "                   'ECG_T_Duration_SD']\n",
    "\n",
    "# create new objects for each parameter name\n",
    "ECG_duration_intervals = []\n",
    "for name in parameter_names:\n",
    "    parameter = FEParameter(name)\n",
    "    ECG_duration_intervals.append(parameter)\n",
    "\n",
    "# PSD\n",
    "parameter_names = ['ECG_FQ_10', 'ECG_FQ_20', 'ECG_FQ_30', 'ECG_FQ_40', 'ECG_FQ_50', 'ECG_FQ_60', 'ECG_FQ_70', 'ECG_FQ_80', 'ECG_FQ_90', 'ECG_FQ_100', 'ECG_FQ_110', 'ECG_FQ_120', 'ECG_FQ_130', 'ECG_FQ_140', 'ECG_FQ_150', 'ECG_FQ_160', 'ECG_FQ_170', 'ECG_FQ_180', 'ECG_FQ_190', 'ECG_FQ_200', 'ECG_FQ_210']\n",
    "ECG_PSD = []\n",
    "for name in parameter_names:\n",
    "    parameter = FEParameter(name)\n",
    "    ECG_PSD.append(parameter)\n",
    "\n",
    "\n",
    "# EDR Distance: Breathing interval measured in ms.\n",
    "# Breathing rate range is approximately 12 to 25 per minute - see https://my.clevelandclinic.org/health/articles/10881-vital-signs\n",
    "EDR_Distance = FEParameter('EDR_Distance', min=2000, max=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs = FeatureSelection(fe.feature_extracted_ECG)\n",
    "fs = FeatureSelection(Utilities.load_dataframe(f'{dataframe_path}/Feature Extracted BP EDR 30s.csv'))\n",
    "''' Select Desired features for classification: '''\n",
    "# HRV\n",
    "selected_features = [HRV_MinNN, HRV_MaxNN, HRV_MeanNN, HRV_SDNN, HRV_RMSSD, HRV_ShanEn, HRV_DFA_alpha1, HRV_pNN20, HRV_pNN50] \n",
    "# Intervals and Durations\n",
    "selected_features.extend(ECG_duration_intervals)\n",
    "# PSD\n",
    "selected_features.extend(ECG_PSD)\n",
    "# EDR\n",
    "selected_features.append(EDR_Distance)\n",
    "\n",
    "fs.select(selected_features)\n",
    "fs.impute()\n",
    "# fs.visualise(plot_type='kdeplot', single_feature='ECG_P_Duration_Mean')\n",
    "# fs.visualise(plot_type='kdeplot')\n",
    "# fs.visualise(plot_type='pairplot')\n",
    "Utilities.save_dataframe(fs.selected_features_ECG, dataframe_path, 'Feature Selected')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Traditional Machine Learning Methods\n",
    "* Prepares data by randomly splitting data into train, test and validation data\n",
    "* Option for Linear Discriminant Analysis (LDA) for dimension reduction\n",
    "* Implements the following classification models, with methods for tuning: LDA, Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure of dataset that will be passed to Models as object\n",
    "class Dataset():\n",
    "    def __init__(self, X, y, X_train, y_train, X_test, y_test, sss, num_of_labels):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.sss = sss\n",
    "        self.num_of_labels = num_of_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities for preparing and evaluating classification\n",
    "class ML_Utilities():  \n",
    "    # corrects imbalanced dataset using SMOTE, and splits into n_splitted stratified shuffle split .\n",
    "    def prepare(selected_features_ECG:pd.DataFrame, num_of_labels, test_size:float, n_splits:int,  normalise=True) -> Dataset:     \n",
    "        # remove medium labels for binary classification\n",
    "        if num_of_labels == 2:\n",
    "            selected_features_ECG = selected_features_ECG[selected_features_ECG['Stress Level'] != 2.0]\n",
    "\n",
    "        y = selected_features_ECG['Stress Level']\n",
    "        X = selected_features_ECG.loc[:, selected_features_ECG.columns != 'Stress Level']\n",
    "\n",
    "        # L2 normalization for each feature (if required)\n",
    "        if normalise:\n",
    "            X_normalized = pd.DataFrame(normalize(X.values, norm='l2', axis=0), columns=X.columns)\n",
    "            # Scale the values to be between 0 and 1 with MinMaxScaler\n",
    "            scaler = MinMaxScaler()\n",
    "            X_scaled = scaler.fit_transform(X_normalized)\n",
    "            # Replace X with the scaled values\n",
    "            X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "        print(\"Before imbalanced label correction (SMOTE):\")\n",
    "        low = y.value_counts()[1.0]/len(y)*100\n",
    "        print(f'    Low Stress in dataset: {low:.2f}%')\n",
    "        if num_of_labels == 3:\n",
    "            medium = y.value_counts()[2.0]/len(y)*100\n",
    "            print(f'    Medium Stress in dataset: {medium:.2f}%')\n",
    "        high = y.value_counts()[3.0]/len(y)*100\n",
    "        print(f'    High Stress in dataset: {high:.2f}%')\n",
    "        \n",
    "        # SMOTE class rebalance\n",
    "        X, y = SMOTE(sampling_strategy='not majority').fit_resample(X, y)\n",
    "\n",
    "        # Split the data into training and test sets\n",
    "        sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=15)\n",
    "        \n",
    "        # Obtain the training and testing sets for the first fold\n",
    "        for train_index, test_index in sss.split(X, y):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            break\n",
    "\n",
    "        X_train = X_train.reset_index(drop=True)\n",
    "        X_test = X_test.reset_index(drop=True)\n",
    "        y_train = y_train.reset_index(drop=True)\n",
    "        y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "        # create data structure\n",
    "        dataset = Dataset(X, y, X_train, y_train, X_test, y_test, sss, num_of_labels)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def plot_confustion_matrix(num_of_labels, y_test=0, y_pred=0, cm=0):\n",
    "        display_labels = ['Low', 'High'] if (num_of_labels==2) else ['Low', 'Medium', 'High']\n",
    "        # Confusion Matrix\n",
    "        default_font_size = plt.rcParams['font.size']\n",
    "        plt.rcParams.update({'font.size': default_font_size * 1.4})\n",
    "        if isinstance(cm, int):\n",
    "            ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=display_labels, normalize='true')\n",
    "        else:\n",
    "            disp = ConfusionMatrixDisplay(cm, display_labels=display_labels)\n",
    "            disp.plot(values_format='.2f')\n",
    "            plt.show()\n",
    "        plt.rcParams.update({'font.size': default_font_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotLearning(Callback):\n",
    "    \"\"\"\n",
    "    Callback to plot the learning curves of the model during training.\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.metrics = {}\n",
    "        for metric in logs:\n",
    "            self.metrics[metric] = []\n",
    "            \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Storing metrics\n",
    "        for metric in logs:\n",
    "            if metric in self.metrics:\n",
    "                self.metrics[metric].append(logs.get(metric))\n",
    "            else:\n",
    "                self.metrics[metric] = [logs.get(metric)]\n",
    "        \n",
    "        # Plotting\n",
    "        metrics = [x for x in logs if 'val' not in x]\n",
    "        \n",
    "        f, axs = plt.subplots(1, len(metrics), figsize=(15,5))\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            axs[i].plot(range(1, epoch + 2), \n",
    "                        self.metrics[metric], \n",
    "                        label='Train ' + metric)\n",
    "            if logs['val_' + metric]:\n",
    "                axs[i].plot(range(1, epoch + 2), \n",
    "                            self.metrics['val_' + metric], \n",
    "                            label='Test ' + metric)\n",
    "            axs[i].legend()\n",
    "            axs[i].grid()\n",
    "            axs[i].set_xlabel('Epochs')\n",
    "            if metric == 'accuracy':\n",
    "                axs[i].set_ylabel('Classification ' + metric.capitalize() + ' (%)')\n",
    "                axs[i].set_ylim([0,1])\n",
    "            elif metric == 'loss':\n",
    "                axs[i].set_ylabel('Classification ' + metric.capitalize())\n",
    "                axs[i].set_ylim([0,1.5])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.678855Z",
     "iopub.status.busy": "2023-03-28T14:15:57.678628Z",
     "iopub.status.idle": "2023-03-28T14:15:57.685391Z",
     "shell.execute_reply": "2023-03-28T14:15:57.685101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Traditional Machine Learning class, which implements different linear and non linear classification methods\n",
    "class Traditional_ML():\n",
    "    def __init__(self, dataset_binary:Dataset, dataset_three_level:Dataset, number_of_cores:int=1):\n",
    "        self.dataset_binary = dataset_binary\n",
    "        self.dataset_three_level = dataset_three_level\n",
    "        self.n_jobs = number_of_cores\n",
    "        self.model = 'RF'\n",
    "\n",
    "    # main tune method\n",
    "    def tune(self):\n",
    "        if self.model == 'NB':\n",
    "            tuning_func = lambda: self.NB_tuner()\n",
    "        elif self.model == 'SVM':\n",
    "            tuning_func = lambda: self.SVM_tuner()\n",
    "        elif self.model == 'RF':\n",
    "            tuning_func = lambda: self.RF_tuner()\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported model: {self.model}')\n",
    "        tuning_func()\n",
    "\n",
    "\n",
    "    # main classification method\n",
    "    def classify(self):\n",
    "        if self.model == 'NB':\n",
    "            tuning_func = lambda: self.NB_classifier()\n",
    "        elif self.model == 'SVM':\n",
    "            tuning_func = lambda: self.SVM_classifier()\n",
    "        elif self.model == 'RF':\n",
    "            tuning_func = lambda: self.RF_classifier()\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported model: {self.model}')\n",
    "        tuning_func()\n",
    "\n",
    "\n",
    "    def evaluate(self, model, dataset:Dataset, plot_CM=False):\n",
    "        # Cross-validation using Stratified ShuffleSplit \n",
    "        # Initialize the accuracy and confusion matrix lists that will be averaged\n",
    "        accuracies = []\n",
    "        confusion_matrices = []\n",
    "        thr_acc = []\n",
    "        tmr_acc = []\n",
    "        tlr_acc = []\n",
    "        run_times = []\n",
    "        for train_index, test_index in dataset.sss.split(dataset.X, dataset.y):\n",
    "            # Split into train / test data using the SSS as a guide\n",
    "            X_train, X_val = dataset.X.iloc[train_index], dataset.X.iloc[test_index]\n",
    "            y_train, y_val = dataset.y.iloc[train_index], dataset.y.iloc[test_index]\n",
    "\n",
    "            # Fit the classifier to the training data\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict on the validation data and calculate run-time\n",
    "            start_time = time.time()\n",
    "            y_pred = model.predict(X_val)\n",
    "            end_time = time.time()\n",
    "            run_times.append(end_time-start_time)\n",
    "\n",
    "            # Calculate the accuracy and append to the list\n",
    "            accuracy = accuracy_score(y_val, y_pred)\n",
    "            accuracies.append(accuracy)\n",
    "            thr_acc.append(accuracy_score(y_val[y_val == 3.0], y_pred[y_val == 3.0]) * 100)\n",
    "            if dataset.num_of_labels == 3:\n",
    "                tmr_acc.append(accuracy_score(y_val[y_val == 2.0], y_pred[y_val == 2.0]) * 100)\n",
    "            tlr_acc.append(accuracy_score(y_val[y_val == 1.0], y_pred[y_val == 1.0]) * 100)\n",
    "\n",
    "            # Calculate the confusion matrix and append to the list\n",
    "            cm = confusion_matrix(y_val, y_pred, normalize='true')\n",
    "            confusion_matrices.append(cm)\n",
    "\n",
    "        # Calculate the mean and standard deviation of the accuracy across all splits\n",
    "        mean_accuracy = np.mean(accuracies)\n",
    "        std_accuracy = np.std(accuracies)\n",
    "\n",
    "        print(f\"Mean Accuracy for {dataset.num_of_labels} level classification: {mean_accuracy:.3f} +/- {std_accuracy:.3f}\")\n",
    "\n",
    "        # print results\n",
    "        print(f\"    Accuracy for True High Rate (THR): {np.mean(thr_acc):.2f}%\")\n",
    "        if dataset.num_of_labels == 3:\n",
    "            print(f\"    Accuracy for True Medium Rate (TMR): {np.mean(tmr_acc):.2f}%\")\n",
    "        print(f\"    Accuracy for True Low Rate (TLR): {np.mean(tlr_acc):.2f}%\")\n",
    "        print(f\"    Average run-time: {np.mean(run_times)}\")\n",
    "\n",
    "        # Calculate the average confusion matrix\n",
    "        mean_cm = np.mean(confusion_matrices, axis=0)\n",
    "\n",
    "        # Calculate precision, recall, and F1 score\n",
    "        precision = np.diag(mean_cm) / np.sum(mean_cm, axis=0)\n",
    "        recall = np.diag(mean_cm) / np.sum(mean_cm, axis=1)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        # Print the average F1 score\n",
    "        print(f\"    Average F1 Score: {np.mean(f1_score)}\")\n",
    "\n",
    "        # plot confusion matrix if desired\n",
    "        if plot_CM:\n",
    "            ML_Utilities.plot_confustion_matrix(dataset.num_of_labels, cm=mean_cm)\n",
    "\n",
    "        # return as percentage\n",
    "        return mean_accuracy*100, std_accuracy*100\n",
    "        \n",
    "\n",
    "    # Naive Bayes Classifier\n",
    "    def NB_tuner(self):\n",
    "        print(\"\\nNaive Bayes Classifier:\")\n",
    "\n",
    "        # calculate training class probabilities for Gaussian NB\n",
    "        low = self.dataset_three_level.y_train.value_counts()[1.0]/len(self.dataset_three_level.y_train)\n",
    "        if self.dataset_three_level.num_of_labels == 3:\n",
    "            medium = self.dataset_three_level.y_train.value_counts()[2.0]/len(self.dataset_three_level.y_train)\n",
    "        high = self.dataset_three_level.y_train.value_counts()[3.0]/len(self.dataset_three_level.y_train)\n",
    "        Gaussian_priors = [low, medium, high] if self.dataset_three_level.num_of_labels == 3 else [low, high]\n",
    "\n",
    "        # create a dictionary of classifiers\n",
    "        classifiers = {'Multinomial': MultinomialNB(),\n",
    "                       'Bernoulli': BernoulliNB(),\n",
    "                       'Complement': ComplementNB(),\n",
    "                       'Gaussian': GaussianNB()}\n",
    "        \n",
    "        # set up a parameter grid for each classifier\n",
    "        param_grids = {'Multinomial': {'alpha': [0.01, 0.1, 0.5, 1.0, 5.0, 10.0]},\n",
    "                       'Bernoulli': {'alpha': [0.01, 0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "                                     'binarize': [0.0, 0.5, 1.0]},\n",
    "                       'Complement': {'alpha': [0.01, 0.1, 0.5, 1.0, 5.0, 10.0]},\n",
    "                        'Gaussian': {'priors': [Gaussian_priors]}\n",
    "                       }\n",
    "\n",
    "        accuracies_binary = []\n",
    "        std_devs_binary = []\n",
    "        accuracies_three = []\n",
    "        std_devs_three = []\n",
    "        for name, nb in classifiers.items():\n",
    "            # perform a grid search to find the best hyperparameters for each classifier\n",
    "            grid_search = GridSearchCV(nb, param_grid=param_grids[name])\n",
    "            grid_search.fit(self.dataset_three_level.X_train, self.dataset_three_level.y_train)\n",
    "\n",
    "            # print the best hyperparameters for each classifier\n",
    "            print('\\nBest hyperparameters for {}: {}'.format(name, grid_search.best_params_))\n",
    "\n",
    "            # perform a 5 fold cross validation using the best grid search for both 2 and 3 level classification\n",
    "            accuracy, std_dev = self.evaluate(grid_search.best_estimator_, dataset=self.dataset_three_level, plot_CM=True)\n",
    "            accuracies_three.append(accuracy)\n",
    "            std_devs_three.append(std_dev)\n",
    "            if name == 'Gaussian':\n",
    "                grid_search.best_estimator_.priors = [0.5,0.5]\n",
    "            accuracy, std_dev = self.evaluate(grid_search.best_estimator_, dataset=self.dataset_binary, plot_CM=True)\n",
    "            accuracies_binary.append(accuracy)\n",
    "            std_devs_binary.append(std_dev)\n",
    "\n",
    "        # Set the width of each bar\n",
    "        bar_width = 0.35\n",
    "\n",
    "        # Set the positions of the bars on the x-axis\n",
    "        r1 = np.arange(len(classifiers.keys()))\n",
    "        r2 = [x + bar_width for x in r1]\n",
    "\n",
    "        # Create the bar chart\n",
    "        plt.bar(r1, accuracies_binary, width=bar_width, color='paleturquoise', yerr=std_devs_binary, capsize=5, label='Binary classification')\n",
    "        plt.bar(r2, accuracies_three, width=bar_width, color='darkslategray', yerr=std_devs_three, capsize=5, label='Three-level classification')\n",
    "\n",
    "        # Add labels, title, and legend\n",
    "        plt.xlabel('Classifier')\n",
    "        plt.xticks([r + bar_width/2 for r in range(len(classifiers.keys()))], classifiers.keys())\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.ylim([0, 100])\n",
    "        plt.legend()\n",
    "\n",
    "        # Show the bar chart\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # Post tuned Naive Bayes Classifer\n",
    "    def NB_classifier(self):\n",
    "        # binary classification\n",
    "        low = self.dataset_binary.y_train.value_counts()[1.0]/len(self.dataset_binary.y_train)\n",
    "        high = self.dataset_binary.y_train.value_counts()[3.0]/len(self.dataset_binary.y_train)\n",
    "        Gaussian_priors = [low, high]\n",
    "        nb = GaussianNB(priors=Gaussian_priors)\n",
    "        self.evaluate(nb, self.dataset_binary) \n",
    "\n",
    "        # three-level classification\n",
    "        low = self.dataset_three_level.y_train.value_counts()[1.0]/len(self.dataset_three_level.y_train)\n",
    "        medium = self.dataset_three_level.y_train.value_counts()[2.0]/len(self.dataset_three_level.y_train)\n",
    "        high = self.dataset_three_level.y_train.value_counts()[3.0]/len(self.dataset_three_level.y_train)\n",
    "        Gaussian_priors = [low, medium, high]\n",
    "        nb = GaussianNB(priors=Gaussian_priors)\n",
    "        self.evaluate(nb, self.dataset_three_level)\n",
    "        \n",
    "\n",
    "    # Support Vector Machine Classifier \n",
    "    def SVM_tuner(self):\n",
    "        print(\"\\nSVM Classifier:\")\n",
    "        \n",
    "        # create a dictionary of kernels\n",
    "        kernels = {'Linear': 'linear',\n",
    "                   'Polynomial': 'poly',\n",
    "                   'Radial basis function': 'rbf',\n",
    "                   'Sigmoid': 'sigmoid'}\n",
    "        \n",
    "        # set up a parameter grid for each kernel\n",
    "        param_grids = {'Linear': {'C': [0.1, 1, 10, 100, 1000]},\n",
    "                       'Polynomial': {'C': [0.1, 1, 10, 100, 1000],\n",
    "                                      'degree': [2, 3, 4],\n",
    "                                      'gamma': [0.1, 0.01]},\n",
    "                       'Radial basis function': {'C': [0.1, 1, 10, 100, 1000],\n",
    "                                                 'gamma': [0.1, 0.01, 0.001]},\n",
    "                       'Sigmoid': {'C': [0.1, 1, 10, 100, 1000],\n",
    "                                    'gamma': [0.1, 0.01, 0.001],\n",
    "                                    'coef0': [0.1, 0.01, 0.001]}\n",
    "                      }\n",
    "\n",
    "        accuracies_binary = []\n",
    "        std_devs_binary = []\n",
    "        accuracies_three = []\n",
    "        std_devs_three = []\n",
    "        dataset = self.dataset_binary\n",
    "        for name, kernel in kernels.items():\n",
    "            # perform a grid search to find the best hyperparameters for each kernel\n",
    "            grid_search = GridSearchCV(SVC(kernel=kernel), param_grid=param_grids[name], n_jobs=self.n_jobs)\n",
    "            grid_search.fit(dataset.X_train, dataset.y_train)\n",
    "\n",
    "            # print the best hyperparameters for each kernel\n",
    "            print('\\nBest hyperparameters for {}: {}'.format(name, grid_search.best_params_))\n",
    "\n",
    "            # perform a 5 fold cross validation using the best grid search for both 2 and 3 level classification\n",
    "            accuracy, std_dev = self.evaluate(grid_search.best_estimator_, dataset=self.dataset_three_level, plot_CM=True)\n",
    "            accuracies_three.append(accuracy)\n",
    "            std_devs_three.append(std_dev)\n",
    "            accuracy, std_dev = self.evaluate(grid_search.best_estimator_, dataset=self.dataset_binary, plot_CM=True)\n",
    "            accuracies_binary.append(accuracy)\n",
    "            std_devs_binary.append(std_dev)\n",
    "\n",
    "        # Set the width of each bar\n",
    "        bar_width = 0.35\n",
    "\n",
    "        # Set the positions of the bars on the x-axis\n",
    "        r1 = np.arange(len(kernels.keys()))\n",
    "        r2 = [x + bar_width for x in r1]\n",
    "\n",
    "        # Create the bar chart\n",
    "        plt.bar(r1, accuracies_binary, width=bar_width, color='lightsalmon', yerr=std_devs_binary, capsize=5, label='Binary classification')\n",
    "        plt.bar(r2, accuracies_three, width=bar_width, color='orangered', yerr=std_devs_three, capsize=5, label='Three-level classification')\n",
    "\n",
    "        # Add labels, title, and legend\n",
    "        plt.xlabel('Classifier')\n",
    "        plt.xticks([r + bar_width/2 for r in range(len(kernels.keys()))], kernels.keys())\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.ylim([0, 100])\n",
    "        plt.legend()\n",
    "\n",
    "        # Show the bar chart\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # Post-tuned Support Vector Machine Classifier \n",
    "    def SVM_classifier(self):\n",
    "        # binary classification\n",
    "        svm = SVC(kernel='rbf', C=1000, gamma=0.1, random_state=15)\n",
    "        self.evaluate(svm, self.dataset_binary) \n",
    "\n",
    "        # three-level classification\n",
    "        svm = SVC(kernel='poly', C=1000, degree=4, gamma=0.1, random_state=15)\n",
    "        self.evaluate(svm, self.dataset_three_level) \n",
    "\n",
    "    \n",
    "    # Random Forest Tuner\n",
    "    def RF_tuner(self):\n",
    "        print(\"\\nRandom Forest Classifier:\")\n",
    "        \n",
    "        # define the criterion options\n",
    "        criterions = {'Gini Index': 'gini',\n",
    "                    'Entropy': 'entropy',\n",
    "                    'Logarithmic Loss': 'log_loss'}\n",
    "\n",
    "        # define the parameter grid\n",
    "        param_grid = {'n_estimators': [10, 50, 100, 250, 500],\n",
    "                    'max_depth': [None, 2, 3, 4, 5, 7],\n",
    "                    'min_samples_split': [2, 5, 10],\n",
    "                    'min_samples_leaf': [1, 2, 4],\n",
    "                    'max_features': [1, 'sqrt', 'log2', None],\n",
    "                    'bootstrap': [True, False]}\n",
    "\n",
    "        accuracies_binary = []\n",
    "        std_devs_binary = []\n",
    "        accuracies_three = []\n",
    "        std_devs_three = []\n",
    "        dataset = self.dataset_three_level\n",
    "\n",
    "        for name, criterion in criterions.items():\n",
    "            # create a random forest classifier with the current criterion\n",
    "            rf = RandomForestClassifier(criterion=criterion)\n",
    "\n",
    "            # perform a grid search to find the best hyperparameters for the classifier\n",
    "            grid_search = GridSearchCV(rf, param_grid=param_grid, n_jobs=self.n_jobs)\n",
    "            grid_search.fit(dataset.X_train, dataset.y_train)\n",
    "\n",
    "            # print the best hyperparameters for the classifier\n",
    "            print('\\nBest hyperparameters for {}: {}'.format(name, grid_search.best_params_))\n",
    "\n",
    "            # evaluate the classifier on the test set\n",
    "            accuracy, std_dev = self.evaluate(grid_search.best_estimator_, dataset=self.dataset_three_level, plot_CM=True)\n",
    "            accuracies_three.append(accuracy)\n",
    "            std_devs_three.append(std_dev)\n",
    "            accuracy, std_dev = self.evaluate(grid_search.best_estimator_, dataset=self.dataset_binary, plot_CM=True)\n",
    "            accuracies_binary.append(accuracy)\n",
    "            std_devs_binary.append(std_dev)\n",
    "\n",
    "        # Set the width of each bar\n",
    "        bar_width = 0.35\n",
    "\n",
    "        # Set the positions of the bars on the x-axis\n",
    "        r1 = np.arange(len(criterions.keys()))\n",
    "        r2 = [x + bar_width for x in r1]\n",
    "\n",
    "        # Create the bar chart\n",
    "        plt.bar(r1, accuracies_binary, width=bar_width, color='greenyellow', yerr=std_devs_binary, capsize=5, label='Binary classification')\n",
    "        plt.bar(r2, accuracies_three, width=bar_width, color='forestgreen', yerr=std_devs_three, capsize=5, label='Three-level classification')\n",
    "\n",
    "        # Add labels, title, and legend\n",
    "        plt.xlabel('Criterion')\n",
    "        plt.xticks([r + bar_width/2 for r in range(len(criterions.keys()))], criterions.keys())\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.ylim([0, 100])\n",
    "        plt.legend()\n",
    "\n",
    "        # Show the bar chart\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # Post-tuned Random Forest Classifier\n",
    "    def RF_classifier(self):\n",
    "        params = {'criterion': 'gini', 'bootstrap': False, 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
    "        rf = RandomForestClassifier(**params, random_state=15)\n",
    "\n",
    "        # binary classification\n",
    "        self.evaluate(rf, self.dataset_binary) \n",
    "\n",
    "        # three-level classification\n",
    "        self.evaluate(rf, self.dataset_three_level) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.686647Z",
     "iopub.status.busy": "2023-03-28T14:15:57.686556Z",
     "iopub.status.idle": "2023-03-28T14:15:58.903839Z",
     "shell.execute_reply": "2023-03-28T14:15:58.903584Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_features_ECG = Utilities.load_dataframe(f'{dataframe_path}/Feature Selected.csv')\n",
    "\n",
    "# make two datasets - binary and three-level classification\n",
    "dataset_binary = ML_Utilities.prepare(selected_features_ECG, num_of_labels=2, test_size=0.30, n_splits=5, normalise=True)\n",
    "dataset_three_level = ML_Utilities.prepare(selected_features_ECG, num_of_labels=3, test_size=0.30, n_splits=5, normalise=True)\n",
    "\n",
    "tml = Traditional_ML(dataset_binary, dataset_three_level, number_of_cores=1)\n",
    "\n",
    "# 'NB' for Naive Bayes\n",
    "# 'SVM' for Support Vector Machine\n",
    "# 'RF' for Random Forest\n",
    "tml.model = 'SVM'\n",
    "# tml.tune()\n",
    "tml.classify()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Convolutional Neural Network\n",
    "* Prepares data by converting the sorted data into a 3 dimensional shape\n",
    "* 1D Convolutional Neural Network tuning and regression model\n",
    "* Regression is converted to discrete classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 dimensional Convolutional Deep Neural Network\n",
    "class CNN1D():\n",
    "    def __init__(self, dataset:Dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "\n",
    "    # prepare data by setting shape to 3 dimensions\n",
    "    def prepare(self):\n",
    "        # get shape of data\n",
    "        self.n_features, self.n_classes = self.dataset.X_train.shape[1], self.dataset.y_train.nunique()\n",
    "\n",
    "        # Format to a NumPy array of 32-bit floating-point numbers and reshape to a 3D array with dimensions (number of samples, number of features, 1)\n",
    "        self.dataset.X = np.asarray(self.dataset.X).astype(np.float32).reshape(-1, self.n_features, 1)\n",
    "        self.dataset.X_test = np.asarray(self.dataset.X_test).astype(np.float32).reshape(-1, self.n_features, 1)\n",
    "        self.dataset.X_train = np.asarray(self.dataset.X_train).astype(np.float32).reshape(-1, self.n_features, 1)\n",
    "        self.dataset.y = np.asarray(self.dataset.y).astype(np.float32)\n",
    "        self.dataset.y_test = np.asarray(self.dataset.y_test).astype(np.float32)\n",
    "        self.dataset.y_train = np.asarray(self.dataset.y_train).astype(np.float32)\n",
    "\n",
    "        if self.n_classes == 2:\n",
    "            # Convert 1.0 to 0 and 3.0 to 1\n",
    "            self.dataset.y = np.where(self.dataset.y == 1.0, 0, 1)\n",
    "            self.dataset.y_train = np.where(self.dataset.y_train == 1.0, 0, 1)\n",
    "            self.dataset.y_test = np.where(self.dataset.y_test == 1.0, 0, 1)\n",
    "\n",
    "        elif self.n_classes == 3:\n",
    "            # Replace 1.0 with 0, 2.0 with 1, and 3.0 with 2\n",
    "            self.dataset.y = np.where(self.dataset.y == 1.0, 0, np.where(self.dataset.y == 2.0, 1, np.where(self.dataset.y == 3.0, 2, self.dataset.y)))\n",
    "            self.dataset.y_train = np.where(self.dataset.y_train == 1.0, 0, np.where(self.dataset.y_train == 2.0, 1, np.where(self.dataset.y_train == 3.0, 2, self.dataset.y_train)))\n",
    "            self.dataset.y_test = np.where(self.dataset.y_test == 1.0, 0, np.where(self.dataset.y_test == 2.0, 1, np.where(self.dataset.y_test == 3.0, 2, self.dataset.y_test)))\n",
    "    \n",
    "        # convert to one-hot encoding\n",
    "        self.dataset.y = to_categorical(self.dataset.y)\n",
    "        self.dataset.y_train = to_categorical(self.dataset.y_train)\n",
    "        self.dataset.y_test = to_categorical(self.dataset.y_test)\n",
    "    \n",
    "\n",
    "    def convert_to_labels(self, y_true, y_pred):\n",
    "        # convert from regression to class label\n",
    "        max_indices = np.argmax(y_pred, axis=1)\n",
    "        result = np.eye(y_pred.shape[1])[max_indices]\n",
    "        # Convert the one-hot vectors back to their original labels\n",
    "        if self.dataset.num_of_labels == 3:\n",
    "            labels = [1.0, 2.0, 3.0]\n",
    "        else:\n",
    "            labels = [1.0, 3.0]\n",
    "        label_indices = np.argmax(result, axis=1)\n",
    "        y_pred_l = [labels[i] for i in label_indices]\n",
    "\n",
    "        label_indices = np.argmax(y_true, axis=1)\n",
    "        y_true_l = [labels[i] for i in label_indices]     \n",
    "\n",
    "        return y_true_l, y_pred_l \n",
    "    \n",
    "\n",
    "    def tune(self):        \n",
    "        # hyperparameters\n",
    "        learning_rate, epochs, batch_size = 0.00001, 1000, 32\n",
    "\n",
    "        # Define the model architecture \n",
    "        self.model = Sequential([\n",
    "            # Input layer (feature (input) length is 3000 samples - assuming a sampling frequency of 1kHz)\n",
    "            InputLayer(input_shape=(self.n_features, 1)),\n",
    "            \n",
    "            # 1D Convolutional layer (kernel size 600)\n",
    "            Conv1D(filters=64, kernel_size=int(pp.sampling_frequency*0.6), activation='relu'),\n",
    "            \n",
    "            # Max Pooling layer (pooling length 800)\n",
    "            MaxPooling1D(pool_size=int(pp.sampling_frequency*0.8)),\n",
    "            \n",
    "            # Flatten layer\n",
    "            Flatten(),\n",
    "            \n",
    "            # Dropout layer\n",
    "            Dropout(0.1),\n",
    "            \n",
    "            # Fully Connected layer 1\n",
    "            Dense(units=128, activation='relu'),\n",
    "            \n",
    "            # Fully Connected layer 2 to number of class outputs\n",
    "            Dense(units=self.n_classes, activation='softmax')\n",
    "        ])\n",
    "\n",
    "\n",
    "        # print model architecture summary\n",
    "        self.model.summary()\n",
    "\n",
    "        # Compile the model using the Adam Optimiser\n",
    "        self.model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Train the model\n",
    "        self.model.fit(self.dataset.X_train, self.dataset.y_train, epochs=epochs, batch_size=batch_size, validation_data=(self.dataset.X_test, self.dataset.y_test), callbacks=[PlotLearning()])\n",
    "\n",
    "        # Evaluate the model\n",
    "        score = self.model.evaluate(self.dataset.X_test, self.dataset.y_test, batch_size=batch_size)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "\n",
    "        start_time = time.time()\n",
    "        y_pred = self.model.predict(self.dataset.X_test)\n",
    "        end_time = time.time()\n",
    "        run_time = end_time-start_time\n",
    "        print(f\"Run time: {run_time}\")\n",
    "        y_true_l, y_pred_l = self.convert_to_labels(self.dataset.y_test, y_pred)\n",
    "\n",
    "        # Calculate precision, recall, and F1 score\n",
    "        cm = confusion_matrix(y_true_l, y_pred_l, normalize='true')\n",
    "        precision = np.diag(cm) / np.sum(cm, axis=0)\n",
    "        recall = np.diag(cm) / np.sum(cm, axis=1)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        # Print the average F1 score\n",
    "        print(f\"F1 Score: {f1_score}\")\n",
    "        \n",
    "        # plot confusion matrix\n",
    "        ML_Utilities.plot_confustion_matrix(self.dataset.num_of_labels, y_true_l, y_pred_l)\n",
    "\n",
    "\n",
    "    # post-tuned classifier with stratified shuffle split evaluation and confusion matrix\n",
    "    def predict(self, n_splits=5):\n",
    "        print(f\"Conducting {n_splits} Cross Validation Folds\")\n",
    "\n",
    "        # split the dataset into training and validation sets using StratifiedShuffleSplit\n",
    "        sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.3, random_state=15)\n",
    "        accuracies = []        \n",
    "        confusion_matrices = []\n",
    "        thr_acc = []\n",
    "        tmr_acc = []\n",
    "        tlr_acc = []\n",
    "        for fold, (train_index, val_index) in enumerate(sss.split(self.dataset.X, self.dataset.y)):\n",
    "            print('Fold', fold+1)\n",
    "            X_val, y_val = self.dataset.X[val_index], self.dataset.y[val_index]\n",
    "\n",
    "            # Evaluate the model on the test set for this fold and store the loss and accuracy scores\n",
    "            loss, accuracy = self.model.evaluate(X_val, y_val, verbose=0)\n",
    "            accuracies.append([loss, accuracy])\n",
    "\n",
    "            # predict on validation set for this fold\n",
    "            y_pred = self.model.predict(X_val)\n",
    "            y_val_l, y_pred_l = self.convert_to_labels(y_val, y_pred)\n",
    "            # append confusion matrix to list\n",
    "            confusion_matrices.append(confusion_matrix(y_val_l, y_pred_l, normalize='true'))\n",
    "\n",
    "            y_val_np = np.array(y_val_l)\n",
    "            y_pred_np = np.array(y_pred_l)\n",
    "\n",
    "            thr_acc.append(accuracy_score(y_val_np[y_val_np == 3.0], y_pred_np[y_val_np == 3.0]) * 100)\n",
    "            if self.dataset.num_of_labels == 3:\n",
    "                tmr_acc.append(accuracy_score(y_val_np[y_val_np == 2.0], y_pred_np[y_val_np == 2.0]) * 100)\n",
    "            tlr_acc.append(accuracy_score(y_val_np[y_val_np == 1.0], y_pred_np[y_val_np == 1.0]) * 100)\n",
    "            \n",
    "        # Calculate the average loss and accuracy scores across all folds\n",
    "        avg_loss = 100 * sum(accuracy[0] for accuracy in accuracies) / len(accuracies)\n",
    "        avg_accuracy = 100 * sum(accuracy[1] for accuracy in accuracies) / len(accuracies)\n",
    "\n",
    "        # print results\n",
    "        print(f'    Average test loss: {avg_loss:2f}')\n",
    "        print(f'    Average test accuracy: {avg_accuracy:2f}')\n",
    "        print(f\"    Accuracy for True High Rate (THR): {np.mean(thr_acc):.2f}%\")\n",
    "        if self.dataset.num_of_labels == 3:\n",
    "            print(f\"    Accuracy for True Medium Rate (TMR): {np.mean(tmr_acc):.2f}%\")\n",
    "        print(f\"    Accuracy for True Low Rate (TLR): {np.mean(tlr_acc):.2f}%\")\n",
    "\n",
    "        # Calculate and plot the mean confusion matrix across all splits\n",
    "        mean_cm = np.mean(confusion_matrices, axis=0)\n",
    "        precision = np.diag(mean_cm) / np.sum(mean_cm, axis=0)\n",
    "        recall = np.diag(mean_cm) / np.sum(mean_cm, axis=1)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        # Print the average F1 score\n",
    "        print(f\"F1 Score: {np.mean(f1_score)}\")\n",
    "        ML_Utilities.plot_confustion_matrix(self.dataset.num_of_labels, cm=mean_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment data with a sliding window of length to be 0.8s which is average heart beat\n",
    "pp.segment(window_length_s=3.0, overlap=0.1) \n",
    "pp.clean() \n",
    "pp.create_2d() # converts data to 2d vector (required for CNN)\n",
    "Utilities.save_dataframe(pp.preprocessed_ECG_2d, dataframe_path, 'Preprocessed_2d_3s_0.1_Overlap')\n",
    "\n",
    "# make two datasets - binary and three-level classification\n",
    "# dataset_binary = ML_Utilities.prepare(pp.preprocessed_ECG_2d, num_of_labels=2, test_size=0.30, n_splits=5, normalise=True)\n",
    "dataset_three_level = ML_Utilities.prepare(pp.preprocessed_ECG_2d, num_of_labels=3, test_size=0.30, n_splits=5, normalise=True)\n",
    "\n",
    "# Tune and classify the one dimensional convolutional deep neural network\n",
    "cnn = CNN1D(dataset_three_level)\n",
    "cnn.prepare()\n",
    "cnn.tune()\n",
    "# cnn.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
