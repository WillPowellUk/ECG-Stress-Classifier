{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stress Classifier\n",
    "## Overview\n",
    "1. [Select Database](#1-select-database)\n",
    "2. [Import Libraries and Load Useful Utilities](#2-import-libraries-and-load-useful-utilities)\n",
    "3. [Data Extraction](#3-data-extraction): Downloads and sorts through databases.\n",
    "4. [Pre-processing](#4-pre-processing): filtering and signal cleaning.\n",
    "5. [Feature Extraction](#5-feature-extraction): R-R peaks, PQRST peaks, EDR, in addition to mean, kurtosis etc.\n",
    "6. [Feature Selection](#6-feature-selection): Visualise labelled feature distribution, select desired labels etc.\n",
    "7. [Traditional Machine Learning Methods](#7-traditional-machine-learning-methods): Linear classification including Random Forests, LDA, Bagged Trees and linear/non-linear SVM.\n",
    "8. [Convolutional Neural Network](#8-convolutional-neural-network): 1-D convolutional neural network with automatic feature extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select Database\n",
    "Select between:\n",
    "* [Spider-Fearful ECG](https://doi.org/10.1371/journal.pone.0231517) \n",
    "* BrainPatch ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:56.740564Z",
     "iopub.status.busy": "2023-03-28T14:15:56.740124Z",
     "iopub.status.idle": "2023-03-28T14:15:56.746295Z",
     "shell.execute_reply": "2023-03-28T14:15:56.745553Z"
    }
   },
   "outputs": [],
   "source": [
    "# DATABASE = 'Spider'\n",
    "DATABASE = 'BrainPatch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Load Useful Utilities\n",
    "Please run `pip install -r requirements.txt` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:56.748730Z",
     "iopub.status.busy": "2023-03-28T14:15:56.748407Z",
     "iopub.status.idle": "2023-03-28T14:15:57.623281Z",
     "shell.execute_reply": "2023-03-28T14:15:57.623003Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, GridSearchCV, ShuffleSplit\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, roc_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "import multiprocessing as mp\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import subprocess\n",
    "import warnings\n",
    "from itertools import product\n",
    "from typing import List\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.624839Z",
     "iopub.status.busy": "2023-03-28T14:15:57.624743Z",
     "iopub.status.idle": "2023-03-28T14:15:57.631111Z",
     "shell.execute_reply": "2023-03-28T14:15:57.630859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Useful Utilities\n",
    "class Utilities():\n",
    "    def __init__(self):    \n",
    "        pass\n",
    "\n",
    "    def progress_bar(current_message, current, total, bar_length=20):\n",
    "        fraction = current / total\n",
    "        arrow = int(fraction * bar_length - 1) * '-' + '>'\n",
    "        padding = int(bar_length - len(arrow)) * ' '\n",
    "        ending = '\\n' if current == total else '\\r'\n",
    "        print(f'{current_message}: [{arrow}{padding}] {int(fraction*100)}%', end=ending)\n",
    "\n",
    "\n",
    "    def check_csv_exists(folder_path, sample_index):\n",
    "        # read the CSV file into a dataframe and append to the list\n",
    "        filename = os.path.join(folder_path, f'df_{index}.csv')\n",
    "        try:\n",
    "            df = pd.read_csv(filename)\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "        return filename\n",
    "\n",
    "\n",
    "    def load_dataframe(filename):\n",
    "        # read the CSV file into a dataframe and append to the list\n",
    "        df = pd.read_csv(filename)\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def save_dataframe_list(list_of_dfs:List[pd.DataFrame], folder_path:str, file_name:str):\n",
    "        # create directoy if necessary\n",
    "        os.makedirs(folder_path, exist_ok=True) \n",
    "        for i, df in enumerate(list_of_dfs):\n",
    "            file_path = f\"{folder_path}/{file_name}_{i}.csv\"\n",
    "            df.to_csv(file_path, index=False)\n",
    "\n",
    "\n",
    "    def save_dataframe(df:pd.DataFrame, folder_path:str, file_name:str):\n",
    "        print(f\"Saving Dataframe to: {folder_path}/{file_name}.csv...\", end='')\n",
    "        # create directoy if necessary\n",
    "        os.makedirs(folder_path, exist_ok=True) \n",
    "        df.to_csv(f'{folder_path}/{file_name}.csv', index=False)\n",
    "        print(\"Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For bigger plots\n",
    "plt.rcParams['figure.figsize'] = [10, 6]  \n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Extraction\n",
    "* Downloads data, normalizes timeframe, attaches labels, and saves sorted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.632504Z",
     "iopub.status.busy": "2023-03-28T14:15:57.632310Z",
     "iopub.status.idle": "2023-03-28T14:15:57.638674Z",
     "shell.execute_reply": "2023-03-28T14:15:57.638402Z"
    }
   },
   "outputs": [],
   "source": [
    "# This class handles downloading and sorting the Spider database.\n",
    "# Method from this paper https://doi.org/10.1371/journal.pone.0231517\n",
    "class SpiderDataExtraction():\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory + 'Spider'\n",
    "        self.sampling_frequency = 100\n",
    "        self.sorted_ECG = pd.DataFrame(columns=['Timestamp', 'ECG', 'Stress Level'])\n",
    "\n",
    "    \n",
    "    # Downloads and extracts database \n",
    "    def download_data(self):\n",
    "        if sys.platform == 'linux':\n",
    "            url = 'https://physionet.org/files/ecg-spider-clip/1.0.0/'\n",
    "            print(\"Downloading database...this may take a while\")\n",
    "            os.makedirs(self.directory, exist_ok=True)\n",
    "            cmd = f\"wget -r -N -c -np -P {self.directory} {url}\"\n",
    "            subprocess.run(cmd)\n",
    "        else:\n",
    "            sys.exit(\"Unable to download database. If you are running Windows/Mac please download manually via https://physionet.org/content/ecg-spider-clip/1.0.0/\")\n",
    "    \n",
    "\n",
    "    # sorts data from each participant, labelling each ECG recording and appends to one dataframe.\n",
    "    # Following the SB approach in the study.\n",
    "    def sort_data(self):       \n",
    "        directory = self.directory + '/physionet.org/files/ecg-spider-clip/1.0.0/'\n",
    "        \n",
    "        # Exclude VP70 because of noise\n",
    "        sub_directories = ['VP02', 'VP03','VP05','VP06','VP08','VP09','VP11','VP12','VP14','VP15','VP17','VP18','VP20','VP23','VP24','VP26','VP27',\n",
    "                'VP29','VP30','VP32','VP33','VP35','VP36','VP38','VP39','VP41','VP42','VP44','VP45','VP47','VP48','VP50','VP51','VP53',\n",
    "                'VP54','VP56','VP57','VP59','VP61','VP62','VP63','VP64','VP65','VP66','VP68','VP69','VP71','VP72','VP73','VP74',\n",
    "                'VP75','VP76','VP77','VP78','VP79','VP80']\n",
    "        \n",
    "        # Path to Ratings file for all particpants\n",
    "        subjective_ratings_file = f'{self.directory}/Subjective Ratings.txt'\n",
    "\n",
    "        # Read in the subject ratings file (ignore arousal markers, interested in angst)\n",
    "        ratings_df = (\n",
    "            pd.read_csv(subjective_ratings_file, sep='\\t', names=['Subject','Group','Session', '4', '8', '12', '16', 'NA1', 'NA2', 'NA3', 'NA4'], encoding='UTF-16')\n",
    "            .drop(columns=['NA1', 'NA2', 'NA3', 'NA4'])\n",
    "            .iloc[1:]\n",
    "            .reset_index(drop=True)\n",
    "            .astype(int)\n",
    "        )\n",
    "\n",
    "        for index, sub_directory in enumerate(sub_directories):\n",
    "            Utilities.progress_bar('Sorting database', index, len(sub_directories)-1)\n",
    "\n",
    "            # set participant data paths\n",
    "            ECG_file = f'{directory}{sub_directory}/BitalinoECG.txt'\n",
    "            triggers_file = f'{directory}{sub_directory}/Triggers.txt'\n",
    "\n",
    "            # Get participant number\n",
    "            participant_no = int(sub_directory[2:])\n",
    "\n",
    "            # read in particpant ECG raw data file and reorder to get columns Timestamp, ECG \n",
    "            raw_df = pd.read_csv(ECG_file, sep='\\t', names = ['ECG','Timestamp','NA'])\n",
    "            raw_df = raw_df.drop(columns=['NA'])\n",
    "            raw_df = raw_df[['Timestamp', 'ECG']]\n",
    "            \n",
    "            # Read in participant trigger file\n",
    "            triggers_df = pd.read_csv(triggers_file, sep='\\t', names = ['Clip','On','Off'])\n",
    "\n",
    "            # Determine stress levels by correspoding the raw ecg data with the triggers file and the Subjective Ratings file.\n",
    "            # Iterate through the 16 stress clips (first clip is a demo):\n",
    "            for i in range(1, 17):\n",
    "                # Determine row in ratings file\n",
    "                row = ratings_df.loc[ratings_df['Subject'] == participant_no].index[0]\n",
    "\n",
    "                # find stress for the clip in the ratings file\n",
    "                stress_level = ratings_df.iloc[row]['4'] if i <= 4 else ratings_df.iloc[row]['8'] if i <= 8 else ratings_df.iloc[row]['12'] if i <= 12 else ratings_df.iloc[row]['16']\n",
    "\n",
    "                # convert stress level to Low, Medium or High (1-3)\n",
    "                stress_level = 2 if (stress_level <= 2) else 3\n",
    "\n",
    "                # Get 60 second slice of ECG data for that clip\n",
    "                clip_start_time = triggers_df.iloc[i]['On']\n",
    "                start_index = raw_df.index[raw_df['Timestamp']>clip_start_time].tolist()[0]\n",
    "                clip_df = raw_df.iloc[start_index:start_index + (self.sampling_frequency * 60)].copy(deep=False)\n",
    "                clip_df['Stress Level'] = stress_level\n",
    "                self.sorted_ECG = pd.concat([self.sorted_ECG, clip_df], axis=0, ignore_index=True)\n",
    "\n",
    "            # Add the last 3 minute resting phase (stress level Low) to the data\n",
    "            rest_start_time = triggers_df.iloc[-1]['On']\n",
    "            start_index = (raw_df['Timestamp'] > rest_start_time).idxmin() + (self.sampling_frequency * 120)\n",
    "            rest_df = raw_df.iloc[start_index: start_index + (self.sampling_frequency * 180)].copy(deep=False)\n",
    "            rest_df.loc[:, 'Stress Level'] = 1\n",
    "            self.sorted_ECG = pd.concat([self.sorted_ECG, rest_df], axis=0, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class handles sorting through the data collected using the BrainPatch ECG Recorder.\n",
    "class BPDataExtraction():\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory + 'BrainPatch/'\n",
    "        self.sorted_ECG = pd.DataFrame(columns=['Timestamp', 'ECG', 'Stress Level'])\n",
    "\n",
    "    \n",
    "    # interpolates data to achieve sampling rate\n",
    "    def interpolate(self, sample:pd.DataFrame) -> pd.DataFrame:\n",
    "        # convert timestamp column to a NumPy array\n",
    "        timestamps = sample['Timestamp'].to_numpy()\n",
    "\n",
    "        # calculate the time difference between each pair of adjacent timestamps\n",
    "        time_diff = np.diff(timestamps)\n",
    "\n",
    "        # calculate the average sampling rate of the data\n",
    "        sampling_frequency = (1 / np.mean(time_diff))*1e6\n",
    "\n",
    "        interpolated_sample = pd.DataFrame(columns=['Timestamp', 'ECG'])\n",
    "\n",
    "        # interpolate the data to obtain desired sampling rate\n",
    "        interpolated_sample['ECG'] = nk.signal_resample(sample['ECG'], sampling_rate=sampling_frequency, desired_sampling_rate=self.sampling_frequency)\n",
    "        time_interval_us = (1/self.sampling_frequency)*1e6\n",
    "        interpolated_sample['Timestamp'] = [i*time_interval_us for i in range(len(interpolated_sample['ECG']))]\n",
    "\n",
    "        return interpolated_sample.astype('int64')\n",
    "\n",
    "\n",
    "    # define a function to map stress levels to labels\n",
    "    def map_stress_level(self, level):\n",
    "        if level >= 7:\n",
    "            return 3\n",
    "        elif level >= 5:\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "\n",
    "     # Sorts data from each participant, interpolates it (downsampling), labelling each ECG recording and appends to one dataframe.\n",
    "    def sort_data(self, sampling_frequency:int=1000):\n",
    "        self.sampling_frequency = sampling_frequency\n",
    "        # list the contents of the directory and filter out subfolders\n",
    "        recordings = [f for f in os.listdir(self.directory) if os.path.isfile(os.path.join(self.directory, f))]\n",
    "\n",
    "        for index, recording in enumerate(recordings):\n",
    "            Utilities.progress_bar('Sorting database', index, len(recordings)-1)\n",
    "            # get recording number\n",
    "            split_string = recording.split(' ')\n",
    "            second_element = split_string[1]\n",
    "            second_split = second_element.split('.')\n",
    "            recording_number = int(second_split[0])\n",
    "\n",
    "            # extract recording dataframe\n",
    "            recording_df = pd.read_csv(self.directory + recording, skiprows=2, usecols=[0, 1], header=None, names=['Timestamp', 'ECG'])\n",
    "            # interpolate it to desired sampling frequency\n",
    "            recording_df = self.interpolate(recording_df)\n",
    "            # load events containing the stress level markers\n",
    "            events_df = pd.read_csv(self.directory + 'Events/Events.csv')\n",
    "            # extract events for that recording\n",
    "            events_df = events_df[events_df['Recording No'] == recording_number].dropna(how='all', axis=1)\n",
    "            # convert string values to datetime objects\n",
    "            events_df['Timestamp'] = pd.to_datetime(events_df['Timestamp'], format='%H:%M')\n",
    "            # normalise timestamps \n",
    "            events_df['Timestamp'] = events_df['Timestamp'] - events_df['Timestamp'].iloc[0]\n",
    "            # convert from nanoseconds to microseconds\n",
    "            events_df['Timestamp'] = (events_df['Timestamp']/ 1e3).astype('int64')\n",
    "            # label recording with Events label\n",
    "            recording_df = pd.merge_asof(recording_df, events_df, on='Timestamp', direction='backward')\n",
    "            recording_df = recording_df.drop('Recording No', axis=1)\n",
    "            # convert stress level to Low(1-4), Medium(5-6) or High(7-10)\n",
    "            recording_df['Stress Level'] = recording_df['Stress Level'].apply(self.map_stress_level)\n",
    "            # concatentate to main dataframe\n",
    "            self.sorted_ECG = pd.concat([self.sorted_ECG, recording_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting database: [>                   ] 0%\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39melif\u001b[39;00m DATABASE \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mBrainPatch\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     11\u001b[0m     bpde \u001b[39m=\u001b[39m BPDataExtraction(data_path)\n\u001b[0;32m---> 12\u001b[0m     bpde\u001b[39m.\u001b[39;49msort_data(sampling_frequency\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m) \u001b[39m# downsamples to 1kHz\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     dataframe_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdata_path\u001b[39m}\u001b[39;00m\u001b[39mBrainPatch/Dataframes\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     14\u001b[0m     Utilities\u001b[39m.\u001b[39msave_dataframe(bpde\u001b[39m.\u001b[39msorted_ECG, dataframe_path, \u001b[39m'\u001b[39m\u001b[39mSorted\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[87], line 71\u001b[0m, in \u001b[0;36mBPDataExtraction.sort_data\u001b[0;34m(self, sampling_frequency)\u001b[0m\n\u001b[1;32m     69\u001b[0m recording_df \u001b[39m=\u001b[39m recording_df\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mRecording No\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[39m# convert stress level to Low(1-4), Medium(5-6) or High(7-10)\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m recording_df[\u001b[39m'\u001b[39m\u001b[39mStress Level\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m recording_df[\u001b[39m'\u001b[39;49m\u001b[39mStress Level\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmap_stress_level)\n\u001b[1;32m     72\u001b[0m \u001b[39m# concatentate to main dataframe\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msorted_ECG \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msorted_ECG, recording_df], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/data/myenv/ECG-Stress-Classifier/.venv/lib/python3.8/site-packages/pandas/core/series.py:4631\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4522\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4523\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4526\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4527\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4528\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4530\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4629\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4630\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4631\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m/data/myenv/ECG-Stress-Classifier/.venv/lib/python3.8/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/data/myenv/ECG-Stress-Classifier/.venv/lib/python3.8/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1077\u001b[0m             values,\n\u001b[1;32m   1078\u001b[0m             f,\n\u001b[1;32m   1079\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1080\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m/data/myenv/ECG-Stress-Classifier/.venv/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[87], line 30\u001b[0m, in \u001b[0;36mBPDataExtraction.map_stress_level\u001b[0;34m(self, level)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m interpolated_sample\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mint64\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[39m# define a function to map stress levels to labels\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap_stress_level\u001b[39m(\u001b[39mself\u001b[39m, level):\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m level \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m7\u001b[39m:\n\u001b[1;32m     32\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m3\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_path = '/data/myenv/Data/'\n",
    "\n",
    "if DATABASE == 'Spider':\n",
    "    sde = SpiderDataExtraction(data_path)\n",
    "    # sde.download_data()\n",
    "    sde.sort_data()\n",
    "    dataframe_path = f'{data_path}Spider/Dataframes'\n",
    "    Utilities.save_dataframe(sde.sorted_ECG, dataframe_path, 'Sorted')\n",
    "\n",
    "elif DATABASE == 'BrainPatch':\n",
    "    bpde = BPDataExtraction(data_path)\n",
    "    bpde.sort_data(sampling_frequency=1000) # downsamples to 1kHz\n",
    "    dataframe_path = f'{data_path}BrainPatch/Dataframes'\n",
    "    Utilities.save_dataframe(bpde.sorted_ECG, dataframe_path, 'Sorted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pre-Processing\n",
    "* Interpolates data to sample rate using timestamps \n",
    "* Semgents data using rolling window with overlap\n",
    "* Cleans data using Neurokit's 5th Order Butterworth filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.639837Z",
     "iopub.status.busy": "2023-03-28T14:15:57.639761Z",
     "iopub.status.idle": "2023-03-28T14:15:57.644906Z",
     "shell.execute_reply": "2023-03-28T14:15:57.644649Z"
    }
   },
   "outputs": [],
   "source": [
    "class PreProcessing():\n",
    "    def __init__(self, sorted_ECG:pd.DataFrame, sampling_frequency:int):\n",
    "        self.sorted_ECG = sorted_ECG\n",
    "        self.sampling_frequency = sampling_frequency\n",
    "       \n",
    "\n",
    "    # segments data with overlap using rolling window, if semgnet_heartbeats true, then segment is centralised around the heartbeat (R peak)\n",
    "    def segment(self, window_length_s:float, overlap:float, segment_hearbeats=False):\n",
    "        # convert window_length in seconds to samples\n",
    "        self.window_samples = int(window_length_s * self.sampling_frequency)\n",
    "        # Calculate the step_size as the fraction of the total window samples\n",
    "        step_size = int(self.window_samples * (1-overlap)) \n",
    "\n",
    "        # Initialize starting variables\n",
    "        current_index = 0\n",
    "        current_stressed = self.sorted_ECG['Stress Level'][current_index]\n",
    "        self.preprocessed_ECG = pd.DataFrame(columns=['Timestamp', 'ECG', 'Stress Level'])\n",
    "\n",
    "        # faster to concatenate at the end \n",
    "        preprocessed_ECG_list = []\n",
    "\n",
    "        # get all R peaks index if required\n",
    "        if segment_hearbeats:\n",
    "            r_peaks = nk.ecg_peaks(self.sorted_ECG['ECG'], sampling_rate=self.sampling_frequency)\n",
    "        \n",
    "        # Loop through the entire dataframe\n",
    "        while current_index < len(self.sorted_ECG['ECG']):  \n",
    "            Utilities.progress_bar('Segmenting data', current_index, len(self.sorted_ECG))\n",
    "            # calculate end index in window and exit if out of bounds          \n",
    "            end_index = current_index + self.window_samples\n",
    "            if (end_index > len(self.sorted_ECG['ECG'])):\n",
    "                break\n",
    "            \n",
    "            # Check if the window overlaps a different label\n",
    "            end_stressed = self.sorted_ECG['Stress Level'][end_index]\n",
    "\n",
    "            # If the next window has a different label, skip to next start of next label\n",
    "            if end_stressed != current_stressed:\n",
    "                while (current_stressed == self.sorted_ECG['Stress Level'][current_index]):\n",
    "                    current_index += 1\n",
    "                current_stressed = end_stressed\n",
    "\n",
    "            # otherwise, add segment to list of pre-processed ECG\n",
    "            else:\n",
    "                if segment_hearbeats:\n",
    "                    # get index of next r peak\n",
    "                    while not bool(r_peaks[0]['ECG_R_Peaks'][current_index]):\n",
    "                        current_index += 1\n",
    "                    # append segment centred on r-peak to dataframe\n",
    "                    preprocessed_ECG_list.append(self.sorted_ECG.iloc[(current_index - (self.window_samples//2)):(current_index + (self.window_samples//2))].astype('Float64'))\n",
    "                    # shift the window to next non r-peak index\n",
    "                    current_index += 1\n",
    "                else:\n",
    "                    # append segment to dataframe\n",
    "                    preprocessed_ECG_list.append(self.sorted_ECG.iloc[current_index:current_index + self.window_samples].astype('Float64'))\n",
    "                    # Shift the window\n",
    "                    current_index += step_size\n",
    "            \n",
    "        self.preprocessed_ECG = pd.concat(preprocessed_ECG_list, axis=0, ignore_index=True).astype('Float64')\n",
    "        Utilities.progress_bar('Segmenting data', current_index, current_index)\n",
    "    \n",
    "    \n",
    "    def create_2d(self):\n",
    "        # convert the pandas DataFrame into a 2D pandas where each row has the size of window and the corresponding label (stress level)\n",
    "        # self.preprocessed_ECG_2d = self.preprocessed_ECG.groupby(self.preprocessed_ECG.index // self.window_samples).apply(lambda x: pd.Series({'ECG': x['ECG'].values, 'Stress Level': x['Stress Level'].values[0]}))\n",
    "        \n",
    "        # Calculate the number of rows required\n",
    "        num_rows = len(self.preprocessed_ECG['ECG']) // self.window_samples\n",
    "\n",
    "        # Create an empty dataframe to hold the reshaped data\n",
    "        df_reshaped = pd.DataFrame(index=range(num_rows), columns=[f\"ECG {i}\" for i in range(self.window_samples)])\n",
    "\n",
    "        # Reshape the data\n",
    "        for i in range(num_rows):\n",
    "            start_idx = i * self.window_samples\n",
    "            end_idx = (i + 1) * self.window_samples\n",
    "            values = self.preprocessed_ECG['ECG'].iloc[start_idx:end_idx].values\n",
    "            df_reshaped.iloc[i, :] = values\n",
    "        \n",
    "        self.preprocessed_ECG_2d = df_reshaped\n",
    "        self.preprocessed_ECG_2d['Stress Level'] = self.preprocessed_ECG['Stress Level'][::self.window_samples].reset_index(drop=True)\n",
    "\n",
    "\n",
    "    def clean(self):\n",
    "        # Clean each sample in the stressed and not stressed data (overwrites original data)\n",
    "        # using method 'neurokit' (0.5 Hz high-pass butterworth filter (order = 5), followed by powerline filtering) but can be changed to other cleaning methods\n",
    "        print(\"Cleaning data...\")\n",
    "        self.preprocessed_ECG['ECG'] = pd.Series(nk.ecg_clean(self.preprocessed_ECG['ECG'], self.sampling_frequency, method='neurokit')).astype('Float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting data: [------------------->] 100%\n",
      "Cleaning data...\n",
      "Saving Dataframe to: /data/myenv/Data/BrainPatch/Dataframes/Preprocessed.csv...Saved.\n"
     ]
    }
   ],
   "source": [
    "window_length_s = 20 # window length for each segment\n",
    "overlap = 0.1 # overlap for sliding window\n",
    "\n",
    "if DATABASE == 'Spider':\n",
    "    pp = PreProcessing(sde.sorted_ECG, sde.sampling_frequency)\n",
    "    pp.segment(window_length_s, overlap)\n",
    "    pp.clean()\n",
    "    Utilities.save_dataframe(pp.preprocessed_ECG, dataframe_path, 'Preprocessed')\n",
    "\n",
    "elif DATABASE == 'BrainPatch':\n",
    "    pp = PreProcessing(bpde.sorted_ECG, 1000) # the data will be downsampled to 1000Hz\n",
    "    pp.segment(window_length_s, overlap)\n",
    "    pp.clean()\n",
    "    Utilities.save_dataframe(pp.preprocessed_ECG, dataframe_path, 'Preprocessed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction\n",
    "* Extracts features such as HRV time, frequency and non-linear domain, EDR etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.649031Z",
     "iopub.status.busy": "2023-03-28T14:15:57.648948Z",
     "iopub.status.idle": "2023-03-28T14:15:57.656454Z",
     "shell.execute_reply": "2023-03-28T14:15:57.656197Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main class that extracts features from a dictionary of sorted dataframes and stores to csv\n",
    "class FeatureExtraction():\n",
    "    # takes in cleaned ECG data\n",
    "    def __init__(self, preprocessed_ECG:pd.DataFrame, window_samples:int, sampling_frequency:int):\n",
    "        self.preprocessed_ECG = preprocessed_ECG\n",
    "        self.window_samples = window_samples\n",
    "        self.sampling_frequency = sampling_frequency\n",
    "        self.feature_extracted_ECG = pd.DataFrame()\n",
    "\n",
    "    \n",
    "    def wave_analysis(self, segment:pd.DataFrame, plot=False) -> pd.DataFrame:\n",
    "        ECG_processed, info = nk.ecg_process(segment.to_numpy(dtype='float64'), sampling_rate=self.sampling_frequency, method='neurokit')\n",
    "        # calculate the mean and SD of the peak intervals\n",
    "        peaks = ['ECG_P_Peaks', 'ECG_Q_Peaks', 'ECG_S_Peaks', 'ECG_T_Peaks']\n",
    "        # Minimum and maximum expected HR (beats per min)\n",
    "        min_HR = 30\n",
    "        max_HR = 200\n",
    "        min_interval = 60e6/max_HR\n",
    "        max_interval = 60e6/min_HR\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for peak in peaks:\n",
    "            intervals = np.diff(np.where(np.array(ECG_processed[peak]==1))) * self.sampling_frequency\n",
    "            # Remove any intervals that are out of range\n",
    "            intervals = intervals[(intervals >= min_interval) & (intervals <= max_interval)]\n",
    "\n",
    "            df[f'{peak}_Interval_Mean'] = [np.mean(intervals)]\n",
    "            df[f'{peak}_Interval_SD'] = [np.std(intervals)]\n",
    "\n",
    "        # calculate the average lenght of each wave\n",
    "        waves = ['P', 'R', 'T']\n",
    "        max_duration = [120000, 120000, 200000]\n",
    "        for w, wave in enumerate(waves):\n",
    "            onsets = np.where(np.array(ECG_processed[f'ECG_{wave}_Onsets']==1))[0]\n",
    "            offsets = np.where(np.array(ECG_processed[f'ECG_{wave}_Offsets']==1))[0]\n",
    "            # find index of first element in offsets that is >= first element in onsets\n",
    "            idx_offset = np.where(offsets >= onsets[0])[0][0]\n",
    "            # find size of smallest array\n",
    "            duration_size = min(onsets.size, offsets.size)\n",
    "            # slice offsets array to start at same index as onset\n",
    "            offsets = offsets[idx_offset:duration_size]\n",
    "            # set onset to same length\n",
    "            onsets = onsets[:duration_size]\n",
    "            # calculate durations taking into account missed onset detection\n",
    "            durations = []\n",
    "            # iterate over elements of both arrays\n",
    "            i = 0\n",
    "            j = 0\n",
    "            while i < len(offsets) and j < len(onsets):\n",
    "                diff = offsets[i] - onsets[j]\n",
    "                if diff < 0:\n",
    "                    i += 1\n",
    "                else:\n",
    "                    durations.append(diff)\n",
    "                    i += 1\n",
    "                    j += 1\n",
    "            durations = np.array(durations * self.sampling_frequency)\n",
    "            # Remove any intervals that are out of range\n",
    "            durations = durations[(durations <= max_duration[w])]\n",
    "\n",
    "            duration_mean = np.mean(durations)\n",
    "            duration_SD = np.std(durations)\n",
    "            df[f'ECG_{wave}_Duration_Mean'] = duration_mean\n",
    "            df[f'ECG_{wave}_Duration_SD'] = duration_SD\n",
    "\n",
    "        return df    \n",
    "\n",
    "    \n",
    "    def calc_PSD(self, segment:pd.DataFrame) -> pd.DataFrame:\n",
    "        # Sum the power across 10 Hz bands from 0 to 200 Hz\n",
    "        # Compute the power spectrum using a Fast Fourier Transform\n",
    "        PSD = nk.signal_psd(segment.to_list(), sampling_rate=self.sampling_frequency, method=\"welch\", min_frequency=0.5, max_frequency=200)\n",
    "        # Create an empty list to store the binned power values\n",
    "        binned_power = []\n",
    "        # Set the initial frequency and bin range values\n",
    "        frequency = 0\n",
    "        bin_range = 10\n",
    "\n",
    "        # Loop through the frequency ranges of 10Hz\n",
    "        while bin_range <= 200:\n",
    "            # Initialize the total power for the current bin\n",
    "            total_power = 0\n",
    "            \n",
    "            # Loop through the rows of the original dataframe\n",
    "            for index, row in PSD.iterrows():\n",
    "                # Check if the frequency falls within the current bin range\n",
    "                if row['Frequency'] >= frequency and row['Frequency'] < bin_range:\n",
    "                    # Add the power value to the total power\n",
    "                    total_power += row['Power']\n",
    "            \n",
    "            # Calculate the logarithm of the total power for the current bin and append it to the binned_power list\n",
    "            if total_power > 0:\n",
    "                binned_power.append(np.log10(total_power))\n",
    "            else:\n",
    "                binned_power.append(-np.inf)\n",
    "            \n",
    "            # Increment the frequency and bin range values for the next iteration\n",
    "            frequency += 10\n",
    "            bin_range += 10\n",
    "\n",
    "        # Create a new dataframe with the binned power values and the frequency ranges as the index\n",
    "        binned_PSD = pd.DataFrame({'Power': binned_power})\n",
    "        binned_PSD['Frequency Band'] = list(range(10, 201, 10))\n",
    "        # Convert to columns\n",
    "        ECG_Frequencies = pd.DataFrame(columns=[f\"ECG_FQ_{i}\" for i in range(10, 210, 10)])\n",
    "        for i, column in enumerate(ECG_Frequencies.columns):\n",
    "            ECG_Frequencies[column] = [binned_PSD.iloc[i]['Power']]\n",
    "        \n",
    "        return ECG_Frequencies\n",
    "\n",
    "\n",
    "    # Main method to extracts features from ECG using neurokit\n",
    "    def extract(self, ECG:bool=False, EDR:bool=False, show_plot:bool=False):\n",
    "        # collective epoch analysis\n",
    "        if ECG:\n",
    "            print(\"Extracting Collective Features...\")\n",
    "            # warnings.filterwarnings('ignore') # temporarily supress warnings\n",
    "            # # automated pipeline for preprocessing an ECG signal\n",
    "            # ECG_processed, info = nk.ecg_process(self.preprocessed_ECG['ECG'].to_numpy(dtype='float64'), sampling_rate=self.sampling_frequency, method='neurokit')\n",
    "            # events = np.arange(self.window_samples, self.preprocessed_ECG.shape[0], self.window_samples)\n",
    "            # epochs = nk.epochs_create(ECG_processed, events=events, sampling_rate=self.sampling_frequency)\n",
    "            # # calculate ECG Features such as pqrstu intevals etc.\n",
    "            # ECG_events = nk.ecg_analyze(epochs, sampling_rate=self.sampling_frequency, method='event-related')\n",
    "            # warnings.filterwarnings('default')\n",
    "\n",
    "        # individual epoch analysis\n",
    "        sample_index = 0\n",
    "        epoch_index = 0\n",
    "        while sample_index < (len(self.preprocessed_ECG['ECG']) - self.window_samples):\n",
    "            Utilities.progress_bar('Extracting Individual Features', sample_index, len(self.preprocessed_ECG['ECG']))\n",
    "\n",
    "            # get segment ECG and stress level from dataframe \n",
    "            segment = self.preprocessed_ECG.iloc[sample_index:sample_index + self.window_samples]['ECG']\n",
    "            stress_level = self.preprocessed_ECG.iloc[sample_index]['Stress Level']\n",
    "            features = pd.DataFrame({'Stress Level': [stress_level]})\n",
    "            # extract R-R peaks\n",
    "            r_peaks_df = nk.ecg_peaks(segment, sampling_rate=self.sampling_frequency, correct_artifacts=True)[0]\n",
    "            \n",
    "            sample_index += self.window_samples\n",
    "\n",
    "            if ECG:\n",
    "                warnings.filterwarnings('ignore') # temporarily supress warnings\n",
    "                ECG_processed_segment, info = nk.ecg_process(segment.to_numpy(dtype='float64'), sampling_rate=self.sampling_frequency, method='neurokit')\n",
    "                if show_plot:\n",
    "                    nk.ecg_plot(ECG_processed_segment)\n",
    "\n",
    "                # Calculate ECG_HRV - different heart rate variability metrices.\n",
    "                HRV_intervals = nk.ecg_intervalrelated(ECG_processed_segment, sampling_rate=self.sampling_frequency)\n",
    "\n",
    "                # calculate waveform intervals (PQRSTU)\n",
    "                ECG_intervals = self.wave_analysis(segment)\n",
    "\n",
    "                # get the binned power spectrum frequencies from the ECG segment\n",
    "                ECG_frequencies = self.calc_PSD(segment)\n",
    "                \n",
    "                # add ECG_event to dataframe (ECG Features such as pqrstu intevals etc.))\n",
    "                ECG_event = ECG_events.iloc[epoch_index].to_frame().transpose().reset_index()\n",
    "                ECG_event = ECG_event.drop(['index', 'Label', 'Event_Onset'], axis=1)\n",
    "                epoch_index += 1\n",
    "\n",
    "                # concat to dataframe\n",
    "                features = pd.concat([features, HRV_intervals, ECG_intervals, ECG_Frequencies, ECG_event], axis=1)\n",
    "\n",
    "            # If not all ECG features desired, obtain HRV and Shannon Entropy\n",
    "            else:\n",
    "                np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "                # skip segment if insufficient peaks are detected (otherwise will cause NK error)\n",
    "                if int(r_peaks_df[r_peaks_df == 1].sum().iloc[0]) < 4:\n",
    "                    continue\n",
    "\n",
    "                # Extract HRV features from R-R peaks, see https://neuropsychology.github.io/NeuroKit/functions/hrv.html \n",
    "                # compute HRV - time, frequency and nonlinear indices.\n",
    "                warnings.filterwarnings('ignore') # temporarily supress warnings\n",
    "                HRV_time = nk.hrv_time(r_peaks_df, sampling_rate=self.sampling_frequency, show=show_plot)\n",
    "                HRV_frequency = nk.hrv_frequency(r_peaks_df, sampling_rate=self.sampling_frequency, show=show_plot)\n",
    "                warnings.filterwarnings('default')\n",
    "\n",
    "                # compute Shannon Entropy (SE) using signal symbolization and discretization\n",
    "                # see https://neuropsychology.github.io/NeuroKit/functions/complexity.html#entropy-shannon \n",
    "                SE = nk.entropy_shannon(segment, symbolize='A')[0]\n",
    "                HRV_ShanEn = pd.DataFrame([SE], columns=['HRV_ShanEn'])\n",
    "                # concat to feature dataframe\n",
    "                features = pd.concat([features, HRV_time, HRV_frequency, HRV_ShanEn], axis=1)\n",
    "                \n",
    "            if EDR:\n",
    "                # Get ECG Derived Respiration (EDR) and add to the data\n",
    "                warnings.filterwarnings('ignore') # temporarily supress warnings\n",
    "                ecg_rate = nk.signal_rate(r_peaks_df, sampling_rate=self.sampling_frequency, desired_length=len(r_peaks_df))\n",
    "                warnings.filterwarnings('default')\n",
    "                EDR_sample = nk.ecg_rsp(ecg_rate, sampling_rate=self.sampling_frequency)\n",
    "                if show_plot:\n",
    "                    nk.signalplot(segment)\n",
    "                    nk.signal_plot(EDR_sample)\n",
    "                EDR_Distance = pd.DataFrame([np.average(nk.signal_findpeaks(EDR_sample)[\"Distance\"])], columns=['EDR_Distance'])\n",
    "                # concat to dataframe\n",
    "                features = pd.concat([features, EDR_Distance], axis=1)\n",
    "\n",
    "            # concat features to main dataframe\n",
    "            self.feature_extracted_ECG = pd.concat([self.feature_extracted_ECG, features], axis=0, ignore_index=True)\n",
    "        Utilities.progress_bar('Extracting Neurokit Features', sample_index, sample_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.662551Z",
     "iopub.status.busy": "2023-03-28T14:15:57.662447Z",
     "iopub.status.idle": "2023-03-28T14:15:57.666162Z",
     "shell.execute_reply": "2023-03-28T14:15:57.665929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Collective Features...\n",
      "Extracting Individual Features: [>                   ] 0%\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ECG_events' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fe \u001b[39m=\u001b[39m FeatureExtraction(pp\u001b[39m.\u001b[39mpreprocessed_ECG, pp\u001b[39m.\u001b[39mwindow_samples, pp\u001b[39m.\u001b[39msampling_frequency)\n\u001b[0;32m----> 2\u001b[0m fe\u001b[39m.\u001b[39;49mextract(ECG\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, EDR\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, show_plot\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      3\u001b[0m Utilities\u001b[39m.\u001b[39msave_dataframe(fe\u001b[39m.\u001b[39mfeature_extracted_ECG, dataframe_path, \u001b[39m'\u001b[39m\u001b[39mFeature Extracted HRV_ECG_EDA_20s\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[80], line 157\u001b[0m, in \u001b[0;36mFeatureExtraction.extract\u001b[0;34m(self, ECG, EDR, show_plot)\u001b[0m\n\u001b[1;32m    154\u001b[0m ECG_frequencies \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalc_PSD(segment)\n\u001b[1;32m    156\u001b[0m \u001b[39m# add ECG_event to dataframe (ECG Features such as pqrstu intevals etc.))\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m ECG_event \u001b[39m=\u001b[39m ECG_events\u001b[39m.\u001b[39miloc[epoch_index]\u001b[39m.\u001b[39mto_frame()\u001b[39m.\u001b[39mtranspose()\u001b[39m.\u001b[39mreset_index()\n\u001b[1;32m    158\u001b[0m ECG_event \u001b[39m=\u001b[39m ECG_event\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLabel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mEvent_Onset\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    159\u001b[0m epoch_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ECG_events' is not defined"
     ]
    }
   ],
   "source": [
    "fe = FeatureExtraction(pp.preprocessed_ECG, pp.window_samples, pp.sampling_frequency)\n",
    "fe.extract(ECG=True, EDR=True, show_plot=False)\n",
    "Utilities.save_dataframe(fe.feature_extracted_ECG, dataframe_path, 'Feature Extracted HRV_ECG_EDA_20s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Selection:\n",
    "* Select desired features, sanity check the values, and save them to Features directory\n",
    "* Visualise most feature and cross-feature distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.646170Z",
     "iopub.status.busy": "2023-03-28T14:15:57.646007Z",
     "iopub.status.idle": "2023-03-28T14:15:57.647802Z",
     "shell.execute_reply": "2023-03-28T14:15:57.647608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Each feature will be an object of FEParameter\n",
    "class FEParameter:\n",
    "    def __init__(self, name:str, min:float=0.0, max:float=9999):\n",
    "        self.name = name\n",
    "        self.min = min\n",
    "        self.max = max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define FeatureSelection class that is used to visualise and select data\n",
    "class FeatureSelection():\n",
    "    def __init__(self, feature_extracted_ECG):\n",
    "        self.feature_extracted_ECG = feature_extracted_ECG\n",
    "\n",
    "\n",
    "    # desired_features is a list of FEParameter objects  \n",
    "    def select(self, desired_features:List[FEParameter]):\n",
    "        self.selected_features_ECG = self.feature_extracted_ECG[['Stress Level']]\n",
    "        \n",
    "        for feature in desired_features:\n",
    "            out_of_range_count = 0\n",
    "            # Sanity check: check if feature exists\n",
    "            if feature.name in self.feature_extracted_ECG.columns:\n",
    "                # Set value to NaN if it falls outside min and max values.\n",
    "                for i, value in enumerate(self.feature_extracted_ECG[feature.name]):\n",
    "                    if (value < feature.min) or (value > feature.max):\n",
    "                        out_of_range_count += 1\n",
    "                        self.feature_extracted_ECG.loc[i, feature.name] = np.nan\n",
    "                # Add column to new selected features\n",
    "                pd.options.mode.chained_assignment = None\n",
    "                self.selected_features_ECG[feature.name] = self.feature_extracted_ECG[[feature.name]].copy()\n",
    "                pd.options.mode.chained_assignment = 'warn'\n",
    "            else:\n",
    "                print(f'Error: No such feature \"{feature}\" in extracted features')\n",
    "            if out_of_range_count != 0:\n",
    "                print(f'Feature: {feature.name} is out of range {out_of_range_count}/{len(self.feature_extracted_ECG[feature.name])} segments')\n",
    "\n",
    "        \n",
    "    # impute missing values in dataset with mean values of column\n",
    "    def impute(self):\n",
    "        pd.options.mode.chained_assignment = None\n",
    "        self.selected_features_ECG.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        pd.options.mode.chained_assignment = 'warn'\n",
    "        imp = SimpleImputer(strategy='mean')\n",
    "        imp.fit(self.selected_features_ECG)\n",
    "        self.selected_features_ECG = pd.DataFrame(imp.transform(self.selected_features_ECG), columns=self.selected_features_ECG.columns)\n",
    "\n",
    "\n",
    "    def visualise(self, plot_type='pairplot'):\n",
    "        print(\"Generating plot...\")\n",
    "        if plot_type == 'pairplot':         \n",
    "            sns.pairplot(data = self.selected_features_ECG, hue='Stress Level', palette=['green', 'yellow', 'red'])\n",
    "        elif plot_type == 'kdeplot':\n",
    "            # Create a figure with subplots for each feature\n",
    "            subplot_size = math.ceil(math.sqrt(len(self.selected_features_ECG.columns)))\n",
    "            fig = plt.figure(figsize=(20, 8*subplot_size))\n",
    "\n",
    "            # Loop through each feature and add it to a subplot\n",
    "            for i, feature in enumerate(self.selected_features_ECG):\n",
    "                fig.add_subplot(subplot_size, subplot_size, i+1)\n",
    "                sns.kdeplot(x=feature, data=self.selected_features_ECG, hue='Stress Level', common_norm=False, warn_singular=False, palette=['green', 'yellow', 'red'])\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Plot type not recognised. Please choose between pairplot, kdeplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.657717Z",
     "iopub.status.busy": "2023-03-28T14:15:57.657598Z",
     "iopub.status.idle": "2023-03-28T14:15:57.661341Z",
     "shell.execute_reply": "2023-03-28T14:15:57.661082Z"
    }
   },
   "outputs": [],
   "source": [
    "# Desired Features to be selected from Feature Extracted ECG.\n",
    "# See Neurokit2 HRV for more information - https://neuropsychology.github.io/NeuroKit/functions/hrv.html\n",
    "\n",
    "# Minimum and maximum expected HR (beats per min)\n",
    "min_HR = 30\n",
    "max_HR = 200\n",
    "\n",
    "# MinNN: The minimum of the RR intervals (Parent, 2019; Subramaniam, 2022).\n",
    "HRV_MinNN = FEParameter('HRV_MinNN', min=60000.0/max_HR, max=60000.0/min_HR)\n",
    "# MaxNN: The maximum of the RR intervals (Parent, 2019; Subramaniam, 2022).\n",
    "HRV_MaxNN = FEParameter('HRV_MaxNN', min=60000.0/max_HR, max=60000.0/min_HR)\n",
    "# MeanNN: The mean of the RR intervals.\n",
    "HRV_MeanNN = FEParameter('HRV_MeanNN', min=60000.0/max_HR, max=60000.0/min_HR)\n",
    "\n",
    "\n",
    "\n",
    "# pNN20: The proportion of RR intervals greater than 20ms, out of the total number of RR intervals.\n",
    "HRV_pNN20 = FEParameter('HRV_pNN20')\n",
    "# pNN50: The proportion of RR intervals greater than 50ms, out of the total number of RR intervals.\n",
    "HRV_pNN50 = FEParameter('HRV_pNN50')\n",
    "# A geometrical parameter of the HRV, or more specifically, the baseline width of the RR intervals distribution \n",
    "# TINN: obtained by triangular interpolation, where the error of least squares determines the triangle. \n",
    "# It is an approximation of the RR interval distribution.\n",
    "HRV_TINN = FEParameter('HRV_TINN')\n",
    "# HTI: The HRV triangular index, measuring the total number of RR intervals divided by the height of the RR intervals histogram.\n",
    "HRV_HTI = FEParameter('HRV_HTI')\n",
    "\n",
    "# VLF: The spectral power (W/Hz) of very low frequencies (.0033 to .04 Hz).\n",
    "HRV_VLF = FEParameter('HRV_VLF', min=0.0, max=9) # hidden due to use of 0.5 Hz high-pass butterworth filter\n",
    "# LF: The spectral power (W/Hz) of low frequencies (.04 to .15 Hz).\n",
    "HRV_LF = FEParameter('HRV_LF', max=1.00)\n",
    "# HF: The spectral power (W/Hz) of high frequencies (.15 to .4 Hz).\n",
    "HRV_HF = FEParameter('HRV_HF', max=1.00)\n",
    "# LFHF: The ratio obtained by dividing the low frequency power by the high frequency power.\n",
    "HRV_LFHF = FEParameter('HRV_LFHF', max=1.00)\n",
    "\n",
    "# SDNN: The standard deviation of the RR intervals.\n",
    "# See https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5624990/ for chosen max value.\n",
    "HRV_SDNN = FEParameter('HRV_SDNN', max=200)\n",
    "# RMSSD: The square root of the mean of the squared successive differences between adjacent RR intervals. \n",
    "# # It is equivalent (although on another scale) to SD1, and therefore it is redundant to report correlations with both (Ciccone, 2017).\n",
    "# See https://help.welltory.com/en/articles/4413231-what-normal-ranges-and-measurement-standards-we-use-to-interpret-your-heart-rate-variability for chosen max value.\n",
    "HRV_RMSSD = FEParameter('HRV_RMSSD', max=107)\n",
    "# The root mean square of successive differences (RMSSD) divided by the mean of the RR intervals (MeanNN).\n",
    "cv_foldsSD = FEParameter('HRV_cv_foldsSD')\n",
    "# Shannon Entropy\n",
    "HRV_ShanEn = FEParameter('HRV_ShanEn')\n",
    "# Sample Entropy\n",
    "HRV_SampEn = FEParameter('HRV_SampEn')\n",
    "# DFA_alpha1: The monofractal detrended fluctuation analysis of the HR signal, corresponding to short-term correlations.\n",
    "HRV_DFA_alpha1 = FEParameter('HRV_DFA_alpha1')\n",
    "\n",
    "# EDR Distance: Breathing interval measured in ms.\n",
    "# Breathing rate range is approximately 12 to 25 per minute - see https://my.clevelandclinic.org/health/articles/10881-vital-signs\n",
    "EDR_Distance = FEParameter('EDR_Distance', min=2000, max=4167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m fs \u001b[39m=\u001b[39m FeatureSelection(fe\u001b[39m.\u001b[39mfeature_extracted_ECG)\n\u001b[1;32m      2\u001b[0m \u001b[39m#fs = FeatureSelection(Utilities.load_dataframe(f'{dataframe_path}/Feature Extracted HRV_ECG_EDA.csv'))\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# desired features for classification\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m selected_features \u001b[39m=\u001b[39m [HRV_MinNN, HRV_MaxNN, HRV_MeanNN, HRV_SDNN, HRV_RMSSD, HRV_ShanEn, HRV_SampEn, HRV_DFA_alpha1, HRV_pNN20, HRV_pNN50, EDR_Distance]\n\u001b[1;32m      5\u001b[0m fs\u001b[39m.\u001b[39mselect(selected_features)\n\u001b[1;32m      6\u001b[0m fs\u001b[39m.\u001b[39mimpute()\n",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m fs \u001b[39m=\u001b[39m FeatureSelection(fe\u001b[39m.\u001b[39mfeature_extracted_ECG)\n\u001b[1;32m      2\u001b[0m \u001b[39m#fs = FeatureSelection(Utilities.load_dataframe(f'{dataframe_path}/Feature Extracted HRV_ECG_EDA.csv'))\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# desired features for classification\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m selected_features \u001b[39m=\u001b[39m [HRV_MinNN, HRV_MaxNN, HRV_MeanNN, HRV_SDNN, HRV_RMSSD, HRV_ShanEn, HRV_SampEn, HRV_DFA_alpha1, HRV_pNN20, HRV_pNN50, EDR_Distance]\n\u001b[1;32m      5\u001b[0m fs\u001b[39m.\u001b[39mselect(selected_features)\n\u001b[1;32m      6\u001b[0m fs\u001b[39m.\u001b[39mimpute()\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/data/myenv/ECG-Stress-Classifier/.venv/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m/data/myenv/ECG-Stress-Classifier/.venv/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fs = FeatureSelection(fe.feature_extracted_ECG)\n",
    "#fs = FeatureSelection(Utilities.load_dataframe(f'{dataframe_path}/Feature Extracted HRV_ECG_EDA.csv'))\n",
    "# desired features for classification\n",
    "selected_features = [HRV_MinNN, HRV_MaxNN, HRV_MeanNN, HRV_SDNN, HRV_RMSSD, HRV_ShanEn, HRV_SampEn, HRV_DFA_alpha1, HRV_pNN20, HRV_pNN50, EDR_Distance]\n",
    "fs.select(selected_features)\n",
    "fs.impute()\n",
    "fs.visualise(plot_type='kdeplot')\n",
    "fs.visualise(plot_type='pairplot')\n",
    "Utilities.save_dataframe(fs.selected_features_ECG, dataframe_path, 'Feature Selected')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Traditional Machine Learning Methods\n",
    "* Prepares data by randomly splitting data into train, test and validation data\n",
    "* Option for Linear Discriminant Analysis (LDA) for dimension reduction\n",
    "* Implements the following classification models, with methods for tuning: LDA, Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure of dataset that will be passed to Models as object\n",
    "class Dataset():\n",
    "    def __init__(self, X, y, X_train, y_train, X_test, y_test, X_valid, y_valid):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.X_valid = X_valid\n",
    "        self.y_valid = y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities for preparing and evaluating classification\n",
    "class ML_Utilities():  \n",
    "    # corrects imbalanced dataset using SMOTE, randomizes, splits into test, training and validation data.\n",
    "    def prepare(selected_features_ECG:pd.DataFrame, train_split:float, test_split:float, validation_split:float=None, normalise=True, LDA_dimension_reduction:bool=False) -> Dataset:     \n",
    "        label = selected_features_ECG['Stress Level']\n",
    "        features = selected_features_ECG.loc[:, selected_features_ECG.columns != 'Stress Level']\n",
    "\n",
    "        print(\"Before imbalanced label correction (SMOTE):\")\n",
    "        low = label.value_counts()[1.0]/len(label)*100\n",
    "        print(f'    Low Stress in dataset: {low:.2f}%')\n",
    "        medium = label.value_counts()[2.0]/len(label)*100\n",
    "        print(f'    Medium Stress in dataset: {medium:.2f}%')\n",
    "        high = label.value_counts()[3.0]/len(label)*100\n",
    "        print(f'    High Stress in dataset: {high:.2f}%')\n",
    "        \n",
    "        features, label = SMOTE().fit_resample(features, label)\n",
    "\n",
    "        # split into test, training and validation data (hold-out validation)\n",
    "        # Split the data into training and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=test_split, random_state=15)\n",
    "\n",
    "\n",
    "        if validation_split != None:\n",
    "            # Split the training data into training and validation sets\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=validation_split/(train_split), random_state=15)\n",
    "            if normalise:\n",
    "                X_valid = pd.DataFrame(normalize(X_valid.values, axis=0), columns=X_valid.columns)\n",
    "\n",
    "        else:\n",
    "            X_valid = None\n",
    "            y_valid = None        \n",
    "\n",
    "        # L2 normalization between 0 and 1 for each feature (if required)\n",
    "        if normalise:\n",
    "            features = pd.DataFrame(normalize(features.values, axis=0), columns=features.columns)\n",
    "            X_train = pd.DataFrame(normalize(X_train.values, axis=0), columns=X_train.columns)\n",
    "            X_test = pd.DataFrame(normalize(X_test.values, axis=0), columns=X_test.columns)\n",
    "\n",
    "        # LDA for dimension reduction creates new train and test data\n",
    "        if LDA_dimension_reduction:\n",
    "            lda = LinearDiscriminantAnalysis()\n",
    "            X_train = lda.fit_transform(X_train, y_train)\n",
    "            X_test = lda.transform(X_test)\n",
    "            X_valid = lda.transform(X_valid)\n",
    "\n",
    "        # create data structure\n",
    "        dataset = Dataset(features, label, X_train, y_train, X_test, y_test, X_valid, y_valid)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def plot_confustion_matrix(y_test, y_pred):\n",
    "        # Confusion Matrix\n",
    "        ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=['Low', 'Medium', 'High'], normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback for plotting live loss curve whilst training\n",
    "class PlotLearning(Callback):\n",
    "    \"\"\"\n",
    "    Callback to plot the learning curves of the model during training.\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.metrics = {}\n",
    "        for metric in logs:\n",
    "            self.metrics[metric] = []\n",
    "            \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Storing metrics\n",
    "        for metric in logs:\n",
    "            if metric in self.metrics:\n",
    "                self.metrics[metric].append(logs.get(metric))\n",
    "            else:\n",
    "                self.metrics[metric] = [logs.get(metric)]\n",
    "        \n",
    "        # Plotting\n",
    "        metrics = [x for x in logs if 'val' not in x]\n",
    "        \n",
    "        f, axs = plt.subplots(1, len(metrics), figsize=(15,5))\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            axs[i].plot(range(1, epoch + 2), \n",
    "                        self.metrics[metric], \n",
    "                        label='Train ' + metric)\n",
    "            if logs['val_' + metric]:\n",
    "                axs[i].plot(range(1, epoch + 2), \n",
    "                            self.metrics['val_' + metric], \n",
    "                            label='Test ' + metric)\n",
    "                \n",
    "            axs[i].legend()\n",
    "            axs[i].grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.678855Z",
     "iopub.status.busy": "2023-03-28T14:15:57.678628Z",
     "iopub.status.idle": "2023-03-28T14:15:57.685391Z",
     "shell.execute_reply": "2023-03-28T14:15:57.685101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Traditional Machine Learning class, which implements different linear and non linear classification methods\n",
    "class Traditional_ML():\n",
    "    def __init__(self, dataset:Dataset, database, number_of_cores:int=1):\n",
    "        self.dataset = dataset\n",
    "        self.database = database\n",
    "        self.n_jobs = number_of_cores\n",
    "        self.model = 'RF'\n",
    "\n",
    "    # main tune method\n",
    "    def tune(self):\n",
    "        if self.model == 'LDA':\n",
    "            tuning_func = lambda: self.LDA_tuner()\n",
    "        elif self.model == 'RF':\n",
    "            tuning_func = lambda: self.RF_tuner()\n",
    "        elif self.model == 'BT':\n",
    "            tuning_func = lambda: self.BT_tuner()\n",
    "        elif self.model == 'NB':\n",
    "            tuning_func = lambda: self.NB_tuner()\n",
    "        elif self.model == 'SVM':\n",
    "            tuning_func = lambda: self.SVM_tuner()\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported model: {self.model}')\n",
    "        tuning_func()\n",
    "\n",
    "\n",
    "    # main classification method\n",
    "    def classify(self, cv_folds:int=10):\n",
    "        if self.model == 'LDA':\n",
    "            tuning_func = lambda cv_folds: self.LDA_classifier(cv_folds)\n",
    "        elif self.model == 'BT':\n",
    "            tuning_func = lambda cv_folds: self.BT_classifier(cv_folds)\n",
    "        elif self.model == 'RF':\n",
    "            tuning_func = lambda cv_folds: self.RF_classifier(cv_folds)\n",
    "        elif self.model == 'NB':\n",
    "            tuning_func = lambda cv_folds: self.NB_classifier(cv_folds)\n",
    "        elif self.model == 'SVM':\n",
    "            tuning_func = lambda cv_folds: self.SVM_classifier(cv_folds)\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported model: {self.model}')\n",
    "        tuning_func(cv_folds)\n",
    "\n",
    "\n",
    "    def evaluate(self, model, cv_folds:int=0):\n",
    "        # Cross-validation prediction \n",
    "        if cv_folds >= 2:\n",
    "            X = self.dataset.X\n",
    "            y = self.dataset.y\n",
    "            y_pred = cross_val_predict(model, X, y, cv=cv_folds, n_jobs=1)\n",
    "            # Obtain Accuracy score of cross-validation scores\n",
    "            acc_cv_folds = accuracy_score(y, y_pred) * 100\n",
    "            print(f\"    {cv_folds} fold cross validation accuracy: {acc_cv_folds.mean():.2f}%\")\n",
    "        \n",
    "        # Validate on test data\n",
    "        else:\n",
    "            X = self.dataset.X_test\n",
    "            y = self.dataset.y_test\n",
    "            model.fit(self.dataset.X_train, self.dataset.y_train)\n",
    "            y_pred_train = model.predict(X=self.dataset.X_train)\n",
    "            y_pred_test = model.predict(X=X)\n",
    "            # Obtain accuracy score for train and test data\n",
    "            acc = accuracy_score(self.dataset.y_train, y_pred_train) * 100\n",
    "            print(f\"    Accuracy on Train Data: {acc:.2f}%\")\n",
    "\n",
    "            acc = accuracy_score(y, y_pred_test) * 100\n",
    "            print(f\"    Accuracy on Test Data: {acc:.2f}%\\n\")\n",
    "            y_pred = y_pred_test\n",
    "\n",
    "         # calculate accuracy for each label\n",
    "        thr_acc = accuracy_score(y[y == 3.0], y_pred[y == 3.0]) * 100\n",
    "        tmr_acc = accuracy_score(y[y == 2.0], y_pred[y == 2.0]) * 100\n",
    "        tlr_acc = accuracy_score(y[y == 1.0], y_pred[y == 1.0]) * 100\n",
    "        # print results\n",
    "        print(f\"    Accuracy for True High Rate (THR): {thr_acc:.2f}%\")\n",
    "        print(f\"    Accuracy for True Medium Rate (TMR): {tmr_acc:.2f}%\")\n",
    "        print(f\"    Accuracy for True Low Rate (TLR): {tlr_acc:.2f}%\")\n",
    "\n",
    "        # Plot the confusion matrix\n",
    "        ML_Utilities.plot_confustion_matrix(y, y_pred)\n",
    "\n",
    "\n",
    "    def LDA_tuner(self):\n",
    "        solvers = ['lsqr', 'eigen']\n",
    "        shrinkages = [None, 'auto', 0.1, 0.5, 0.9]\n",
    "\n",
    "        for solver, shrinkage in product(solvers, shrinkages):\n",
    "            clf = LinearDiscriminantAnalysis(solver=solver, shrinkage=shrinkage).fit(self.dataset.X_train, self.dataset.y_train)\n",
    "            clf_score = clf.score(self.dataset.X_test, self.dataset.y_test)\n",
    "            print(f'Score for solver={solver}, shrinkage={shrinkage}: {clf_score}')\n",
    "\n",
    "\n",
    "    def LDA_classifier(self, cv_folds):\n",
    "        clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage=0.5)\n",
    "\n",
    "        cv_folds_scores = cross_val_score(clf, self.dataset.X_train, self.dataset.y_train, cv=cv_folds)\n",
    "        mean_cv_folds_score = cv_folds_scores.mean()\n",
    "\n",
    "        clf.fit(self.dataset.X_train, self.dataset.y_train)\n",
    "        test_score = clf.score(self.dataset.X_test, self.dataset.y_test)\n",
    "\n",
    "        print(f'Mean cross-validation score for LDA: {mean_cv_folds_score}')\n",
    "        print(f'Test score for LDA: {test_score}')\n",
    "        \n",
    "\n",
    "    def RF_tuner(self):\n",
    "        print(\"\\nRandom Forest Tuner:\")\n",
    "        max_features = [1, 'sqrt', 'log2']\n",
    "        max_depths = [None, 2, 3, 4, 5]\n",
    "        for f, d in product(max_features, max_depths): # with product we can iterate through all possible combinations\n",
    "            rf = RandomForestClassifier(n_estimators=100, \n",
    "                                        criterion='entropy', \n",
    "                                        max_features=f, \n",
    "                                        max_depth=d, \n",
    "                                        n_jobs=self.n_jobs,\n",
    "                                        random_state=15)\n",
    "            rf.fit(self.dataset.X_train, self.dataset.y_train)\n",
    "            self.dataset.y_pred = rf.predict(X=self.dataset.X_test)\n",
    "            print(' Classification accuracy on test set with max features = {} and max_depth = {}: {:.3f}'.format(f, d, accuracy_score(self.dataset.y_test,self.dataset.y_pred)))\n",
    "\n",
    "        # chosen max_depth of None, max_features of log2\n",
    "        n_estimators = [50, 100, 150, 200, 500]\n",
    "        criterion = ['gini', 'entropy', 'log_loss']\n",
    "        for n, c  in product(n_estimators, criterion):\n",
    "            rf = RandomForestClassifier(n_estimators=n,\n",
    "                                        criterion=c, \n",
    "                                        max_features=1, \n",
    "                                        max_depth=None, \n",
    "                                        n_jobs=self.n_jobs,\n",
    "                                        random_state=15)\n",
    "            rf.fit(self.dataset.X_train, self.dataset.y_train)\n",
    "            self.dataset.y_pred = rf.predict(X=self.dataset.X_test)\n",
    "            print(' Classification accuracy on test set with n_estimators = {} and criterion = {}: {:.3f}'.format(n, c, accuracy_score(self.dataset.y_test,self.dataset.y_pred)))\n",
    "            ML_Utilities.plot_confustion_matrix(self.dataset.y_test, self.dataset.y_pred)\n",
    "\n",
    "\n",
    "    # Post-tuned Random Forest Classifier\n",
    "    def RF_classifier(self, cv_folds:int=10):\n",
    "        # source: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "        print(\"\\nRandom Forest Classifier:\")\n",
    "        \n",
    "        if (self.database == 'Spider'):\n",
    "            rf = RandomForestClassifier(n_estimators=100,\n",
    "                                        criterion='log_loss', \n",
    "                                        max_features='log2', \n",
    "                                        max_depth=None, \n",
    "                                        n_jobs=self.n_jobs,\n",
    "                                        random_state=15)\n",
    "        elif (self.database == 'BrainPatch'):\n",
    "            rf = RandomForestClassifier(n_estimators=500,\n",
    "                                        criterion='gini',\n",
    "                                        max_features=1,\n",
    "                                        max_depth=None,\n",
    "                                        n_jobs=self.n_jobs,\n",
    "                                        random_state=15)\n",
    "\n",
    "        self.evaluate(rf, cv_folds)\n",
    "\n",
    "\n",
    "    # Naive Bayes Classifier\n",
    "    def NB_tuner(self):\n",
    "        print(\"\\nNaive Bayes Classifier:\")\n",
    "        # create a Multinomial Naive Bayes classifier\n",
    "        nb = MultinomialNB()\n",
    "\n",
    "        # set up a parameter grid to search\n",
    "        param_grid = {'alpha': [0.01, 0.1, 0.5, 1.0, 5.0, 10.0]}\n",
    "\n",
    "        # perform a grid search to find the best hyperparameters\n",
    "        grid_search = GridSearchCV(nb, param_grid=param_grid, cv=5, iid=False)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # print the best hyperparameters\n",
    "        print('Best hyperparameters: {}'.format(grid_search.best_params_))\n",
    "\n",
    "        # make predictions on the test data using the best model\n",
    "        y_pred = grid_search.predict(X_test)\n",
    "\n",
    "        # calculate and print the accuracy of the best model\n",
    "        print('Classification accuracy on test set with best hyperparameters: {:.3f}'.format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "    # Post tuned Naive Bayes Classifer\n",
    "    def NB_classifier(self, cv_folds:int=10):\n",
    "        # create a Gaussian Naive Bayes classifier\n",
    "        nb = GaussianNB()\n",
    "\n",
    "        # fit the classifier to the training data\n",
    "        nb.fit(self.dataset.X_train, self.dataset.y_train)\n",
    "\n",
    "        # make predictions on the test data\n",
    "        self.dataset.y_pred = nb.predict(self.dataset.X_test)\n",
    "\n",
    "        # calculate and print the accuracy of the classifier\n",
    "        print('Classification accuracy on test set with Naive Bayes: {:.3f}'.format(accuracy_score(self.dataset.y_test, self.dataset.y_pred)))\n",
    "\n",
    "\n",
    "    # Support Vector Machine Classifier \n",
    "    def SVM_tuner(self):\n",
    "        print(\"\\nSupport Vector Machine Classifier:\")\n",
    "        # Define a list of hyperparameters to tune\n",
    "        kernels = ['linear', 'poly', 'rbf', 'sigmoid, 'precomputed']\n",
    "        Cs = [0.1, 1, 10]\n",
    "\n",
    "        # Iterate through all combinations of hyperparameters\n",
    "        for kernel, C in product(kernels, Cs):\n",
    "            svm = SVC(kernel=kernel, C=C, random_state=15)\n",
    "            svm.fit(self.dataset.X_train, self.dataset.y_train)\n",
    "            y_pred = svm.predict(self.dataset.X_test)\n",
    "            accuracy = accuracy_score(self.dataset.y_test, y_pred)\n",
    "            print(' SVM with kernel = {} and C = {}: accuracy = {:.3f}'.format(kernel, C, accuracy))\n",
    "            ML_Utilities.plot_confustion_matrix(self.dataset.y_test, y_pred)\n",
    "\n",
    "    # Post-tuned Support Vector Machine Classifier \n",
    "    def SVM_classifier(self, cv_folds:int=10):\n",
    "        svm = SVC(kernel='linear', C=1, random_state=15)\n",
    "        # Perform cross-fold validation\n",
    "        scores = cross_val_score(svm, self.dataset.X_train, self.dataset.y_train, cv=5)\n",
    "        # Print the mean and standard deviation of the cross-validation scores\n",
    "        print(\"Cross-validation accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "        # Make predictions using cross-fold validation\n",
    "        y_pred = cross_val_predict(svm, self.dataset.X_test, self.dataset.y_test, cv=5)\n",
    "        print('Classification accuracy with kernel = {} and C = {}: {:.3f}'.format(svm.kernel, svm.C, accuracy_score(self.dataset.y_test, y_pred)))\n",
    "        ML_Utilities.plot_confustion_matrix(self.dataset.y_test, y_pred)\n",
    "\n",
    "\n",
    "    # Tuning Bagging Classifier\n",
    "    def BT_tuner(self):\n",
    "        print(\"\\nBagging Decision Trees Classifier:\")\n",
    "        # Define the hyperparameters to tune\n",
    "        param_grid = {\n",
    "            'n_estimators': [10, 50, 100],\n",
    "            'max_samples': [0.5, 1.0],\n",
    "            'max_features': [0.5, 1.0],\n",
    "            'bootstrap': [True, False],\n",
    "            'bootstrap_features': [True, False]\n",
    "        }\n",
    "\n",
    "        bt_clf = BaggingClassifier(estimator=SVC())\n",
    "\n",
    "        # Perform grid search to find the best hyperparameters\n",
    "        grid_search = GridSearchCV(bt_clf, param_grid=param_grid, cv=2, n_jobs=1, verbose=2)\n",
    "        grid_search.fit(self.dataset.X_train, self.dataset.y_train)\n",
    "\n",
    "        # Print the best hyperparameters\n",
    "        print(\" Best hyperparameters: \", grid_search.best_params_)\n",
    "\n",
    "\n",
    "    # Post-tuned Bagged Trees Classifier\n",
    "    def BT_classifier(self, cv_folds:int=10):\n",
    "        bt = BaggingClassifier()\n",
    "        self.evaluate(bt, cv_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.686647Z",
     "iopub.status.busy": "2023-03-28T14:15:57.686556Z",
     "iopub.status.idle": "2023-03-28T14:15:58.903839Z",
     "shell.execute_reply": "2023-03-28T14:15:58.903584Z"
    }
   },
   "outputs": [],
   "source": [
    "# selected_features_ECG = Utilities.load_dataframe(f'{dataframe_path}Feature Selected.csv')\n",
    "\n",
    "dataset = ML_Utilities.prepare(fs.selected_features_ECG, train_split=0.70, test_split=0.30, normalise=False)\n",
    "\n",
    "tml = Traditional_ML(dataset, DATABASE, number_of_cores=1)\n",
    "\n",
    "# 'LDA' for Linear Discriminant Analysis\n",
    "# 'RF' for Random Forest\n",
    "# 'NB' for Naive Bayes\n",
    "# 'BT' for Bagged Trees\n",
    "# 'SVM' for Support Vector Machine\n",
    "tml.model = 'LDA'\n",
    "tml.tune()\n",
    "tml.classify(cv_folds=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Convolutional Neural Network\n",
    "* Prepares data by converting the sorted data into a 3 dimensional shape\n",
    "* 1D Convolutional Neural Network tuning and regression model\n",
    "* Regression is converted to discrete classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 dimensional Convolutional Deep Neural Network\n",
    "class CNN1D():\n",
    "    def __init__(self, dataset:Dataset, database):\n",
    "        self.dataset = dataset\n",
    "        self.database = database\n",
    "\n",
    "    # prepare data by setting shape to 3 dimensions\n",
    "    def prepare(self):\n",
    "        # get shape of data\n",
    "        self.n_features, self.n_classes = self.dataset.X_train.shape[1], self.dataset.y_train.nunique()\n",
    "\n",
    "        # Format to a NumPy array of 32-bit floating-point numbers and reshape to a 3D array with dimensions (number of samples, number of features, 1)\n",
    "        self.dataset.X = np.asarray(self.dataset.X).astype(np.float32).reshape(-1, self.n_features, 1)\n",
    "        self.dataset.X_test = np.asarray(self.dataset.X_test).astype(np.float32).reshape(-1, self.n_features, 1)\n",
    "        self.dataset.X_train = np.asarray(self.dataset.X_train).astype(np.float32).reshape(-1, self.n_features, 1)\n",
    "        self.dataset.y = np.asarray(self.dataset.y).astype(np.float32)\n",
    "        self.dataset.y_test = np.asarray(self.dataset.y_test).astype(np.float32)\n",
    "        self.dataset.y_train = np.asarray(self.dataset.y_train).astype(np.float32)\n",
    "\n",
    "        # convert to one-hot encoding\n",
    "        self.dataset.y = to_categorical((self.dataset.y-1), self.n_classes)\n",
    "        self.dataset.y_test = to_categorical((self.dataset.y_test-1), self.n_classes)\n",
    "        self.dataset.y_train = to_categorical((self.dataset.y_train-1), self.n_classes)\n",
    "    \n",
    "    \n",
    "    def convert_to_labels(self, y_true, y_pred):\n",
    "        # convert from regression to class label\n",
    "        max_indices = np.argmax(y_pred, axis=1)\n",
    "        result = np.eye(y_pred.shape[1])[max_indices]\n",
    "        # Convert the one-hot vectors back to their original labels\n",
    "        labels = [1.0, 2.0, 3.0]\n",
    "        label_indices = np.argmax(result, axis=1)\n",
    "        y_pred_l = [labels[i] for i in label_indices]\n",
    "\n",
    "        label_indices = np.argmax(y_true, axis=1)\n",
    "        y_true_l = [labels[i] for i in label_indices]     \n",
    "\n",
    "        return y_true_l, y_pred_l \n",
    "    \n",
    "\n",
    "    def tune(self):\n",
    "        # hyperparameters\n",
    "        learning_rate, epochs, batch_size = 0.00001, 20, 32\n",
    "\n",
    "        # Define the model architecture\n",
    "        self.model = Sequential([\n",
    "            # Input layer\n",
    "            InputLayer(input_shape=(self.n_features, 1)),\n",
    "            \n",
    "            # 1D Convolutional layer\n",
    "            Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "            \n",
    "            # Max Pooling layer\n",
    "            MaxPooling1D(pool_size=2),\n",
    "            \n",
    "            # Flatten layer\n",
    "            Flatten(),\n",
    "            \n",
    "            # Dropout layer\n",
    "            Dropout(0.5),\n",
    "            \n",
    "            # Fully Connected layer 1\n",
    "            Dense(units=128, activation='relu'),\n",
    "            \n",
    "            # Fully Connected layer 2 to number of class outputs\n",
    "            Dense(units=self.n_classes, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        # print model architecture summary\n",
    "        self.model.summary()\n",
    "\n",
    "        # Compile the model using the Adam Optimiser\n",
    "        self.model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Train the model\n",
    "        self.model.fit(self.dataset.X_train, self.dataset.y_train, epochs=epochs, batch_size=batch_size, validation_data=(self.dataset.X_test, self.dataset.y_test), callbacks=[PlotLearning()])\n",
    "\n",
    "        # Evaluate the model\n",
    "        score = self.model.evaluate(self.dataset.X_test, self.dataset.y_test, batch_size=32)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "\n",
    "        y_pred = self.model.predict(self.dataset.X_test)\n",
    "        y_true_l, y_pred_l = self.convert_to_labels(self.dataset.y_test, y_pred)\n",
    "        # plot confusion matrix\n",
    "        ML_Utilities.plot_confustion_matrix(y_true_l, y_pred_l)\n",
    "\n",
    "\n",
    "    # post-tuned classifier with k-fold evaluation and confusion matrix\n",
    "    def predict(self, cv_folds=5):\n",
    "        print(f\"Conducting {cv_folds} Cross Validation Folds\")\n",
    "\n",
    "        # split the dataset into training and validation sets using KFold\n",
    "        kf = KFold(n_splits=cv_folds, shuffle=True, random_state=15)\n",
    "        scores = []\n",
    "        X_val, y_val = 0, 0\n",
    "        for fold, (train_index, val_index) in enumerate(kf.split(self.dataset.X)):\n",
    "            print('Fold', fold+1)\n",
    "            X_train, y_train = self.dataset.X[train_index], self.dataset.y[train_index]\n",
    "            X_val, y_val = self.dataset.X[val_index], self.dataset.y[val_index]\n",
    "\n",
    "            # train your model on the current fold\n",
    "            self.model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), callbacks=[PlotLearning()])\n",
    "\n",
    "            # Evaluate the model on the test set for this fold and store the accuracy score\n",
    "            score = self.model.evaluate(X_val, y_val, verbose=0)\n",
    "            scores.append(score)\n",
    "\n",
    "        # Calculate the average accuracy score across all folds\n",
    "        average_test_loss = sum(scores[0]) / len(scores[0])\n",
    "        average_accuracy = sum(scores[1]) / len(scores[1])\n",
    "\n",
    "        print(f'Average test Loss: {average_test_loss}')\n",
    "        print(f'Average test accuracy: {average_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp.segment(window_length_s=30, overlap=0.1) # segment data with a sliding window of length to be 0.8s which is average heart beat\n",
    "# pp.clean() \n",
    "# pp.create_2d() # converts data to 2d vector (required for CNN)\n",
    "# Utilities.save_dataframe(pp.preprocessed_ECG_2d, dataframe_path, 'Preprocessed-2d-30.s')\n",
    "\n",
    "dataset = ML_Utilities.prepare(pp.preprocessed_ECG_2d, train_split=0.70, test_split=0.30, normalise=True)\n",
    "\n",
    "# Tune and classify the one dimensional convolutional deep neural network\n",
    "cnn = CNN1D(dataset, DATABASE)\n",
    "cnn.prepare()\n",
    "cnn.tune()\n",
    "cnn.predict(2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
