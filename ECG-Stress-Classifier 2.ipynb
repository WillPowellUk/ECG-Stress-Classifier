{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stress Classifier\n",
    "## Overview\n",
    "1. [Select Database](#select-database)\n",
    "2. [Import Libraries and Load Useful Utilities](#import-libraries-and-load-useful-utilities)\n",
    "3. [Data Extraction](#data-extraction): Downloads and sorts through databases.\n",
    "4. [Pre-processing](#pre-processing): filtering and signal cleaning.\n",
    "5. [Feature Extraction](#feature-extraction): R-R peaks, PQRST peaks, EDR, in addition to mean, kurtosis etc.\n",
    "6. [Feature Selection](#feature-selection): Visualise labelled feature distribution, select desired labels etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select Database\n",
    "Select between:\n",
    "* [Spider-Fearful ECG](https://doi.org/10.1371/journal.pone.0231517) \n",
    "* BrainPatch ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:56.740564Z",
     "iopub.status.busy": "2023-03-28T14:15:56.740124Z",
     "iopub.status.idle": "2023-03-28T14:15:56.746295Z",
     "shell.execute_reply": "2023-03-28T14:15:56.745553Z"
    }
   },
   "outputs": [],
   "source": [
    "DATABASE = 'Spider'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Load Useful Utilities\n",
    "Please run `pip install -r requirements.txt` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:56.748730Z",
     "iopub.status.busy": "2023-03-28T14:15:56.748407Z",
     "iopub.status.idle": "2023-03-28T14:15:57.623281Z",
     "shell.execute_reply": "2023-03-28T14:15:57.623003Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.624839Z",
     "iopub.status.busy": "2023-03-28T14:15:57.624743Z",
     "iopub.status.idle": "2023-03-28T14:15:57.631111Z",
     "shell.execute_reply": "2023-03-28T14:15:57.630859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Useful Utilities\n",
    "class Utilities():\n",
    "    def __init__(self):    \n",
    "        pass\n",
    "\n",
    "    def progress_bar(current_message, current, total, bar_length=20):\n",
    "        fraction = current / total\n",
    "        arrow = int(fraction * bar_length - 1) * '-' + '>'\n",
    "        padding = int(bar_length - len(arrow)) * ' '\n",
    "        ending = '\\n' if current == total else '\\r'\n",
    "        print(f'{current_message}: [{arrow}{padding}] {int(fraction*100)}%', end=ending)\n",
    "\n",
    "\n",
    "    def check_csv_exists(folder_path, index):\n",
    "        # read the CSV file into a dataframe and append to the list\n",
    "        filename = os.path.join(folder_path, f'df_{index}.csv')\n",
    "        try:\n",
    "            df = pd.read_csv(filename)\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "        return filename\n",
    "\n",
    "\n",
    "    def load_dataframe(filename):\n",
    "        # read the CSV file into a dataframe and append to the list\n",
    "        df = pd.read_csv(filename)\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def save_dataframe_list(list_of_dfs:list[pd.DataFrame], folder_path:str, file_name:str):\n",
    "        # create directoy if necessary\n",
    "        os.makedirs(folder_path, exist_ok=True) \n",
    "        for i, df in enumerate(list_of_dfs):\n",
    "            file_path = f\"{folder_path}/{file_name}_{i}.csv\"\n",
    "            df.to_csv(file_path, index=False)\n",
    "\n",
    "\n",
    "    def save_dataframe(df:pd.DataFrame, folder_path:str, file_name:str):\n",
    "        # create directoy if necessary\n",
    "        os.makedirs(folder_path, exist_ok=True) \n",
    "        df.to_csv(f'{folder_path}/{file_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For bigger plots\n",
    "plt.rcParams['figure.figsize'] = [10, 6]  \n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Extraction\n",
    "* Downloads data, normalizes timeframe, attaches labels, and saves sorted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.632504Z",
     "iopub.status.busy": "2023-03-28T14:15:57.632310Z",
     "iopub.status.idle": "2023-03-28T14:15:57.638674Z",
     "shell.execute_reply": "2023-03-28T14:15:57.638402Z"
    }
   },
   "outputs": [],
   "source": [
    "# This class handles downloading and sorting the Spider database.\n",
    "# Method from this paper https://doi.org/10.1371/journal.pone.0231517\n",
    "class SpiderDataExtraction():\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory\n",
    "        self.sampling_frequency = 100\n",
    "        self.sorted_ECG = pd.DataFrame(columns=['Timestamp', 'ECG', 'Stress Level'])\n",
    "\n",
    "    \n",
    "    # Downloads and extracts database \n",
    "    def download_data(self):\n",
    "        if sys.platform == 'linux':\n",
    "            url = 'https://physionet.org/files/ecg-spider-clip/1.0.0/'\n",
    "            print(\"Downloading database...this may take a while\")\n",
    "            os.makedirs(self.directory, exist_ok=True)\n",
    "            cmd = f\"wget -r -N -c -np -P {self.directory} {url}\"\n",
    "            subprocess.run(cmd)\n",
    "        else:\n",
    "            sys.exit(\"Unable to download database. If you are running Windows/Mac please download manually via https://physionet.org/content/ecg-spider-clip/1.0.0/\")\n",
    "    \n",
    "\n",
    "    # sorts data from each participant, labelling each ECG recording and appends to one dataframe.\n",
    "    # Following the SB approach in the study.\n",
    "    def sort_data(self):       \n",
    "        directory = self.directory + '/physionet.org/files/ecg-spider-clip/1.0.0/'\n",
    "        \n",
    "        # Exclude VP70 because of noise\n",
    "        sub_directories = ['VP02', 'VP03','VP05','VP06','VP08','VP09','VP11','VP12','VP14','VP15','VP17','VP18','VP20','VP23','VP24','VP26','VP27',\n",
    "                'VP29','VP30','VP32','VP33','VP35','VP36','VP38','VP39','VP41','VP42','VP44','VP45','VP47','VP48','VP50','VP51','VP53',\n",
    "                'VP54','VP56','VP57','VP59','VP61','VP62','VP63','VP64','VP65','VP66','VP68','VP69','VP71','VP72','VP73','VP74',\n",
    "                'VP75','VP76','VP77','VP78','VP79','VP80']\n",
    "        \n",
    "        # Path to Ratings file for all particpants\n",
    "        subjective_ratings_file = f'{self.directory}/Subjective Ratings.txt'\n",
    "\n",
    "        # Read in the subject ratings file (ignore arousal markers, interested in angst)\n",
    "        ratings_df = (\n",
    "            pd.read_csv(subjective_ratings_file, sep='\\t', names=['Subject','Group','Session', '4', '8', '12', '16', 'NA1', 'NA2', 'NA3', 'NA4'], encoding='UTF-16')\n",
    "            .drop(columns=['NA1', 'NA2', 'NA3', 'NA4'])\n",
    "            .iloc[1:]\n",
    "            .reset_index(drop=True)\n",
    "            .astype(int)\n",
    "        )\n",
    "\n",
    "        for index, sub_directory in enumerate(sub_directories):\n",
    "            Utilities.progress_bar('Sorting database', index, len(sub_directories)-1)\n",
    "\n",
    "            # set participant data paths\n",
    "            ECG_file = f'{directory}{sub_directory}/BitalinoECG.txt'\n",
    "            triggers_file = f'{directory}{sub_directory}/Triggers.txt'\n",
    "\n",
    "            # Get participant number\n",
    "            participant_no = int(sub_directory[2:])\n",
    "\n",
    "            # read in particpant ECG raw data file and reorder to get columns Timestamp, ECG \n",
    "            raw_df = pd.read_csv(ECG_file, sep='\\t', names = ['ECG','Timestamp','NA'])\n",
    "            raw_df = raw_df.drop(columns=['NA'])\n",
    "            raw_df = raw_df[['Timestamp', 'ECG']]\n",
    "            \n",
    "            # Read in participant trigger file\n",
    "            triggers_df = pd.read_csv(triggers_file, sep='\\t', names = ['Clip','On','Off'])\n",
    "\n",
    "            # Determine stress levels by correspoding the raw ecg data with the triggers file and the Subjective Ratings file.\n",
    "            # Iterate through the 16 stress clips (first clip is a demo):\n",
    "            for i in range(1, 17):\n",
    "                # Determine row in ratings file\n",
    "                row = ratings_df.loc[ratings_df['Subject'] == participant_no].index[0]\n",
    "\n",
    "                # find stress for the clip in the ratings file\n",
    "                stress_level = ratings_df.iloc[row]['4'] if i <= 4 else ratings_df.iloc[row]['8'] if i <= 8 else ratings_df.iloc[row]['12'] if i <= 12 else ratings_df.iloc[row]['16']\n",
    "\n",
    "                # convert stress level to Low, Medium or High (1-3)\n",
    "                stress_level = 2 if (stress_level <= 2) else 3\n",
    "\n",
    "                # Get 60 second slice of ECG data for that clip\n",
    "                clip_start_time = triggers_df.iloc[i]['On']\n",
    "                start_index = raw_df.index[raw_df['Timestamp']>clip_start_time].tolist()[0]\n",
    "                clip_df = raw_df.iloc[start_index:start_index + (self.sampling_frequency * 60)].copy(deep=False)\n",
    "                clip_df['Stress Level'] = stress_level\n",
    "                self.sorted_ECG = pd.concat([self.sorted_ECG, clip_df], axis=0, ignore_index=True)\n",
    "\n",
    "            # Add the last 3 minute resting phase (stress level Low) to the data\n",
    "            rest_start_time = triggers_df.iloc[-1]['On']\n",
    "            start_index = (raw_df['Timestamp'] > rest_start_time).idxmin() + (self.sampling_frequency * 120)\n",
    "            rest_df = raw_df.iloc[start_index: start_index + (self.sampling_frequency * 180)].copy(deep=False)\n",
    "            rest_df.loc[:, 'Stress Level'] = 1\n",
    "            self.sorted_ECG = pd.concat([self.sorted_ECG, rest_df], axis=0, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting database: [->                  ] 10%\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m sde \u001b[39m=\u001b[39m SpiderDataExtraction(\u001b[39m'\u001b[39m\u001b[39mData/Spider\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[39m# sde.download_data()\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m sde\u001b[39m.\u001b[39;49msort_data()\n\u001b[0;32m      5\u001b[0m Utilities\u001b[39m.\u001b[39msave_dataframe(sde\u001b[39m.\u001b[39msorted_ECG, \u001b[39m'\u001b[39m\u001b[39mData/Spider/Dataframes\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSorted\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[35], line 87\u001b[0m, in \u001b[0;36mSpiderDataExtraction.sort_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m rest_df \u001b[39m=\u001b[39m raw_df\u001b[39m.\u001b[39miloc[start_index: start_index \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_frequency \u001b[39m*\u001b[39m \u001b[39m180\u001b[39m)]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     86\u001b[0m rest_df\u001b[39m.\u001b[39mloc[:, \u001b[39m'\u001b[39m\u001b[39mStress Level\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msorted_ECG \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msorted_ECG, rest_df], axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, ignore_index\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\wfp21\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wfp21\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:381\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39mConcatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39m1   3   4\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    368\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    369\u001b[0m     objs,\n\u001b[0;32m    370\u001b[0m     axis\u001b[39m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    378\u001b[0m     sort\u001b[39m=\u001b[39msort,\n\u001b[0;32m    379\u001b[0m )\n\u001b[1;32m--> 381\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mget_result()\n",
      "File \u001b[1;32mc:\\Users\\wfp21\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:616\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    612\u001b[0m             indexers[ax] \u001b[39m=\u001b[39m obj_labels\u001b[39m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    614\u001b[0m     mgrs_indexers\u001b[39m.\u001b[39mappend((obj\u001b[39m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 616\u001b[0m new_data \u001b[39m=\u001b[39m concatenate_managers(\n\u001b[0;32m    617\u001b[0m     mgrs_indexers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew_axes, concat_axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbm_axis, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy\n\u001b[0;32m    618\u001b[0m )\n\u001b[0;32m    619\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy:\n\u001b[0;32m    620\u001b[0m     new_data\u001b[39m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mc:\\Users\\wfp21\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:223\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    217\u001b[0m vals \u001b[39m=\u001b[39m [ju\u001b[39m.\u001b[39mblock\u001b[39m.\u001b[39mvalues \u001b[39mfor\u001b[39;00m ju \u001b[39min\u001b[39;00m join_units]\n\u001b[0;32m    219\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m blk\u001b[39m.\u001b[39mis_extension:\n\u001b[0;32m    220\u001b[0m     \u001b[39m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[39m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[39m#  than concat_compat\u001b[39;00m\n\u001b[1;32m--> 223\u001b[0m     values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate(vals, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    224\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m     \u001b[39m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[0;32m    226\u001b[0m     values \u001b[39m=\u001b[39m concat_compat(vals, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if DATABASE == 'Spider':\n",
    "    sde = SpiderDataExtraction('Data/Spider')\n",
    "    # sde.download_data()\n",
    "    sde.sort_data()\n",
    "    Utilities.save_dataframe(sde.sorted_ECG, 'Data/Spider/Dataframes', 'Sorted')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pre-Processing\n",
    "* Interpolates data to sample rate using timestamps \n",
    "* Semgents data using rolling window with overlap\n",
    "* Cleans data using Neurokit's 5th Order Butterworth filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.639837Z",
     "iopub.status.busy": "2023-03-28T14:15:57.639761Z",
     "iopub.status.idle": "2023-03-28T14:15:57.644906Z",
     "shell.execute_reply": "2023-03-28T14:15:57.644649Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define PreProcessing Class:\n",
    "# Segments data using rolling window \n",
    "# Cleans data using Neurokit2\n",
    "# Documentation can be found here: https://neuropsychology.github.io/NeuroKit/functions/ecg.html       \n",
    "class PreProcessing():\n",
    "    def __init__(self, sorted_ECG:pd.DataFrame, sampling_frequency:int):\n",
    "        self.sorted_ECG = sorted_ECG\n",
    "        self.sampling_frequency = sampling_frequency\n",
    "        self.preprocessed_ECG = pd.DataFrame(columns=['Timestamp', 'ECG', 'Stress Level'])\n",
    "\n",
    "\n",
    "    # interpolates data to achieve sampling rate\n",
    "    def interpolate(self):\n",
    "        # convert timestamp column to a NumPy array\n",
    "        timestamps = self.sorted_ECG['Timestamp'].to_numpy()\n",
    "\n",
    "        # calculate the time difference between each pair of adjacent timestamps\n",
    "        time_diff = np.diff(timestamps)\n",
    "\n",
    "        # calculate the average sampling rate of the data\n",
    "        sampling_frequency = 1 / np.mean(time_diff)\n",
    "\n",
    "        print(f\"Average sampling rate: {sampling_frequency}\")\n",
    "\n",
    "        # interpolate the data to obtain 100Hz sampling rate\n",
    "        self.sorted_ECG['Timestamp'] = np.arange(timestamps[0], timestamps[-1], 1 / self.sampling_frequency)\n",
    "        self.sorted_ECG['ECG'] = np.interp(self.sorted_ECG['Timestamp'], timestamps, self.sorted_ECG['ECG'])\n",
    "        \n",
    "\n",
    "    # segments data with overlap using rolling window\n",
    "    def segment(self, window_length_s:int, overlap:float):\n",
    "        # convert window_length in seconds to samples\n",
    "        self.window_samples = window_length_s * self.sampling_frequency\n",
    "        # Calculate the step_size as the fraction of the total window samples\n",
    "        step_size = int(self.window_samples * (1-overlap)) \n",
    "\n",
    "        # Initialize starting variables\n",
    "        current_index = 0\n",
    "        current_stressed = self.sorted_ECG['Stress Level'][current_index]\n",
    "\n",
    "        # faster to concatenate at the end \n",
    "        preprocessed_ECG_list = []\n",
    "        \n",
    "        # Loop through the entire dataframe\n",
    "        while current_index < len(self.sorted_ECG['ECG']):  \n",
    "            Utilities.progress_bar('Segmenting data', current_index, len(self.sorted_ECG))\n",
    "            # calculate end index in window and exit if out of bounds          \n",
    "            end_index = current_index + self.window_samples\n",
    "            if (end_index > len(self.sorted_ECG['ECG'])):\n",
    "                break\n",
    "            \n",
    "            # Check if the window overlaps a different label\n",
    "            end_stressed = self.sorted_ECG['Stress Level'][end_index]\n",
    "\n",
    "            # If the next window has a different label, skip to next start of next label\n",
    "            if end_stressed != current_stressed:\n",
    "                while (current_stressed == self.sorted_ECG['Stress Level'][current_index]):\n",
    "                    current_index += 1\n",
    "                current_stressed = end_stressed\n",
    "\n",
    "            # otherwise, add segment to list of pre-processed ECG\n",
    "            else:\n",
    "                # append segment to dataframe\n",
    "                preprocessed_ECG_list.append(self.sorted_ECG.iloc[current_index:current_index + self.window_samples].astype('Float64'))\n",
    "\n",
    "                # Shift the window\n",
    "                current_index += step_size\n",
    "        \n",
    "        self.preprocessed_ECG = pd.concat(preprocessed_ECG_list, axis=0, ignore_index=True).astype('Float64')\n",
    "        Utilities.progress_bar('Segmenting data', current_index, current_index)\n",
    "\n",
    "    def clean(self):\n",
    "        # Clean each sample in the stressed and not stressed data (overwrites original data)\n",
    "        # using method 'neurokit' (0.5 Hz high-pass butterworth filter (order = 5), followed by powerline filtering) but can be changed to other cleaning methods\n",
    "        print(\"Cleaning data...\")\n",
    "        self.preprocessed_ECG['ECG'] = pd.Series(nk.ecg_clean(self.preprocessed_ECG['ECG'], self.sampling_frequency, method='neurokit')).astype('Float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting data: [------------------->] 100%\n",
      "Cleaning data...\n"
     ]
    }
   ],
   "source": [
    "pp = PreProcessing(sde.sorted_ECG, sde.sampling_frequency)\n",
    "\n",
    "# Timestamps are not valid for Spider database, so should not interpolate.\n",
    "if DATABASE != 'Spider':\n",
    "    pp.interpolate()\n",
    "\n",
    "window_length_s = 10\n",
    "overlap = 0.1\n",
    "pp.segment(window_length_s, overlap)\n",
    "pp.clean()\n",
    "Utilities.save_dataframe(pp.preprocessed_ECG, 'Data/Spider/Dataframes', 'Preprocessed')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction\n",
    "* Extracts features such as HRV time, frequency and non-linear domain, EDR etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.649031Z",
     "iopub.status.busy": "2023-03-28T14:15:57.648948Z",
     "iopub.status.idle": "2023-03-28T14:15:57.656454Z",
     "shell.execute_reply": "2023-03-28T14:15:57.656197Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main class that extracts features from a dictionary of sorted dataframes and stores to csv\n",
    "class FeatureExtraction():\n",
    "    # takes in cleaned ECG data\n",
    "    def __init__(self, preprocessed_ECG:pd.DataFrame, window_samples:int, sampling_frequency:int, show_plot:bool=False):\n",
    "        self.preprocessed_ECG = preprocessed_ECG\n",
    "        self.window_samples = window_samples\n",
    "        self.sampling_frequency = sampling_frequency\n",
    "        self.show_plot = show_plot\n",
    "        self.feature_extracted_ECG = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # Extracts features from ECG using neurokit.\n",
    "    def get_neurokit_features(self, EDR=False):\n",
    "        index = 0\n",
    "        while index < len(self.preprocessed_ECG['ECG']):\n",
    "            Utilities.progress_bar('Extracting Neurokit Features', index, len(self.preprocessed_ECG['ECG']))\n",
    "\n",
    "            # get segment ECG and stress level from dataframe \n",
    "            segment = self.preprocessed_ECG.iloc[index:index + self.window_samples]['ECG']\n",
    "            stress_level = self.preprocessed_ECG.iloc[index]['Stress Level']\n",
    "            features = pd.DataFrame({'Stress Level': [stress_level]})\n",
    "            \n",
    "            index += self.window_samples\n",
    "\n",
    "            # only show plot once\n",
    "            if index!=0:\n",
    "                self.show_plot = False \n",
    "\n",
    "            # extract R-R peaks\n",
    "            np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "            r_peaks_df = nk.ecg_peaks(segment, sampling_rate=self.sampling_frequency, correct_artifacts=True)[0]\n",
    "\n",
    "            # Extract HRV features from R-R peaks, see https://neuropsychology.github.io/NeuroKit/functions/hrv.html \n",
    "            # compute HRV - time, frequency and nonlinear indices.\n",
    "            warnings.filterwarnings('ignore') # temporarily supress warnings\n",
    "            HRV_time = nk.hrv_time(r_peaks_df, sampling_frequency=self.sampling_frequency, show=self.show_plot)\n",
    "            HRV_frequency = nk.hrv_frequency(r_peaks_df, sampling_frequency=self.sampling_frequency, show=self.show_plot)\n",
    "            warnings.filterwarnings('default')\n",
    "\n",
    "            # compute Shannon Entropy (SE) using signal symbolization and discretization\n",
    "            # see https://neuropsychology.github.io/NeuroKit/functions/complexity.html#entropy-shannon \n",
    "            SE = nk.entropy_shannon(segment, symbolize='A')[0]\n",
    "            HRV_SE = pd.DataFrame([SE], columns=['HRV_SE'])\n",
    "            # concat to dataframe\n",
    "            features = pd.concat([features, HRV_time, HRV_frequency, HRV_SE], axis=1)\n",
    "            \n",
    "            if EDR:\n",
    "                # Get ECG Derived Respiration (EDR) and add to the data\n",
    "                warnings.filterwarnings('ignore') # temporarily supress warnings\n",
    "                ecg_rate = nk.signal_rate(r_peaks_df, sampling_rate=self.sampling_frequency, desired_length=len(r_peaks_df))\n",
    "                warnings.filterwarnings('default')\n",
    "                EDR_sample = nk.ecg_rsp(ecg_rate, sampling_rate=self.sampling_frequency)\n",
    "                if self.show_plot:\n",
    "                    nk.signalplot(segment)\n",
    "                    nk.signal_plot(EDR_sample)\n",
    "                EDR_Distance = pd.DataFrame(nk.signal_findpeaks(EDR_sample)[\"Distance\"], columns=['EDR_Distance'])\n",
    "                # concat to dataframe\n",
    "                features = pd.concat([features, EDR_Distance], axis=1)\n",
    "\n",
    "            # concat features to main dataframe\n",
    "            self.feature_extracted_ECG = pd.concat([self.feature_extracted_ECG, features], axis=0, ignore_index=True)\n",
    "        Utilities.progress_bar('Extracting Neurokit Features', index, index)\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.662551Z",
     "iopub.status.busy": "2023-03-28T14:15:57.662447Z",
     "iopub.status.idle": "2023-03-28T14:15:57.666162Z",
     "shell.execute_reply": "2023-03-28T14:15:57.665929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Neurokit Features: [>                   ] 0%\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m fe \u001b[39m=\u001b[39m FeatureExtraction(pp\u001b[39m.\u001b[39mpreprocessed_ECG, pp\u001b[39m.\u001b[39mwindow_samples, pp\u001b[39m.\u001b[39msampling_frequency, show_plot\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m fe\u001b[39m.\u001b[39;49mget_neurokit_features(EDR\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m      3\u001b[0m Utilities\u001b[39m.\u001b[39msave_dataframe(fe\u001b[39m.\u001b[39mfeature_extracted_ECG, \u001b[39m'\u001b[39m\u001b[39mData/Spider/Dataframes\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFeature Extracted\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[59], line 45\u001b[0m, in \u001b[0;36mFeatureExtraction.get_neurokit_features\u001b[1;34m(self, EDR)\u001b[0m\n\u001b[0;32m     43\u001b[0m HRV_SE \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame([SE], columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mHRV_SE\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     44\u001b[0m \u001b[39m# concat to dataframe\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m features \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([features, HRV_time, HRV_frequency, HRV_SE], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[39mif\u001b[39;00m EDR:\n\u001b[0;32m     48\u001b[0m     \u001b[39m# Get ECG Derived Respiration (EDR) and add to the data\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# temporarily supress warnings\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[59], line 45\u001b[0m, in \u001b[0;36mFeatureExtraction.get_neurokit_features\u001b[1;34m(self, EDR)\u001b[0m\n\u001b[0;32m     43\u001b[0m HRV_SE \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame([SE], columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mHRV_SE\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     44\u001b[0m \u001b[39m# concat to dataframe\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m features \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([features, HRV_time, HRV_frequency, HRV_SE], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[39mif\u001b[39;00m EDR:\n\u001b[0;32m     48\u001b[0m     \u001b[39m# Get ECG Derived Respiration (EDR) and add to the data\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# temporarily supress warnings\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:1197\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m   1195\u001b[0m \u001b[39mif\u001b[39;00m is_line:\n\u001b[0;32m   1196\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_suspend(thread, step_cmd, original_step_cmd\u001b[39m=\u001b[39minfo\u001b[39m.\u001b[39mpydev_original_step_cmd)\n\u001b[1;32m-> 1197\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_wait_suspend(thread, frame, event, arg)\n\u001b[0;32m   1198\u001b[0m \u001b[39melif\u001b[39;00m is_return:  \u001b[39m# return event\u001b[39;00m\n\u001b[0;32m   1199\u001b[0m     back \u001b[39m=\u001b[39m frame\u001b[39m.\u001b[39mf_back\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_wait_suspend\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 165\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_args[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mdo_wait_suspend(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[0;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fe = FeatureExtraction(pp.preprocessed_ECG, pp.window_samples, pp.sampling_frequency, show_plot=True)\n",
    "fe.get_neurokit_features(EDR=False)\n",
    "Utilities.save_dataframe(fe.feature_extracted_ECG, 'Data/Spider/Dataframes', 'Feature Extracted')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Selection:\n",
    "* Select desired features, sanity check the values, and save them to Features directory\n",
    "* Visualise most feature and cross-feature distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.646170Z",
     "iopub.status.busy": "2023-03-28T14:15:57.646007Z",
     "iopub.status.idle": "2023-03-28T14:15:57.647802Z",
     "shell.execute_reply": "2023-03-28T14:15:57.647608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Each feature will be an object of FEParameter\n",
    "class FEParameter:\n",
    "    def __init__(self, name:str, min:float=0.0, max:float=9999):\n",
    "        self.name = name\n",
    "        self.min = min\n",
    "        self.max = max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define FeatureSelection class that is used to visualise and select data\n",
    "class FeatureSelection():\n",
    "    def __init__(self, feature_extracted_ECG):\n",
    "        self.feature_extracted_ECG = feature_extracted_ECG\n",
    "\n",
    "\n",
    "    # desired_features is a list of FEParameter objects  \n",
    "    def select(self, desired_features:list[FEParameter]):\n",
    "        self.selected_features_ECG = self.feature_extracted_ECG['Stress Level']\n",
    "\n",
    "        for feature in desired_features:\n",
    "            # Sanity check: check if feature exists\n",
    "            if feature.name in self.feature_extracted_ECG.columns:\n",
    "                # Set value to NaN if it falls outside min and max values.\n",
    "                for i, value in enumerate(self.feature_extracted_ECG[feature.name]):\n",
    "                    if (value < feature.min) or (value > feature.max):\n",
    "                        self.selected_features_ECG.loc[i, feature.name] = np.nan\n",
    "                # Add column to new selected features\n",
    "                self.selected_features_ECG[feature.name] = self.feature_extracted_ECG[feature.name]\n",
    "            else:\n",
    "                print(f'Error: No such feature \"{feature}\" in extracted features')\n",
    "\n",
    "\n",
    "    # impute missing values in dataset with mean values of column\n",
    "    def impute(self):\n",
    "        self.selected_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        imp = SimpleImputer(strategy='mean')\n",
    "        imp.fit(self.selected_features)\n",
    "        self.selected_features = pd.DataFrame(imp.transform(self.selected_features), columns=self.selected_features.columns)\n",
    "\n",
    "\n",
    "    def visualise(self, plot_type='pairplot'):\n",
    "        print(\"Generating plot...\")\n",
    "        if plot_type == 'pairplot':         \n",
    "            sns.pairplot(data = self.selected_features, hue = 'Stress Level')\n",
    "        elif plot_type == 'kdeplot':\n",
    "            # Create a figure with subplots for each feature\n",
    "            subplot_size = math.ceil(math.sqrt(len(self.selected_features.columns)))\n",
    "            fig = plt.figure(figsize=(20, 8*subplot_size))\n",
    "\n",
    "            # Loop through each feature and add it to a subplot\n",
    "            for i, feature in enumerate(self.selected_features):\n",
    "                fig.add_subplot(subplot_size, subplot_size, i+1)\n",
    "                sns.kdeplot(x=feature, data=self.selected_features, hue='Stress Level', common_norm=False, warn_singular=False)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Plot type not recognised. Please choose between pairplot, kdeplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.657717Z",
     "iopub.status.busy": "2023-03-28T14:15:57.657598Z",
     "iopub.status.idle": "2023-03-28T14:15:57.661341Z",
     "shell.execute_reply": "2023-03-28T14:15:57.661082Z"
    }
   },
   "outputs": [],
   "source": [
    "# See Neurokit2 HRV - https://neuropsychology.github.io/NeuroKit/functions/hrv.html\n",
    "\n",
    "# Minimum and maximum expected HR (beats per min)\n",
    "min_HR = 30\n",
    "max_HR = 200\n",
    "\n",
    "# MinNN: The minimum of the RR intervals (Parent, 2019; Subramaniam, 2022).\n",
    "HRV_MinNN = FE_Parameter('HRV_MinNN', min=60000.0/max_HR, max=60000.0/min_HR)\n",
    "# MaxNN: The maximum of the RR intervals (Parent, 2019; Subramaniam, 2022).\n",
    "HRV_MaxNN = FE_Parameter('HRV_MaxNN', min=60000.0/max_HR, max=60000.0/min_HR)\n",
    "# MeanNN: The mean of the RR intervals.\n",
    "HRV_MeanNN = FE_Parameter('HRV_MeanNN', min=60000.0/max_HR, max=60000.0/min_HR)\n",
    "\n",
    "# SDNN: The standard deviation of the RR intervals.\n",
    "# See https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5624990/ for chosen min and max values.\n",
    "HRV_SDNN = FE_Parameter('HRV_SDNN', min=30, max=150)\n",
    "# RMSSD: The square root of the mean of the squared successive differences between adjacent RR intervals. \n",
    "# # It is equivalent (although on another scale) to SD1, and therefore it is redundant to report correlations with both (Ciccone, 2017).\n",
    "# See https://help.welltory.com/en/articles/4413231-what-normal-ranges-and-measurement-standards-we-use-to-interpret-your-heart-rate-variability for chosen min and max values.\n",
    "HRV_RMSSD = FE_Parameter('HRV_RMSSD', min=13, max=107)\n",
    "# The root mean square of successive differences (RMSSD) divided by the mean of the RR intervals (MeanNN).\n",
    "CVSD = FE_Parameter('HRV_CVSD')\n",
    "# Shannon Entropy\n",
    "HRV_SE = FE_Parameter('HRV_SE')\n",
    "\n",
    "# pNN50: The proportion of RR intervals greater than 20ms, out of the total number of RR intervals.\n",
    "HRV_pNN50 = FE_Parameter('HRV_pNN20')\n",
    "# A geometrical parameter of the HRV, or more specifically, the baseline width of the RR intervals distribution \n",
    "# TINN: obtained by triangular interpolation, where the error of least squares determines the triangle. \n",
    "# It is an approximation of the RR interval distribution.\n",
    "HRV_TINN = FE_Parameter('HRV_TINN')\n",
    "# HTI: The HRV triangular index, measuring the total number of RR intervals divided by the height of the RR intervals histogram.\n",
    "HRV_HTI = FE_Parameter('HRV_HTI')\n",
    "\n",
    "# VLF: The spectral power (W/Hz) of very low frequencies (.0033 to .04 Hz).\n",
    "# HRV_VLF = FE_Parameter('HRV_VLF', min=0.0, max=9) # hidden due to use of 0.5 Hz high-pass butterworth filter\n",
    "# LF: The spectral power (W/Hz) of low frequencies (.04 to .15 Hz).\n",
    "HRV_LF = FE_Parameter('HRV_LF', max=1.00)\n",
    "# HF: The spectral power (W/Hz) of high frequencies (.15 to .4 Hz).\n",
    "HRV_HF = FE_Parameter('HRV_HF', max=1.00)\n",
    "# LFHF: The ratio obtained by dividing the low frequency power by the high frequency power.\n",
    "HRV_LFHF = FE_Parameter('HRV_LFHF', max=1.00)\n",
    "\n",
    "\n",
    "# Append all FE parameters to a list which will be passed to Feature_Extraction's Select method\n",
    "selected_features = []\n",
    "all_variables = dict(globals(), **locals())\n",
    "for name, var in all_variables.items():\n",
    "    if isinstance(var, FE_Parameter):\n",
    "        selected_features.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if feature file exists\n",
    "if os.path.exists(f'{directory}/Features/Selected/Featured_ECG_with_{window_length}s_Window_Length.csv'):\n",
    "    print(\"Skipping Feature Selection\")\n",
    "else:\n",
    "    fs = FeatureSelection(f'{directory}/Features')\n",
    "    fs.select(selected_features)\n",
    "    fs.impute()\n",
    "    fs.visualise(plot_type='kdeplot')\n",
    "    fs.visualise(plot_type='pairplot')\n",
    "    fs.save_features()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Classification\n",
    "* Prepares data by randomly splitting data into train, test and validation data\n",
    "* Option for Linear Discriminant Analysis (LDA) for dimension reduction\n",
    "* Implements the following classification models: LDA, Random Forests with methods for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.678855Z",
     "iopub.status.busy": "2023-03-28T14:15:57.678628Z",
     "iopub.status.idle": "2023-03-28T14:15:57.685391Z",
     "shell.execute_reply": "2023-03-28T14:15:57.685101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define LinearML method, which implements different linear classification methods\n",
    "class Linear_ML():\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    \n",
    "    # randomizes, standardizes, and splits into test and training data.\n",
    "    def prepare(self, LDA_dimension_reduction=False):       \n",
    "        dataset_size = len(self.dataset.columns)\n",
    "        features = self.dataset.iloc[:, 0:dataset_size-1].values\n",
    "        label = self.dataset.iloc[:,dataset_size-1].values\n",
    "        # split into test, training and validation data (hold-out validation)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(features, label, train_size = 0.8, test_size=0.2, random_state=15)\n",
    "        self.X_train, self.X_valid, self.y_train, self.y_valid = train_test_split(self.X_train, self.y_train, train_size = 0.9, test_size=0.1, random_state=15)\n",
    "        \n",
    "        # check distribution of stress to not stressed is even for each split:\n",
    "        stressed_dataset = (np.count_nonzero(label == 1) / (np.count_nonzero(label == 1) + np.count_nonzero(label == 0))) * 100\n",
    "        stressed_y_test = (np.count_nonzero(self.y_test == 1) / (np.count_nonzero(self.y_test == 1) + np.count_nonzero(self.y_test == 0))) * 100\n",
    "        stressed_y_train = (np.count_nonzero(self.y_train == 1) / (np.count_nonzero(self.y_train == 1) + np.count_nonzero(self.y_train == 0))) * 100\n",
    "        stressed_y_valid = (np.count_nonzero(self.y_valid == 1) / (np.count_nonzero(self.y_valid == 1) + np.count_nonzero(self.y_valid == 0))) * 100\n",
    "\n",
    "        print(f'Stressed Percentage for dataset is {stressed_dataset:.2f}%')\n",
    "        print(f'Stressed Percentage for test data is {stressed_y_test:.2f}%')\n",
    "        print(f'Stressed Percentage for train data is {stressed_y_train:.2f}%')\n",
    "        print(f'Stressed Percentage for valid data is {stressed_y_valid:.2f}%')\n",
    "\n",
    "        # scale \n",
    "        sc = StandardScaler()\n",
    "        self.X_train = sc.fit_transform(self.X_train)\n",
    "        self.X_test = sc.transform(self.X_test)\n",
    "\n",
    "        # LDA for dimension reduction creates new train and test data\n",
    "        if LDA_dimension_reduction:\n",
    "            lda = LinearDiscriminantAnalysis()\n",
    "            self.X_train = lda.fit_transform(self.X_train, self.y_train)\n",
    "            self.X_test = lda.transform(self.X_test)\n",
    "            self.X_valid = lda.transform(self.X_valid)\n",
    "\n",
    "\n",
    "    # LDA classification\n",
    "    def LDA_classifier(self):\n",
    "        clf1 = LinearDiscriminantAnalysis(solver='lsqr', shrinkage=0.5).fit(self.X_train, self.y_train)\n",
    "        clf2 = LinearDiscriminantAnalysis(solver='lsqr', shrinkage=None).fit(self.X_train, self.y_train)\n",
    "\n",
    "        clf1_score = clf1.score(self.X_test, self.y_test)\n",
    "        clf2_score = clf2.score(self.X_test, self.y_test)\n",
    "\n",
    "        print(f'Score for CLF1: {clf1_score}')\n",
    "        print(f'Score for CLF2: {clf2_score}')\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    # Tuning Random Forest\n",
    "    def rf_tuner(self):\n",
    "        n_estimators = 100\n",
    "        max_features = [1, 'sqrt', 'log2']\n",
    "        max_depths = [None, 2, 3, 4, 5]\n",
    "        for f, d in product(max_features, max_depths): # with product we can iterate through all possible combinations\n",
    "            rf = RandomForestClassifier(n_estimators=n_estimators, \n",
    "                                        criterion='entropy', \n",
    "                                        max_features=f, \n",
    "                                        max_depth=d, \n",
    "                                        n_jobs=-1,\n",
    "                                        random_state=1337)\n",
    "            rf.fit(self.X_train, self.y_train)\n",
    "            self.y_pred = rf.predict(X=self.X_test)\n",
    "            print('Classification accuracy on test set with max features = {} and max_depth = {}: {:.3f}'.format(f, d, accuracy_score(self.y_test,self.y_pred)))\n",
    "            # self.plot_confustion_matrix()\n",
    "\n",
    "\n",
    "    # Post-tuned Random Forests Classifier\n",
    "    def rf_classifier(self):\n",
    "        # source: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "        rf = RandomForestClassifier(n_estimators=100, criterion='entropy')\n",
    "        rf.fit(self.X_train, self.y_train)\n",
    "        self.y_pred = rf.predict(X=self.X_test)\n",
    "\n",
    "        # Accuracy on Test\n",
    "        print(\"Training Accuracy is: \", rf.score(self.X_train, self.y_train))\n",
    "        # Accuracy on Train\n",
    "        print(\"Testing Accuracy is: \", rf.score(self.X_test, self.y_test))\n",
    "\n",
    "        self.plot_confustion_matrix()\n",
    "\n",
    "    \n",
    "    def plot_confustion_matrix(self):\n",
    "        # Confusion Matrix\n",
    "        ConfusionMatrixDisplay.from_predictions(self.y_test, self.y_pred, display_labels=['Negative', 'Positive'])\n",
    "\n",
    "\n",
    "    # main method to conduct LDA\n",
    "    def model(self, classification_type):\n",
    "        classification_types = {\n",
    "            \"LDA\": self.LDA_classifier,\n",
    "            \"RF\": self.rf_classifier\n",
    "        }\n",
    "        classification_types.get(classification_type, lambda: print(\"Invalid model. Please enter LDA, RF\"))()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T14:15:57.686647Z",
     "iopub.status.busy": "2023-03-28T14:15:57.686556Z",
     "iopub.status.idle": "2023-03-28T14:15:58.903839Z",
     "shell.execute_reply": "2023-03-28T14:15:58.903584Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retreive dataset of pre-processed and featured data from csv \n",
    "dataset_path = f'{directory}/Features/Selected/Featured_ECG_with_{window_length}s_Window_Length.csv'\n",
    "dataset = Utilities.load_dataframe(dataset_path)\n",
    "\n",
    "# pass to LinearML class\n",
    "lml = Linear_ML(dataset)\n",
    "# prepare dataset, split to a 33% test split and use LDA for dimension reduction\n",
    "lml.prepare(LDA_dimension_reduction=False)\n",
    "\n",
    "lml.model('LDA')\n",
    "\n",
    "# run Random Forest Tuner\n",
    "# lml.rf_tuner()\n",
    "\n",
    "#lml.model('RF')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
